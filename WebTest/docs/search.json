[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Project 2 Supervised Learning - Machine Learning and Data Mining"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "WebTest",
    "section": "",
    "text": "Hello, My name is Caro and this is my first website test for a jupyter notebook!"
  },
  {
    "objectID": "Project2_ML.html",
    "href": "Project2_ML.html",
    "title": "Project 2: South African Heart Disease",
    "section": "",
    "text": "Group 22\nimport pandas as pd\nimport numpy as np\n\nfrom collections import Counter\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.linalg import svd\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure, plot, xlabel, ylabel, legend, show, clim, semilogx, loglog, title, subplot, grid\nfrom scipy.io import loadmat\n\nimport sklearn.linear_model as lm\nfrom sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\nfrom sklearn import model_selection, tree\nfrom scipy import stats\nimport torch\nimport seaborn as sns\n\nfrom toolbox_02450 import feature_selector_lr, bmplot, rlr_validate, train_neural_net\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n# Load data from csv and store it in dataframe'\ndf = pd.read_csv('SAheart.data.txt')"
  },
  {
    "objectID": "Project2_ML.html#data-visualization",
    "href": "Project2_ML.html#data-visualization",
    "title": "Project 2: South African Heart Disease",
    "section": "Data visualization",
    "text": "Data visualization\n\n\nCode\n# Change variables to integers: Replace absent and present by 0 and 1\ndf = df.replace('Absent',0)\ndf = df.replace('Present',1)\ndf\n\n\n\n\n\n\n\nTable 1: The South Africa Heart Disease Dataset\n\n\n\nrow.names\nsbp\ntobacco\nldl\nadiposity\nfamhist\ntypea\nobesity\nalcohol\nage\nchd\n\n\n\n\n0\n1\n160\n12.00\n5.73\n23.11\n1\n49\n25.30\n97.20\n52\n1\n\n\n1\n2\n144\n0.01\n4.41\n28.61\n0\n55\n28.87\n2.06\n63\n1\n\n\n2\n3\n118\n0.08\n3.48\n32.28\n1\n52\n29.14\n3.81\n46\n0\n\n\n3\n4\n170\n7.50\n6.41\n38.03\n1\n51\n31.99\n24.26\n58\n1\n\n\n4\n5\n134\n13.60\n3.50\n27.78\n1\n60\n25.99\n57.34\n49\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n457\n459\n214\n0.40\n5.98\n31.72\n0\n64\n28.45\n0.00\n58\n0\n\n\n458\n460\n182\n4.20\n4.41\n32.10\n0\n52\n28.61\n18.72\n52\n1\n\n\n459\n461\n108\n3.00\n1.59\n15.23\n0\n40\n20.09\n26.64\n55\n0\n\n\n460\n462\n118\n5.40\n11.61\n30.79\n0\n64\n27.35\n23.97\n40\n0\n\n\n461\n463\n132\n0.00\n4.82\n33.41\n1\n62\n14.70\n0.00\n46\n1\n\n\n\n\n\n462 rows × 11 columns\n\n\n\n\n# Drop row column\ndf.drop(['row.names'], axis = 'columns', inplace=True) \n\n\n# Create matrix with values\nraw_data = df.values  \ncols = range(0, 9) \nX = raw_data[:, cols]\nX = np.asarray(X, dtype = np.intc)\nattributeNames = np.asarray(df.columns[cols])\nclassLabels = raw_data[:,-1]\nclassNames = sorted(set(classLabels))\nclassDict = dict(zip(classNames,range(len(classNames))))\ny = np.array([classDict[cl] for cl in classLabels])\nN, M = X.shape\nC = len(classNames)"
  },
  {
    "objectID": "Project2_ML.html#dependent-variable",
    "href": "Project2_ML.html#dependent-variable",
    "title": "Project 2: South African Heart Disease",
    "section": "Dependent variable",
    "text": "Dependent variable\nWhich attribute is the best suitable for regression analysis?\nWe use OLS linear regression model. We select one variable to be predicted according to all our other variables in the dataset, and measure the values of the MSE residuals when plotting the estimated data points against the true line of fitting.\n\ncriterion_variables = ['sbp', 'tobacco', 'ldl', 'adiposity', 'typea', 'obesity', 'alcohol', 'age','chd','famhist']\n\n# Display plot\nplt.figure(figsize=(25,15))\nplt.subplots_adjust(wspace = 0.5)\n\nfor i in range(10):\n    attr_col = list(df.columns).index(criterion_variables[i])\n    cols = list(range(0, attr_col)) + list(range(attr_col + 1, len(df.columns)))\n\n    X = raw_data[:, cols]\n    y = raw_data[:, attr_col] # the 'oldpeak' column\n    attributeNames = list(df.columns[cols])\n\n    N, M = X.shape\n    \n    X = stats.zscore(X);\n\n    # Fit ordinary least squares regression model    \n    model = lm.LinearRegression()\n    model.fit(X,y)\n\n    # Predict thalach\n    y_est = model.predict(X)\n    residual = y_est-y\n    \n    plt.subplot(2, 5, i+1)\n    plt.plot(y, y, '--r')\n    plt.plot(y, y_est, '.g')\n    plt.xlabel('Value of {0} (true)'.format(criterion_variables[i])); ylabel('Value of {0} variable (estimated)'.format(criterion_variables[i]))\n    plt.legend(['True values', 'Estimated values'], loc = 2);\n\n\n\n\nEstimation values of ‘adiposity’, ‘obesity’, and ‘age’ seem to align well with the line of true fit. However ‘obesity’ has lower average MSE residuals overall, so we will choose it as our dependent variable to be estimated using all other dataset attributes.\n\nFeature selection\nCan we improve the model by reaching the same prediction power using fewer attributes?\nWe use feature forward selection with cross-validation across 10 outer folds and 10 inner folds, where we train a model starting with no features and gradually select and add, one-by-one, the features which minimize the squared error in the inner cross-validation loop, until the error cannot be minimized by adding any further predictors. Finally, we compute the R2 value of both the full model without feature selection and the newly-fitted feature-selecting model upon a test set, in order to see what percentage of the variance within the dataset is explained by each model.\n\nraw_data = df.to_numpy()\nattr_col = list(df.columns).index('obesity')\ncols = list(range(0, attr_col)) + list(range(attr_col + 1, len(df.columns)))\n\nX = raw_data[:, cols]\ny = raw_data[:, attr_col] # the 'adiposity' column\nattributeNames = list(df.columns[cols])\nN, M = X.shape\n\nX = stats.zscore(X)\n\n## Crossvalidation\n# Create crossvalidation partition for evaluation\nK = 10\nCV = model_selection.KFold(n_splits=K,shuffle=True)\n\n# Initialize variables\nFeatures = np.zeros((M,K))\nError_train = np.empty((K,1))\nError_test = np.empty((K,1))\nError_train_fs = np.empty((K,1))\nError_test_fs = np.empty((K,1))\nError_train_nofeatures = np.empty((K,1))\nError_test_nofeatures = np.empty((K,1))\n\nk=0\nfor train_index, test_index in CV.split(X):\n    \n    # extract training and test set for current CV fold\n    X_train = X[train_index,:]\n    y_train = y[train_index]\n    X_test = X[test_index,:]\n    y_test = y[test_index]\n    internal_cross_validation = 10\n    \n    # Compute squared error without using the input data at all\n    Error_train_nofeatures[k] = np.square(y_train-y_train.mean()).sum()/y_train.shape[0]\n    Error_test_nofeatures[k] = np.square(y_test-y_test.mean()).sum()/y_test.shape[0]\n\n    # Compute squared error with all features selected (no feature selection)\n    m = lm.LinearRegression(fit_intercept=True).fit(X_train, y_train)\n    Error_train[k] = np.square(y_train-m.predict(X_train)).sum()/y_train.shape[0]\n    Error_test[k] = np.square(y_test-m.predict(X_test)).sum()/y_test.shape[0]\n\n    # Compute squared error with feature subset selection\n    textout = ''\n    selected_features, features_record, loss_record = feature_selector_lr(X_train, y_train, internal_cross_validation,display=textout)\n    \n    Features[selected_features,k] = 1\n    # .. alternatively you could use module sklearn.feature_selection\n    \n    if len(selected_features) == 0:\n        print('No features were selected, i.e. the data (X) in the fold cannot describe the outcomes (y).' )\n    else:\n        m = lm.LinearRegression(fit_intercept=True).fit(X_train[:,selected_features], y_train)\n        Error_train_fs[k] = np.square(y_train-m.predict(X_train[:,selected_features])).sum()/y_train.shape[0]\n        Error_test_fs[k] = np.square(y_test-m.predict(X_test[:,selected_features])).sum()/y_test.shape[0]\n    \n        figure(k)\n        subplot(1,2,1)\n        plot(range(1,len(loss_record)), loss_record[1:])\n        xlabel('Iteration')\n        ylabel('Squared error (crossvalidation)')    \n        \n        subplot(1,3,3)\n        bmplot(attributeNames, range(1,features_record.shape[1]), -features_record[:,1:])\n        clim(-1.5,0)\n        xlabel('Iteration')\n\n    #print('Cross validation fold {0}/{1}'.format(k+1,K))\n    #print('Train indices: {0}'.format(train_index))\n    #print('Test indices: {0}'.format(test_index))\n    #print('Features no: {0}\\n'.format(selected_features.size))\n\n    k+=1\n    \n# Display results\nprint('Linear regression without feature selection:')\nprint('- Training error: {0}'.format(Error_train.mean()))\nprint('- Test error:     {0}'.format(Error_test.mean()))\nprint('- R^2 train:     {0}'.format((Error_train_nofeatures.sum()-Error_train.sum())/Error_train_nofeatures.sum()))\nprint('- R^2 test:     {0}'.format((Error_test_nofeatures.sum()-Error_test.sum())/Error_test_nofeatures.sum()))\nprint('\\n')\nprint('Linear regression with feature selection:')\nprint('- Training error: {0}'.format(Error_train_fs.mean()))\nprint('- Test error:     {0}'.format(Error_test_fs.mean()))\nprint('- R^2 train:     {0}'.format((Error_train_nofeatures.sum()-Error_train_fs.sum())/Error_train_nofeatures.sum()))\nprint('- R^2 test:     {0}'.format((Error_test_nofeatures.sum()-Error_test_fs.sum())/Error_test_nofeatures.sum()))\n\nfigure(k)\nsubplot(1,3,2)\nbmplot(attributeNames, range(1,Features.shape[1]+1), -Features)\nclim(-1.5,0)\nxlabel('Crossvalidation fold')\nylabel('Attribute')\n\nLinear regression without feature selection:\n- Training error: 7.676131475685457\n- Test error:     8.104654216746123\n- R^2 train:     0.5666276636565277\n- R^2 test:     0.5337053644554508\n\n\nLinear regression with feature selection:\n- Training error: 7.7469134976404685\n- Test error:     8.109273625721451\n- R^2 train:     0.5626315139914363\n- R^2 test:     0.5334395905474004\n\n\nText(0, 0.5, 'Attribute')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the forward selection algorithm, we can see that our dependent variable ‘obesity’ is best described using the features: ‘adiposity’, ‘typea’ and ‘age’, while many other variables have been left out completely from our model, due to the low assistance in further reducing the MSE of the estimates.\nHowever, as can be seen in the table, the R2 value in the test set of both models is similar, representing a variance of ~54%, suggesting that feature selection does not have an optimising impact on our regression model, and may even slightly increase the MSE. This means that, although the criterion is largely predicted by only a few attributes, the full model definitely does not over-fit the data and even terms that have a very low correlation will help to reduce the MSE.\n\n# Inspect selected feature coefficients effect on the entire dataset \n# and plot the fitted model residual error as function of each attribute\n# to inspect for systematic structure in the residual\n\nf=2 # cross-validation fold to inspect\nff=Features[:,f-1].nonzero()[0]\nprint(ff)\nif len(ff) == 0:\n    print('\\nNo features were selected, i.e. the data (X) in the fold cannot describe the outcomes (y).' )\nelse:\n    m = lm.LinearRegression(fit_intercept=True).fit(X[:,ff], y)\n    \n    y_est= m.predict(X[:,ff])\n    residual=y-y_est\n    \n    figure(k+1, figsize=(12,6))\n    title('Residual error vs. Attributes for features selected in cross-validation fold {0}'.format(f))\n    for i in range(0,len(ff)):\n       subplot(4, int( np.ceil(len(ff) // 2)),i+1)\n       plot(X[:,ff[i]],residual,'.')\n       xlabel(attributeNames[ff[i]])\n       ylabel('residual error')\n    plt.tight_layout()\n    \nshow()\n\n[3 4 5 7]\n\n\n\n\n\n\n\nRegularization\nHere we introduce a regularization parameter λ into the linear regression, which will take values between \\(10^-3\\) and \\(10^7\\) because we want to obtain the lowest possible generalization error when using our linear regression model.\nIn order to reliably estimate the generalization error for different values of λ, we have performed two-level cross-validation testing, with the outer layer having K1 = 10 folds and the inner fold being selected to K2 = 10 folds.\n\n# Add offset attribute\nX = np.concatenate((np.ones((X.shape[0],1)),X),1)\nattributeNames= [u'Offset']+attributeNames\nM = M+1\nattributeNames\n\n['Offset',\n 'sbp',\n 'tobacco',\n 'ldl',\n 'adiposity',\n 'famhist',\n 'typea',\n 'alcohol',\n 'age',\n 'chd']\n\n\n\n## Crossvalidation\n# Create crossvalidation partition for evaluation\nK = 10\nCV = model_selection.KFold(K, shuffle=True)\n#CV = model_selection.KFold(K, shuffle=False)\n\n# Values of lambda\nlambdas = np.power(10.,range(-5,9))\n\n# Initialize variables\n#T = len(lambdas)\nError_train = np.empty((K,1))\nError_test = np.empty((K,1))\nError_train_rlr = np.empty((K,1))\nError_test_rlr = np.empty((K,1))\nError_train_nofeatures = np.empty((K,1))\nError_test_nofeatures = np.empty((K,1))\nval_error_lambdas = np.empty((K,len(lambdas)))\nw_rlr = np.empty((M,K))\nmu = np.empty((K, M-1))\nsigma = np.empty((K, M-1))\nw_noreg = np.empty((M,K))\n\nk=0\nfor train_index, test_index in CV.split(X,y):\n    \n    # extract training and test set for current CV fold\n    X_train = X[train_index]\n    y_train = y[train_index]\n    X_test = X[test_index]\n    y_test = y[test_index]\n    internal_cross_validation = 10    \n    \n    opt_val_err, opt_lambda, mean_w_vs_lambda, train_err_vs_lambda, test_err_vs_lambda = rlr_validate(X_train, y_train, lambdas, internal_cross_validation)\n    val_error_lambdas[k] = test_err_vs_lambda\n    \n    # Standardize outer fold based on training set, and save the mean and standard\n    # deviations since they're part of the model (they would be needed for\n    # making new predictions) - for brevity we won't always store these in the scripts\n    mu[k, :] = np.mean(X_train[:, 1:], 0)\n    sigma[k, :] = np.std(X_train[:, 1:], 0)\n    \n    X_train[:, 1:] = (X_train[:, 1:] - mu[k, :] ) / sigma[k, :] \n    X_test[:, 1:] = (X_test[:, 1:] - mu[k, :] ) / sigma[k, :] \n    \n    Xty = X_train.T @ y_train\n    XtX = X_train.T @ X_train\n    \n    # Compute mean squared error without using the input data at all\n    Error_train_nofeatures[k] = np.square(y_train-y_train.mean()).sum(axis=0)/y_train.shape[0]\n    Error_test_nofeatures[k] = np.square(y_test-y_test.mean()).sum(axis=0)/y_test.shape[0]\n\n    # Estimate weights for the optimal value of lambda, on entire training set\n    lambdaI = opt_lambda * np.eye(M)\n    lambdaI[0,0] = 0 # Do no regularize the bias term\n    w_rlr[:,k] = np.linalg.solve(XtX+lambdaI,Xty).squeeze()\n    # Compute mean squared error with regularization with optimal lambda\n    Error_train_rlr[k] = np.square(y_train-X_train @ w_rlr[:,k]).sum(axis=0)/y_train.shape[0]\n    Error_test_rlr[k] = np.square(y_test-X_test @ w_rlr[:,k]).sum(axis=0)/y_test.shape[0]\n\n    # Estimate weights for unregularized linear regression, on entire training set\n    w_noreg[:,k] = np.linalg.solve(XtX,Xty).squeeze()\n    # Compute mean squared error without regularization\n    Error_train[k] = np.square(y_train-X_train @ w_noreg[:,k]).sum(axis=0)/y_train.shape[0]\n    Error_test[k] = np.square(y_test-X_test @ w_noreg[:,k]).sum(axis=0)/y_test.shape[0]\n    # OR ALTERNATIVELY: you can use sklearn.linear_model module for linear regression:\n    #m = lm.LinearRegression().fit(X_train, y_train)\n    #Error_train[k] = np.square(y_train-m.predict(X_train)).sum()/y_train.shape[0]\n    #Error_test[k] = np.square(y_test-m.predict(X_test)).sum()/y_test.shape[0]\n\n    # Display the results for the last cross-validation fold\n    if k == K-1:\n        figure(k, figsize=(12,8))\n        subplot(1,2,1)\n        semilogx(lambdas,mean_w_vs_lambda.T[:,1:],'.-') # Don't plot the bias term\n        xlabel('Regularization factor')\n        ylabel('Mean Coefficient Values')\n        grid()\n        # You can choose to display the legend, but it's omitted for a cleaner \n        # plot, since there are many attributes\n        legend(attributeNames[1:], loc='best')\n        \n        subplot(1,2,2)\n        title('Optimal lambda: 1e{0}'.format(np.log10(opt_lambda)))\n        loglog(lambdas,train_err_vs_lambda.T,'b.-',lambdas,test_err_vs_lambda.T,'r.-')\n        xlabel('Regularization factor')\n        ylabel('Squared error (crossvalidation)')\n        legend(['Train error','Validation error'])\n        grid()\n    \n    # To inspect the used indices, use these print statements\n    #print('Cross validation fold {0}/{1}:'.format(k+1,K))\n    #print('Train indices: {0}'.format(train_index))\n    #print('Test indices: {0}\\n'.format(test_index))\n\n    k+=1\n    plt.tight_layout()\n    \nshow()\n\n# Display results\nprint('Linear regression without feature selection:')\nprint('- Training error: {0}'.format(Error_train.mean()))\nprint('- Test error:     {0}'.format(Error_test.mean()))\nprint('- R^2 train:     {0}'.format((Error_train_nofeatures.sum()-Error_train.sum())/Error_train_nofeatures.sum()))\nprint('- R^2 test:     {0}\\n'.format((Error_test_nofeatures.sum()-Error_test.sum())/Error_test_nofeatures.sum()))\nprint('Regularized linear regression:')\nprint('- Training error: {0}'.format(Error_train_rlr.mean()))\nprint('- Test error:     {0}'.format(Error_test_rlr.mean()))\nprint('- R^2 train:     {0}'.format((Error_train_nofeatures.sum()-Error_train_rlr.sum())/Error_train_nofeatures.sum()))\nprint('- R^2 test:     {0}\\n'.format((Error_test_nofeatures.sum()-Error_test_rlr.sum())/Error_test_nofeatures.sum()))\n\nprint('Weights in last fold:')\nfor m in range(M):\n    print('{:&gt;15} {:&gt;15}'.format(attributeNames[m], np.round(w_rlr[m,-1],2)))\n\nprint('Generalization error for different values of lambda:')\nfor i in range(len(lambdas)):\n    print('{:&gt;20} {:&gt;20}'.format(float(lambdas[i]), str(np.round(val_error_lambdas.mean(axis = 0)[i],2))))\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\nLinear regression without feature selection:\n- Training error: 7.680674622271397\n- Test error:     8.020988394051368\n- R^2 train:     0.5664245683319278\n- R^2 test:     0.5433320689535166\n\nRegularized linear regression:\n- Training error: 7.680865193377744\n- Test error:     8.019454576648586\n- R^2 train:     0.5664138105594974\n- R^2 test:     0.5434193955005107\n\nWeights in last fold:\n         Offset           26.05\n            sbp             0.2\n        tobacco            0.02\n            ldl            0.15\n      adiposity            3.47\n        famhist             0.2\n          typea            0.42\n        alcohol           -0.12\n            age           -0.97\n            chd           -0.34\nGeneralization error for different values of lambda:\n               1e-05                 8.05\n              0.0001                 8.05\n               0.001                 8.05\n                0.01                 8.05\n                 0.1                 8.05\n                 1.0                 8.05\n                10.0                 8.06\n               100.0                 8.82\n              1000.0                13.04\n             10000.0                16.89\n            100000.0                17.72\n           1000000.0                17.82\n          10000000.0                17.83\n         100000000.0                17.83\n\n\nThe model shows the typical trend of the generalisation error falling and then growing as λ increases. The lowest error is obtained when λ=1, hence that is the optimal value for our regularization parameter.\nWhen selecting this parameter we reach the best trade-off between bias and variance. If we had chosen a smaller λ value, the variance would be higher and the bias smaller, thus producing underfitting; whereas if we had set a higher value, the opposite would have happened, leading to overfitting.\nNow we want to find the equation of the regularized linear regresion in order to know how each atribute contributes to the prediction of the obesity\n\nraw_data = df.to_numpy()\nattr_col = list(df.columns).index('obesity')\ncols = list(range(0, attr_col)) + list(range(attr_col + 1, len(df.columns)))\n\nX = raw_data[:, cols]\ny = raw_data[:, attr_col]\nattributeNames = list(df.columns[cols])\nN, M = X.shape\n\nX = stats.zscore(X)\n\nridgereg_model = Ridge(alpha = 10, fit_intercept = True).fit(X, y)\nlinreg_model = LinearRegression(fit_intercept = True).fit(X, y)\n    \nprint('Weights for LinReg model with regularization:')\nprint('{:&gt;20} {:&gt;20}'.format('Intercept', str(np.round(ridgereg_model.intercept_,2))))\nfor m in range(M):\n    print('{:&gt;20} {:&gt;20}'.format(attributeNames[m], str(np.round(ridgereg_model.coef_[m],2))))\n    \nprint()\nprint()\n\nprint('Weights for LinReg model without regularization:')\nprint('{:&gt;20} {:&gt;20}'.format('Intercept', str(np.round(linreg_model.intercept_,2))))\nfor m in range(M):\n    print('{:&gt;20} {:&gt;20}'.format(attributeNames[m], str(np.round(linreg_model.coef_[m],2))))\n    \n\nWeights for LinReg model with regularization:\n           Intercept                26.04\n                 sbp                 0.17\n             tobacco                -0.04\n                 ldl                 0.16\n           adiposity                 3.45\n             famhist                 0.09\n               typea                 0.39\n             alcohol                -0.05\n                 age                 -0.9\n                 chd                -0.24\n\n\nWeights for LinReg model without regularization:\n           Intercept                26.04\n                 sbp                 0.16\n             tobacco                -0.03\n                 ldl                 0.12\n           adiposity                 3.61\n             famhist                 0.09\n               typea                 0.39\n             alcohol                -0.06\n                 age                -1.01\n                 chd                -0.24\n\n\nFrom this values we can say that a person with high values for sbp, ldl, adiposity, famhist, typea and age; and low values for tobacco, alcohol and chd will be more obese. Conversely, if the values of the variables sbp, ldl, adiposity, famhist, typea and ageare low; and the values of tobacco, alcohol and chd are high, the person will be thinner."
  },
  {
    "objectID": "Project2_ML.html#comparison-of-the-3-models",
    "href": "Project2_ML.html#comparison-of-the-3-models",
    "title": "Project 2: South African Heart Disease",
    "section": "Comparison of the 3 models",
    "text": "Comparison of the 3 models\nTwo-level cross-validation with K1 = K2 = 10 outer and inner folds. The inner folds are used to calculate, for each model, a complexity control parameter (λ and number of hidden layers) that minimises the generalisation error for that model, and the outer folds to test model performance.\nWe use the Setup I (training set is fixed): paired -test. Then, we calculate the 1- α confidence interval and the p-value.\n\nraw_data = df.to_numpy()\nattr_col = list(df.columns).index('obesity')\ncols = list(range(0, attr_col)) + list(range(attr_col + 1, len(df.columns)))\n\nX = raw_data[:, cols]\ny = raw_data[:, attr_col]\nattributeNames = list(df.columns[cols])\nN, M = X.shape\n\nK1 = 10 # for model selection\nK2 = 10 # for optimal parameter selection\n\n# K-fold crossvalidation\nCV1 = model_selection.KFold(n_splits=K1, shuffle=True)\n\nX = stats.zscore(X)\n\n# Initialize variable\nlinreg_test_error_k1 = np.zeros(K1)\nann_test_error_k1 = np.zeros(K1)\nbaseline_test_error_k1 = np.zeros(K1)\n\nk1=0\nfor par_index, test_index in CV1.split(X):\n    print('Computing CV1 fold: {0}/{1}..'.format(k1+1,K1))\n    print()\n    \n    # extract training and test set for current CV fold\n    X_par, y_par = X[par_index,:], y[par_index]\n    X_test, y_test = X[test_index,:], y[test_index]\n    \n    CV2 = model_selection.KFold(n_splits=K2, shuffle=False)\n    \n    # Regularized Linear Regression---------------------------------------------------------------------------------\n    lambda_interval = np.power(10.,range(-5,9))\n    linreg_gen_error_rate_s = np.zeros(len(lambda_interval))\n    \n    for s in range(0, len(lambda_interval)):\n        k2 = 0\n        linreg_val_error_rate = np.zeros(K2)\n        \n        for train_index, val_index in CV2.split(X_par):\n\n            # extract training and test set for current CV fold\n            X_train, y_train = X_par[train_index,:], y_par[train_index]\n            X_val, y_val = X_par[val_index,:], y_par[val_index]\n        \n            linreg_model = Ridge(alpha = lambda_interval[s], fit_intercept = True)\n            linreg_model = linreg_model.fit(X_train, y_train)\n\n            linreg_y_val_estimated = linreg_model.predict(X_val).T\n            linreg_val_error_rate[k2] = np.square(y_val - linreg_y_val_estimated).sum() / len(y_val)\n            k2 = k2 + 1\n        \n        linreg_gen_error_rate_s[s] = np.sum(linreg_val_error_rate) / len(linreg_val_error_rate)\n            \n    linreg_min_error = np.min(linreg_gen_error_rate_s)\n    opt_lambda_index = np.argmin(linreg_gen_error_rate_s)\n    opt_lambda = lambda_interval[opt_lambda_index]\n    \n    linreg_model = Ridge(alpha = lambda_interval[opt_lambda_index], fit_intercept = True)\n    linreg_model = linreg_model.fit(X_par, y_par)\n    \n    linreg_y_test_estimated = linreg_model.predict(X_test).T\n    linreg_test_error_k1[k1] = np.square(y_test - linreg_y_test_estimated).sum() / len(y_test)\n    \n    print('Error rate - regularized lin-reg - CV1 fold {0}/{1}: {2}'.format(k1+1, K1, np.round(linreg_test_error_k1[k1], decimals = 2)))\n    print('Optimal lambda: {0}'.format(opt_lambda))\n\n    \n    # ANN Regression -----------------------------------------------------------------------------------------------\n    h_unit_interval = np.arange(1, 4, 1) # number of hidden units in the single hidden layer\n    nr_of_nn_replicates = 3 # when finding loss, take the best neural network from n replicates (to deal with local minima issues)\n    max_iter = 10000 # max nr. of epochs (if convergence is not yet reached)\n    \n    ann_gen_error_rate_s = np.zeros(len(h_unit_interval))\n    \n    for s in range(0, len(h_unit_interval)):\n        k2 = 0\n        ann_val_error_rate = np.zeros(K2)\n             \n        for train_index, val_index in CV2.split(X_par):\n            #print('hello')\n            \n            ann_model = lambda: torch.nn.Sequential(\n                                     torch.nn.Linear(M, h_unit_interval[s]),\n                                     torch.nn.Tanh(),   \n                                     torch.nn.Linear(h_unit_interval[s], 1), # H hidden units to 1 output neuron\n                                     # no final tranfer function, since we are interested in the \"linear output\"\n                                 )\n            \n            #extract training and test set for current CV fold\n            X_train = torch.tensor(X_par[train_index,:], dtype=torch.float)\n            y_train = torch.tensor(y_par[train_index], dtype=torch.float)\n            X_val = torch.tensor(X_par[val_index,:], dtype=torch.float)\n            y_val = torch.tensor(y_par[val_index], dtype=torch.float)\n\n            \n\n            loss_fn = torch.nn.MSELoss() # Mean squared error loss function\n            best_trained_neural_net, final_loss, learning_curve = train_neural_net(ann_model, loss_fn, X = X_train, y = y_train, n_replicates = nr_of_nn_replicates, max_iter = max_iter)\n\n            ann_y_val_estimated = best_trained_neural_net(X_val)\n             # Convert tensors to numpy arrays, to work smoothly with class comparisons\n            ann_y_val_estimated = ann_y_val_estimated.detach().numpy().reshape(len(ann_y_val_estimated))\n            y_val = y_val.numpy().reshape(len(y_val))\n\n            ann_val_error_rate[k2] = np.square(y_val - ann_y_val_estimated).sum() / len(y_val)\n            k2 = k2 + 1\n            \n        ann_gen_error_rate_s[s] = np.sum(ann_val_error_rate) / len(ann_val_error_rate)\n            \n    ann_min_error = np.min(ann_gen_error_rate_s)\n    opt_nr_h_units_index = np.argmin(ann_gen_error_rate_s)\n    opt_nr_h_units = h_unit_interval[opt_nr_h_units_index]\n\n    tensor_X_par = torch.tensor(X_par, dtype=torch.float)\n    tensor_y_par = torch.tensor(y_par, dtype=torch.float)\n    tensor_X_test = torch.tensor(X_test, dtype=torch.float)\n\n    ann_model = lambda: torch.nn.Sequential(\n                                torch.nn.Linear(M, h_unit_interval[opt_nr_h_units_index]),\n                                torch.nn.Tanh(),   \n                                torch.nn.Linear(h_unit_interval[opt_nr_h_units_index], 1), # H hidden units to 1 output neuron\n                                # no final tranfer function, since we are interested in the \"linear output\"\n                            )\n    loss_fn = torch.nn.MSELoss() # Binary classification loss\n    best_trained_neural_net, final_loss, learning_curve = train_neural_net(ann_model, loss_fn, X = tensor_X_par, y = tensor_y_par, n_replicates = nr_of_nn_replicates, max_iter = max_iter)\n\n    ann_y_test_estimated = best_trained_neural_net(tensor_X_test)\n    # Convert tensors to numpy arrays, to work smoothly with class comparisons\n    ann_y_test_estimated = ann_y_test_estimated.detach().numpy().reshape(len(ann_y_test_estimated))\n    ann_test_error_k1[k1] = np.square(y_test - ann_y_test_estimated).sum() / len(y_test)\n\n    print('Error rate - Regression ANN - CV1 fold {0}/{1}: {2}'.format(k1+1, K1, np.round(ann_test_error_k1[k1], decimals = 2)))\n    print('Optimal number of hidden units: {0}'.format(opt_nr_h_units))\n\n    \n    # Baseline - LinearRegression ---------------------------------------------------------------------------------\n    baseline_test_error_k1[k1] = np.square(y_test - y_par.mean()).sum() / len(y_test)\n    \n    print('Error rate - baseline lin-reg - CV1 fold {0}/{1}: {2}'.format(k1+1, K1, np.round(baseline_test_error_k1[k1], decimals = 2)))\n    \n    \n    k1 = k1 + 1\n    print()\n    print()\n    \n# Statistical evaluation of models\nplt.figure(figsize = [18, 5])\nplt.subplots_adjust(wspace = 0.4)\n\nz = (ann_test_error_k1 - linreg_test_error_k1)\nz_mean = z.mean()\ndeg_f = K1-1\nsig =  (z - z_mean).std() / np.sqrt(deg_f)\nalpha = 0.05\n\nzL = z_mean + sig * stats.t.ppf(alpha/2, deg_f);\nzH = z_mean + sig * stats.t.ppf(1-alpha/2, deg_f);\n\nplt.subplot(1, 3, 1)\nplt.boxplot(np.concatenate((ann_test_error_k1.reshape((len(ann_test_error_k1), 1)), linreg_test_error_k1.reshape((len(linreg_test_error_k1), 1))), axis = 1))\nplt.xlabel('Regression ANN vs. Regularized Linear Regression')\nplt.ylabel('Cross-validation error')\n\nprint('Credibility interval has z-scores: ({0}, {1})'.format(np.round(zL, decimals = 2), np.round(zH, decimals = 2)))\nif zL &lt;= 0 and zH &gt;= 0 :\n    print('Regression ANN and regularized lin-reg are NOT significantly different')        \nelse:\n    print('Regression ANN and regularized lin-reg are significantly different.')\nprint()\nprint()\n\n\nz = (ann_test_error_k1 - baseline_test_error_k1)\nz_mean = z.mean()\ndeg_f = K1-1\nsig =  (z - z_mean).std() / np.sqrt(deg_f)\nalpha = 0.05\n\nzL = z_mean + sig * stats.t.ppf(alpha/2, deg_f);\nzH = z_mean + sig * stats.t.ppf(1-alpha/2, deg_f);\n\nplt.subplot(1, 3, 2)\nplt.boxplot(np.concatenate((ann_test_error_k1.reshape((len(ann_test_error_k1), 1)), baseline_test_error_k1.reshape((len(baseline_test_error_k1), 1))), axis = 1))\nplt.xlabel('Regression ANN vs. Baseline-form Linear Regression')\nplt.ylabel('Cross-validation error')\n\nprint('Credibility interval has z-scores: ({0}, {1})'.format(np.round(zL, decimals = 2), np.round(zH, decimals = 2)))\nif zL &lt;= 0 and zH &gt;= 0 :\n    print('Regression ANN and baseline lin-reg classifiers are   NOT significantly different')        \nelse:\n    print('Regression ANN and baseline lin-reg classifiers are significantly different.')\nprint()\nprint()\n\n\nz = (linreg_test_error_k1 - baseline_test_error_k1)\nz_mean = z.mean()\ndeg_f = K1-1\nsig =  (z - z_mean).std() / np.sqrt(deg_f)\nalpha = 0.05\n\nzL = z_mean + sig * stats.t.ppf(alpha/2, deg_f);\nzH = z_mean + sig * stats.t.ppf(1-alpha/2, deg_f);\n\nplt.subplot(1, 3, 3)\nplt.boxplot(np.concatenate((linreg_test_error_k1.reshape((len(linreg_test_error_k1), 1)), baseline_test_error_k1.reshape((len(baseline_test_error_k1), 1))), axis = 1))\nplt.xlabel('Regularized Linear Regression vs. Baseline-form Linear Regression')\nplt.ylabel('Cross-validation error')\n\nprint('Credibility interval has z-scores: ({0}, {1})'.format(np.round(zL, decimals = 2), np.round(zH, decimals = 2)))\nif zL &lt;= 0 and zH &gt;= 0 :\n    print('Regularized lin-reg and baseline lin-reg cla  ssifiers are NOT significantly different')        \nelse:\n    print('Regularized lin-reg and baseline lin-reg classifiers are significantly different.')\nprint()\nprint()\n\n# Generalized error rates \ngen_error_regularized_linreg = np.sum(linreg_test_error_k1) / len(linreg_test_error_k1)\ngen_error_ann = np.sum(ann_test_error_k1) / len(ann_test_error_k1)\ngen_error_baseline_linreg = np.sum(baseline_test_error_k1) / len(baseline_test_error_k1)\n\nprint('Generalized error rate - regularized lin-reg: {0}'.format(np.round(gen_error_regularized_linreg, decimals = 2)))\nprint('Generalized error rate - regression ANN: {0}'.format(np.round(gen_error_ann, decimals = 2)))\nprint('Generalized error rate - baseline lin-reg: {0}'.format(np.round(gen_error_baseline_linreg, decimals = 2)))\n\nComputing CV1 fold: 1/10..\n\nError rate - regularized lin-reg - CV1 fold 1/10: 9.93\nOptimal lambda: 1.0\nError rate - Regression ANN - CV1 fold 1/10: 19.7\nOptimal number of hidden units: 3\nError rate - baseline lin-reg - CV1 fold 1/10: 19.63\n\n\nComputing CV1 fold: 2/10..\n\nError rate - regularized lin-reg - CV1 fold 2/10: 6.74\nOptimal lambda: 1.0\nError rate - Regression ANN - CV1 fold 2/10: 11.9\nOptimal number of hidden units: 3\nError rate - baseline lin-reg - CV1 fold 2/10: 11.79\n\n\nComputing CV1 fold: 3/10..\n\nError rate - regularized lin-reg - CV1 fold 3/10: 5.38\nOptimal lambda: 1.0\nError rate - Regression ANN - CV1 fold 3/10: 11.17\nOptimal number of hidden units: 3\nError rate - baseline lin-reg - CV1 fold 3/10: 11.22\n\n\nComputing CV1 fold: 4/10..\n\nError rate - regularized lin-reg - CV1 fold 4/10: 5.37\nOptimal lambda: 1.0\nError rate - Regression ANN - CV1 fold 4/10: 10.81\nOptimal number of hidden units: 3\nError rate - baseline lin-reg - CV1 fold 4/10: 10.9\n\n\nComputing CV1 fold: 5/10..\n\nError rate - regularized lin-reg - CV1 fold 5/10: 8.78\nOptimal lambda: 1.0\nError rate - Regression ANN - CV1 fold 5/10: 26.12\nOptimal number of hidden units: 3\nError rate - baseline lin-reg - CV1 fold 5/10: 25.99\n\n\nComputing CV1 fold: 6/10..\n\nError rate - regularized lin-reg - CV1 fold 6/10: 5.42\nOptimal lambda: 1.0\nError rate - Regression ANN - CV1 fold 6/10: 14.16\nOptimal number of hidden units: 3\nError rate - baseline lin-reg - CV1 fold 6/10: 14.18\n\n\nComputing CV1 fold: 7/10..\n\nError rate - regularized lin-reg - CV1 fold 7/10: 4.8\nOptimal lambda: 1.0\nError rate - Regression ANN - CV1 fold 7/10: 11.87\nOptimal number of hidden units: 3\nError rate - baseline lin-reg - CV1 fold 7/10: 11.88\n\n\nComputing CV1 fold: 8/10..\n\nError rate - regularized lin-reg - CV1 fold 8/10: 9.71\nOptimal lambda: 1.0\nError rate - Regression ANN - CV1 fold 8/10: 29.91\nOptimal number of hidden units: 3\nError rate - baseline lin-reg - CV1 fold 8/10: 29.87\n\n\nComputing CV1 fold: 9/10..\n\nError rate - regularized lin-reg - CV1 fold 9/10: 18.64\nOptimal lambda: 1.0\nError rate - Regression ANN - CV1 fold 9/10: 25.1\nOptimal number of hidden units: 3\nError rate - baseline lin-reg - CV1 fold 9/10: 25.22\n\n\nComputing CV1 fold: 10/10..\n\nError rate - regularized lin-reg - CV1 fold 10/10: 5.25\nOptimal lambda: 1.0\nError rate - Regression ANN - CV1 fold 10/10: 17.3\nOptimal number of hidden units: 3\nError rate - baseline lin-reg - CV1 fold 10/10: 17.31\n\n\nCredibility interval has z-scores: (6.06, 13.55)\nRegression ANN and regularized lin-reg are significantly different.\n\n\nCredibility interval has z-scores: (-0.05, 0.06)\nRegression ANN and baseline lin-reg classifiers are   NOT significantly different\n\n\nCredibility interval has z-scores: (-13.51, -6.08)\nRegularized lin-reg and baseline lin-reg classifiers are significantly different.\n\n\nGeneralized error rate - regularized lin-reg: 8.0\nGeneralized error rate - regression ANN: 17.8\nGeneralized error rate - baseline lin-reg: 17.8\n\n\n\n\n\nThe best model for our dataset is the regularized linear regression as it achieves the lowest error.\nThe optimal λ is 1 as obtained in the previous exercise and the best number of hidden layers is 3.\n\n# Statistical comparison of models\n# Compute confidence interval of z = zA-zB and p-value of Null hypothesis\nalpha = 0.05\n\n# ANN vs. regularized linear regression \nz1 = (ann_test_error_k1 - linreg_test_error_k1)\nCI1 = stats.t.interval(1-alpha, len(z1)-1, loc=np.mean(z1), scale=stats.sem(z1))  # Confidence interval\np1 = 2*stats.t.cdf(-np.abs( np.mean(z1) )/stats.sem(z1), df=len(z1)-1)  # p-value\nprint('ANN vs. regularized linear regression')\nprint('Confidence interval:', CI1)\nprint('p-value:', p1)\nprint()\n\n# ANN vs. baseline\nz2 = (ann_test_error_k1 - baseline_test_error_k1)\nCI2 = stats.t.interval(1-alpha, len(z2)-1, loc=np.mean(z2), scale=stats.sem(z2))  # Confidence interval\np2 = 2*stats.t.cdf(-np.abs( np.mean(z2) )/stats.sem(z2), df=len(z2)-1)  # p-value\nprint('ANN vs. baseline')\nprint('Confidence interval:', CI2)\nprint('p-value:', p2)\nprint()\n\n# regularized linear regression vs. baseline\nz3 = (linreg_test_error_k1 - baseline_test_error_k1)\nCI3 = stats.t.interval(1-alpha, len(z3)-1, loc=np.mean(z3), scale=stats.sem(z3))  # Confidence interval\np3 = 2*stats.t.cdf( -np.abs( np.mean(z3) )/stats.sem(z3), df=len(z3)-1)  # p-value\nprint('Regularized linear regression vs. baseline')\nprint('Confidence interval:', CI3)\nprint('p-value:', p3)\n\nANN vs. regularized linear regression\nConfidence interval: (6.057463143680125, 13.545235443619362)\np-value: 0.00022284981919282824\n\nANN vs. baseline\nConfidence interval: (-0.05424606240787217, 0.06419466703832133)\np-value: 0.8535153425885907\n\nRegularized linear regression vs. baseline\nConfidence interval: (-13.51169331642087, -6.0810566662481715)\np-value: 0.00021146298100565937\n\n\nThe confidence intervals of the ANN and baseline models coincide, which means that there is no difference between the metrics and, in fact, the generalised error rate is the same. However, if we compare the intervals of these two models with that of the regularised linear regression, we can see that there is no overlap and therefore the difference between the metrics is statistically significant."
  },
  {
    "objectID": "projectML.html",
    "href": "projectML.html",
    "title": "Project 1: South Africa Heart Disease",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure, plot, title, legend, xlabel, ylabel, show\n\n\ndf = pd.read_csv(r\"C:\\Users\\Owner\\Desktop\\on campis\\SAheart.data.txt\", index_col = 'row.names')\n\n\ndf.head(20)\n\n\n\n\n\n\n\n\nsbp\ntobacco\nldl\nadiposity\nfamhist\ntypea\nobesity\nalcohol\nage\nchd\n\n\nrow.names\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n160\n12.00\n5.73\n23.11\nPresent\n49\n25.30\n97.20\n52\n1\n\n\n2\n144\n0.01\n4.41\n28.61\nAbsent\n55\n28.87\n2.06\n63\n1\n\n\n3\n118\n0.08\n3.48\n32.28\nPresent\n52\n29.14\n3.81\n46\n0\n\n\n4\n170\n7.50\n6.41\n38.03\nPresent\n51\n31.99\n24.26\n58\n1\n\n\n5\n134\n13.60\n3.50\n27.78\nPresent\n60\n25.99\n57.34\n49\n1\n\n\n6\n132\n6.20\n6.47\n36.21\nPresent\n62\n30.77\n14.14\n45\n0\n\n\n7\n142\n4.05\n3.38\n16.20\nAbsent\n59\n20.81\n2.62\n38\n0\n\n\n8\n114\n4.08\n4.59\n14.60\nPresent\n62\n23.11\n6.72\n58\n1\n\n\n9\n114\n0.00\n3.83\n19.40\nPresent\n49\n24.86\n2.49\n29\n0\n\n\n10\n132\n0.00\n5.80\n30.96\nPresent\n69\n30.11\n0.00\n53\n1\n\n\n11\n206\n6.00\n2.95\n32.27\nAbsent\n72\n26.81\n56.06\n60\n1\n\n\n12\n134\n14.10\n4.44\n22.39\nPresent\n65\n23.09\n0.00\n40\n1\n\n\n13\n118\n0.00\n1.88\n10.05\nAbsent\n59\n21.57\n0.00\n17\n0\n\n\n14\n132\n0.00\n1.87\n17.21\nAbsent\n49\n23.63\n0.97\n15\n0\n\n\n15\n112\n9.65\n2.29\n17.20\nPresent\n54\n23.53\n0.68\n53\n0\n\n\n16\n117\n1.53\n2.44\n28.95\nPresent\n35\n25.89\n30.03\n46\n0\n\n\n17\n120\n7.50\n15.33\n22.00\nAbsent\n60\n25.31\n34.49\n49\n0\n\n\n18\n146\n10.50\n8.29\n35.36\nPresent\n78\n32.73\n13.89\n53\n1\n\n\n19\n158\n2.60\n7.46\n34.07\nPresent\n61\n29.30\n53.28\n62\n1\n\n\n20\n124\n14.00\n6.23\n35.96\nPresent\n45\n30.09\n0.00\n59\n1\n\n\n\n\n\n\n\n\ndf.shape\n\n(462, 10)\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 462 entries, 1 to 463\nData columns (total 10 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   sbp        462 non-null    int64  \n 1   tobacco    462 non-null    float64\n 2   ldl        462 non-null    float64\n 3   adiposity  462 non-null    float64\n 4   famhist    462 non-null    object \n 5   typea      462 non-null    int64  \n 6   obesity    462 non-null    float64\n 7   alcohol    462 non-null    float64\n 8   age        462 non-null    int64  \n 9   chd        462 non-null    int64  \ndtypes: float64(5), int64(4), object(1)\nmemory usage: 39.7+ KB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nsbp\ntobacco\nldl\nadiposity\ntypea\nobesity\nalcohol\nage\nchd\n\n\n\n\ncount\n462.000000\n462.000000\n462.000000\n462.000000\n462.000000\n462.000000\n462.000000\n462.000000\n462.000000\n\n\nmean\n138.326840\n3.635649\n4.740325\n25.406732\n53.103896\n26.044113\n17.044394\n42.816017\n0.346320\n\n\nstd\n20.496317\n4.593024\n2.070909\n7.780699\n9.817534\n4.213680\n24.481059\n14.608956\n0.476313\n\n\nmin\n101.000000\n0.000000\n0.980000\n6.740000\n13.000000\n14.700000\n0.000000\n15.000000\n0.000000\n\n\n25%\n124.000000\n0.052500\n3.282500\n19.775000\n47.000000\n22.985000\n0.510000\n31.000000\n0.000000\n\n\n50%\n134.000000\n2.000000\n4.340000\n26.115000\n53.000000\n25.805000\n7.510000\n45.000000\n0.000000\n\n\n75%\n148.000000\n5.500000\n5.790000\n31.227500\n60.000000\n28.497500\n23.892500\n55.000000\n1.000000\n\n\nmax\n218.000000\n31.200000\n15.330000\n42.490000\n78.000000\n46.580000\n147.190000\n64.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n\ndf = df.replace('Present',0)\ndf = df.replace('Absent',1)\n\ndf\n\n\n\n\n\n\n\n\nsbp\ntobacco\nldl\nadiposity\nfamhist\ntypea\nobesity\nalcohol\nage\nchd\n\n\nrow.names\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n160\n12.00\n5.73\n23.11\n0\n49\n25.30\n97.20\n52\n1\n\n\n2\n144\n0.01\n4.41\n28.61\n1\n55\n28.87\n2.06\n63\n1\n\n\n3\n118\n0.08\n3.48\n32.28\n0\n52\n29.14\n3.81\n46\n0\n\n\n4\n170\n7.50\n6.41\n38.03\n0\n51\n31.99\n24.26\n58\n1\n\n\n5\n134\n13.60\n3.50\n27.78\n0\n60\n25.99\n57.34\n49\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n459\n214\n0.40\n5.98\n31.72\n1\n64\n28.45\n0.00\n58\n0\n\n\n460\n182\n4.20\n4.41\n32.10\n1\n52\n28.61\n18.72\n52\n1\n\n\n461\n108\n3.00\n1.59\n15.23\n1\n40\n20.09\n26.64\n55\n0\n\n\n462\n118\n5.40\n11.61\n30.79\n1\n64\n27.35\n23.97\n40\n0\n\n\n463\n132\n0.00\n4.82\n33.41\n0\n62\n14.70\n0.00\n46\n1\n\n\n\n\n462 rows × 10 columns\n\n\n\n\n\n\n\n\nfrom matplotlib.pyplot import boxplot, xticks, ylabel, title, show\n\nboxplot(X)\nxticks(range(1,10),attributeNames)\nylabel('cm')\ntitle('SAHeart Disease Data - boxplot')\nshow()"
  },
  {
    "objectID": "projectML.html#loading-data",
    "href": "projectML.html#loading-data",
    "title": "Project 1: South Africa Heart Disease",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure, plot, title, legend, xlabel, ylabel, show\n\n\ndf = pd.read_csv(r\"C:\\Users\\Owner\\Desktop\\on campis\\SAheart.data.txt\", index_col = 'row.names')\n\n\ndf.head(20)\n\n\n\n\n\n\n\n\nsbp\ntobacco\nldl\nadiposity\nfamhist\ntypea\nobesity\nalcohol\nage\nchd\n\n\nrow.names\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n160\n12.00\n5.73\n23.11\nPresent\n49\n25.30\n97.20\n52\n1\n\n\n2\n144\n0.01\n4.41\n28.61\nAbsent\n55\n28.87\n2.06\n63\n1\n\n\n3\n118\n0.08\n3.48\n32.28\nPresent\n52\n29.14\n3.81\n46\n0\n\n\n4\n170\n7.50\n6.41\n38.03\nPresent\n51\n31.99\n24.26\n58\n1\n\n\n5\n134\n13.60\n3.50\n27.78\nPresent\n60\n25.99\n57.34\n49\n1\n\n\n6\n132\n6.20\n6.47\n36.21\nPresent\n62\n30.77\n14.14\n45\n0\n\n\n7\n142\n4.05\n3.38\n16.20\nAbsent\n59\n20.81\n2.62\n38\n0\n\n\n8\n114\n4.08\n4.59\n14.60\nPresent\n62\n23.11\n6.72\n58\n1\n\n\n9\n114\n0.00\n3.83\n19.40\nPresent\n49\n24.86\n2.49\n29\n0\n\n\n10\n132\n0.00\n5.80\n30.96\nPresent\n69\n30.11\n0.00\n53\n1\n\n\n11\n206\n6.00\n2.95\n32.27\nAbsent\n72\n26.81\n56.06\n60\n1\n\n\n12\n134\n14.10\n4.44\n22.39\nPresent\n65\n23.09\n0.00\n40\n1\n\n\n13\n118\n0.00\n1.88\n10.05\nAbsent\n59\n21.57\n0.00\n17\n0\n\n\n14\n132\n0.00\n1.87\n17.21\nAbsent\n49\n23.63\n0.97\n15\n0\n\n\n15\n112\n9.65\n2.29\n17.20\nPresent\n54\n23.53\n0.68\n53\n0\n\n\n16\n117\n1.53\n2.44\n28.95\nPresent\n35\n25.89\n30.03\n46\n0\n\n\n17\n120\n7.50\n15.33\n22.00\nAbsent\n60\n25.31\n34.49\n49\n0\n\n\n18\n146\n10.50\n8.29\n35.36\nPresent\n78\n32.73\n13.89\n53\n1\n\n\n19\n158\n2.60\n7.46\n34.07\nPresent\n61\n29.30\n53.28\n62\n1\n\n\n20\n124\n14.00\n6.23\n35.96\nPresent\n45\n30.09\n0.00\n59\n1\n\n\n\n\n\n\n\n\ndf.shape\n\n(462, 10)\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 462 entries, 1 to 463\nData columns (total 10 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   sbp        462 non-null    int64  \n 1   tobacco    462 non-null    float64\n 2   ldl        462 non-null    float64\n 3   adiposity  462 non-null    float64\n 4   famhist    462 non-null    object \n 5   typea      462 non-null    int64  \n 6   obesity    462 non-null    float64\n 7   alcohol    462 non-null    float64\n 8   age        462 non-null    int64  \n 9   chd        462 non-null    int64  \ndtypes: float64(5), int64(4), object(1)\nmemory usage: 39.7+ KB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nsbp\ntobacco\nldl\nadiposity\ntypea\nobesity\nalcohol\nage\nchd\n\n\n\n\ncount\n462.000000\n462.000000\n462.000000\n462.000000\n462.000000\n462.000000\n462.000000\n462.000000\n462.000000\n\n\nmean\n138.326840\n3.635649\n4.740325\n25.406732\n53.103896\n26.044113\n17.044394\n42.816017\n0.346320\n\n\nstd\n20.496317\n4.593024\n2.070909\n7.780699\n9.817534\n4.213680\n24.481059\n14.608956\n0.476313\n\n\nmin\n101.000000\n0.000000\n0.980000\n6.740000\n13.000000\n14.700000\n0.000000\n15.000000\n0.000000\n\n\n25%\n124.000000\n0.052500\n3.282500\n19.775000\n47.000000\n22.985000\n0.510000\n31.000000\n0.000000\n\n\n50%\n134.000000\n2.000000\n4.340000\n26.115000\n53.000000\n25.805000\n7.510000\n45.000000\n0.000000\n\n\n75%\n148.000000\n5.500000\n5.790000\n31.227500\n60.000000\n28.497500\n23.892500\n55.000000\n1.000000\n\n\nmax\n218.000000\n31.200000\n15.330000\n42.490000\n78.000000\n46.580000\n147.190000\n64.000000\n1.000000\n\n\n\n\n\n\n\n\n\n\n\ndf = df.replace('Present',0)\ndf = df.replace('Absent',1)\n\ndf\n\n\n\n\n\n\n\n\nsbp\ntobacco\nldl\nadiposity\nfamhist\ntypea\nobesity\nalcohol\nage\nchd\n\n\nrow.names\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n160\n12.00\n5.73\n23.11\n0\n49\n25.30\n97.20\n52\n1\n\n\n2\n144\n0.01\n4.41\n28.61\n1\n55\n28.87\n2.06\n63\n1\n\n\n3\n118\n0.08\n3.48\n32.28\n0\n52\n29.14\n3.81\n46\n0\n\n\n4\n170\n7.50\n6.41\n38.03\n0\n51\n31.99\n24.26\n58\n1\n\n\n5\n134\n13.60\n3.50\n27.78\n0\n60\n25.99\n57.34\n49\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n459\n214\n0.40\n5.98\n31.72\n1\n64\n28.45\n0.00\n58\n0\n\n\n460\n182\n4.20\n4.41\n32.10\n1\n52\n28.61\n18.72\n52\n1\n\n\n461\n108\n3.00\n1.59\n15.23\n1\n40\n20.09\n26.64\n55\n0\n\n\n462\n118\n5.40\n11.61\n30.79\n1\n64\n27.35\n23.97\n40\n0\n\n\n463\n132\n0.00\n4.82\n33.41\n0\n62\n14.70\n0.00\n46\n1\n\n\n\n\n462 rows × 10 columns\n\n\n\n\n\n\n\n\nfrom matplotlib.pyplot import boxplot, xticks, ylabel, title, show\n\nboxplot(X)\nxticks(range(1,10),attributeNames)\nylabel('cm')\ntitle('SAHeart Disease Data - boxplot')\nshow()"
  },
  {
    "objectID": "projectML.html#create-matrix-with-values",
    "href": "projectML.html#create-matrix-with-values",
    "title": "Project 1: South Africa Heart Disease",
    "section": "Create matrix with values",
    "text": "Create matrix with values\n\nraw_data = df.values  \nraw_data\n\narray([[1.600e+02, 1.200e+01, 5.730e+00, ..., 9.720e+01, 5.200e+01,\n        1.000e+00],\n       [1.440e+02, 1.000e-02, 4.410e+00, ..., 2.060e+00, 6.300e+01,\n        1.000e+00],\n       [1.180e+02, 8.000e-02, 3.480e+00, ..., 3.810e+00, 4.600e+01,\n        0.000e+00],\n       ...,\n       [1.080e+02, 3.000e+00, 1.590e+00, ..., 2.664e+01, 5.500e+01,\n        0.000e+00],\n       [1.180e+02, 5.400e+00, 1.161e+01, ..., 2.397e+01, 4.000e+01,\n        0.000e+00],\n       [1.320e+02, 0.000e+00, 4.820e+00, ..., 0.000e+00, 4.600e+01,\n        1.000e+00]])\n\n\n\ncols = range(0, 9) \nX = raw_data[:, cols]\nX = np.asarray(X, dtype = np.intc)\nX\n\narray([[160,  12,   5, ...,  25,  97,  52],\n       [144,   0,   4, ...,  28,   2,  63],\n       [118,   0,   3, ...,  29,   3,  46],\n       ...,\n       [108,   3,   1, ...,  20,  26,  55],\n       [118,   5,  11, ...,  27,  23,  40],\n       [132,   0,   4, ...,  14,   0,  46]], dtype=int32)\n\n\n\nattributeNames = np.asarray(df.columns[cols])\nattributeNames\n\narray(['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea',\n       'obesity', 'alcohol', 'age'], dtype=object)\n\n\n\nclassLabels = raw_data[:,-1] # -1 takes the last column\nclassLabels\n\narray([1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n       1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n       0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n       0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n       0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n       0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1.,\n       0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n       0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n       0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n       0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n       1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n       0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n       1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n       0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n       1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n       0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0.,\n       0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n       0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.,\n       0., 0., 1.])\n\n\n\n# Then determine which classes are in the data by finding the set of \n# unique class labels \nclassNames = sorted(set(classLabels))\nclassNames\n\n[0.0, 1.0]\n\n\n\n# We can assign each type of Iris class with a number by making a\n# Python dictionary as so:\nclassDict = dict(zip(classNames,range(len(classNames))))\n\nclassDict\n\n{0.0: 0, 1.0: 1}\n\n\n\ny = np.array([classDict[cl] for cl in classLabels])\nN, M = X.shape\nC = len(classNames)"
  },
  {
    "objectID": "projectML.html#pca",
    "href": "projectML.html#pca",
    "title": "Project 1: South Africa Heart Disease",
    "section": "PCA",
    "text": "PCA\n\nScree plot\n\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import svd\n\n# Subtract mean value from data\nY = X - np.ones((N,1))*X.mean(axis=0)\nY = np.asarray(Y, dtype = np.intc)\n\n# PCA by computing SVD of Y\nU,S,V = svd(Y,full_matrices=False)\n\n# Compute variance explained by principal components\nrho = (S*S) / (S*S).sum() \n\nthreshold = 0.9\n\n# Plot variance explained\nplt.figure()\nplt.plot(range(1,len(rho)+1),rho,'x-')\nplt.plot(range(1,len(rho)+1),np.cumsum(rho),'o-')\nplt.plot([1,len(rho)],[threshold, threshold],'k--')\nplt.title('Variance explained by principal components');\nplt.xlabel('Principal component');\nplt.ylabel('Variance explained');\nplt.legend(['Individual','Cumulative','Threshold'])\nplt.grid()\nplt.show()\n\n\n\n\n\n# PCA by computing SVD of Y\nU,S,Vh = svd(Y,full_matrices=False)\n# scipy.linalg.svd returns \"Vh\", which is the Hermitian (transpose)\n# of the vector V. So, for us to obtain the correct V, we transpose:\nV = Vh.T    \n\n# Project the centered data onto principal component space\nZ = Y @ V\n\n# Indices of the principal components to be plotted\ni = 0\nj = 1\n\n# Plot PCA of the data\nf = figure()\ntitle('SAheart data: PCA')\n#Z = array(Z)\nfor c in range(C):\n    # select indices belonging to class c:\n    class_mask = y==c\n    plot(Z[class_mask,i], Z[class_mask,j], 'o', alpha=.5)\nlegend(classNames)\nxlabel('PC{0}'.format(i+1))\nylabel('PC{0}'.format(j+1))\n\n# Output result to screen\nshow()\n\n\n\n\n\n\nComponent coefficients\n\nU,S,Vh = svd(Y,full_matrices=False)\nV=Vh.T\nN,M = X.shape\n\n# We saw in 2.1.3 that the first 3 components explaiend more than 90\n# percent of the variance. Let's look at their coefficients:\npcs = [0,1,2]\nlegendStrs = ['PC'+str(e+1) for e in pcs]\nc = ['r','g','b']\nbw = .3\nr = np.arange(1,M+1)\nfor i in pcs:    \n    plt.bar(r+i*bw, V[:,i], width=bw)\nplt.xticks(r+bw, attributeNames)\nplt.xlabel('Attributes')\nplt.ylabel('Component coefficients')\nplt.legend(legendStrs)\nplt.grid()\nplt.title('SAHeart: PCA Component Coefficients')\nplt.show()\n\n\n\n\n\n\npcs = [0,1,2]\nfor i in pcs:\n    print('PC',i+1)\n    print(V[:,i])\n\nPC 1\n[-4.05467456e-01 -4.99332923e-02 -4.33822325e-03 -8.92188277e-02\n  1.08420217e-19 -1.03309177e-03 -2.57074859e-02 -8.86057140e-01\n -1.98405622e-01]\nPC 2\n[ 0.78753609  0.037787    0.01997402  0.15836762  0.         -0.05133517\n  0.04947829 -0.46200656  0.36653283]\nPC 3\n[ 4.62516947e-01 -1.02231988e-01 -3.31864416e-02 -2.82673127e-01\n  8.67361738e-19  9.09991032e-02 -6.53004980e-02  9.44181942e-03\n -8.25825114e-01]\n\n\n\n\nClasses projections onto components\n\n\nall_negative_data = Y[y==0,:]\n\n\npcs = [0,1,2]\n\nfor i in pcs:\n    print('Negative observation PC',i+1)\n    print(all_negative_data[i,:])\n    print(' ')\n    print('Projection negative PC', i+1)\n    print(all_negative_data[i,:]@V[:,i])\n    print(' ')\n\nNegative observation PC 1\n[-20  -3  -1   7   0  -1   3 -13   3]\n \nProjection negative PC 1\n18.486392021898233\n \nNegative observation PC 2\n[-6  2  1 11  0  8  4 -2  2]\n \nProjection negative PC 2\n-1.4433141857208174\n \nNegative observation PC 3\n[  3   0  -1  -8   0   5  -5 -14  -4]\n \nProjection negative PC 3\n7.634735282743502\n \n\n\n\n\npcs = [0,1,2]\n\n# Projection and observation of present class\nall_positive_data = Y[y==1,:]\n\nfor i in pcs:\n    print('Positive observation PC',i+1)\n    print(all_positive_data[i,:])\n    print(' ')\n    print('Projection positive PC', i+1)\n    print(all_positive_data[i,:]@V[:,i])\n    print(' ')\n\nPositive observation PC 1\n[21  8  0 -1  0 -4  0 80  9]\n \nProjection positive PC 1\n-81.491153551126\n \nPositive observation PC 2\n[  5  -3   0   3   0   1   2 -14  20]\n \nProjection positive PC 2\n18.145792233414742\n \nPositive observation PC 3\n[31  3  1 13  0 -2  5  7 15]\n \nProjection positive PC 3\n-2.506392363862125\n \n\n\n\n\nGraph of each variable’s standard deviations\n\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import svd\n\nr = np.arange(1,X.shape[1]+1)\nplt.bar(r, np.std(X,0))\nplt.xticks(r, attributeNames)\nplt.ylabel('Standard deviation')\nplt.xlabel('Attributes')\nplt.title('SAHeart: attribute standard deviations')\n\nText(0.5, 1.0, 'SAHeart: attribute standard deviations')"
  },
  {
    "objectID": "projectML.html#pca-with-standardized-dataset",
    "href": "projectML.html#pca-with-standardized-dataset",
    "title": "Project 1: South Africa Heart Disease",
    "section": "PCA with standardized dataset",
    "text": "PCA with standardized dataset\n\n# Subtract the mean from the data and divide by the attribute standard\n# deviation to obtain a standardized dataset:\nY2 = X - np.ones((N, 1))*X.mean(0)\nY2 = Y2*(1/np.std(Y2,0))\n# Here were utilizing the broadcasting of a row vector to fit the dimensions \n# of Y2\n\nU,S,Vh = svd(Y2,full_matrices=False)\nV2=Vh.T\nN2,M2 = X.shape\n\n# Compute variance explained by principal components\nrho = (S*S) / (S*S).sum() \n\nthreshold = 0.9\n\n# Plot variance explained\nplt.figure()\nplt.plot(range(1,len(rho)+1),rho,'x-')\nplt.plot(range(1,len(rho)+1),np.cumsum(rho),'o-')\nplt.plot([1,len(rho)],[threshold, threshold],'k--')\nplt.title('Variance explained by principal components');\nplt.xlabel('Principal component');\nplt.ylabel('Variance explained');\nplt.legend(['Individual','Cumulative','Threshold'])\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n# We saw in 2.1.3 that the first 3 components explaiend more than 90\n# percent of the variance. Let's look at their coefficients:\npcs = [0,1,2,3,4,5]\nlegendStrs = ['PC'+str(e+1) for e in pcs]\nc = ['r','g','b']\nbw = .13\nr = np.arange(1,M2+1)\nplt.figure(figsize=(12,5))\n\nfor i in pcs:\n    plt.subplot()\n    plt.bar(r+i*bw, V2[:,i], width=bw)\nplt.xticks(r+bw, attributeNames)\nplt.xlabel('Attributes')\nplt.ylabel('Component coefficients')\nplt.legend(legendStrs)\nplt.grid()\nplt.title('SAHeart: PCA Component Coefficients')\nplt.show()\n\n\n\n\n\npcs = [0,1,2,3,4,5]\nfor i in pcs:\n    print('PC',i+1)\n    print(V2[:,i])\n\nPC 1\n[-0.32515572 -0.29679111 -0.3350829  -0.51744732  0.19579801  0.01980828\n -0.40122777 -0.11961268 -0.46058264]\nPC 2\n[ 0.24039188  0.46500387 -0.35405988 -0.19050686 -0.00257373 -0.26737564\n -0.39544085  0.54735383  0.19200479]\nPC 3\n[-0.12242603  0.05454167 -0.01107307 -0.07353514 -0.33720033  0.79656035\n  0.06002309  0.45371085 -0.13725262]\nPC 4\n[-0.19986942 -0.00226562  0.1398819  -0.13739481 -0.84038687 -0.20991237\n -0.29848031 -0.24308671  0.15201271]\nPC 5\n[ 0.23030393 -0.62806844 -0.25812766  0.11658561 -0.28681415 -0.3248816\n  0.27938318  0.41369569 -0.18777257]\nPC 6\n[-0.76268467  0.14785984  0.24239452  0.12390782  0.08942369 -0.33498305\n  0.16939436  0.40395795 -0.10126501]\n\n\n\n# Projection and observation of absent class\nall_negative_data = Y2[y==0,:]\n\nfor i in pcs:\n    print('Negative observation PC',i+1)\n    print(all_negative_data[i,:])\n    print(' ')\n    print('Projection Negative PC', i+1)\n    print(all_negative_data[i,:]@V2[:,i])\n    print(' ')\n\nNegative observation PC 1\n[-0.99280637 -0.73592703 -0.58252015  0.9116801  -1.18585412 -0.11256317\n  0.81709446 -0.56138757  0.21818356]\n \nProjection Negative PC 1\n-0.3309216504091384\n \nNegative observation PC 2\n[-0.3090164   0.58019537  0.82650765  1.42627534 -1.18585412  0.9071267\n  1.05466377 -0.11025354  0.1496582 ]\n \nProjection Negative PC 2\n-1.0570016321396427\n \nNegative observation PC 3\n[ 0.17940501  0.14148791 -0.58252015 -1.14670087  0.84327404  0.60121974\n -1.32102936 -0.60239976 -0.33001932]\n \nProjection Negative PC 3\n-0.036229804075974315\n \nNegative observation PC 4\n[-1.18817494 -0.73592703 -0.58252015 -0.76075444 -1.18585412 -0.41847013\n -0.37075211 -0.60239976 -0.94674757]\n \nProjection Negative PC 4\n1.4597850634610399\n \nNegative observation PC 5\n[-0.99280637 -0.73592703 -1.52187203 -1.91859373  0.84327404  0.60121974\n -1.08346005 -0.68442413 -1.7690519 ]\n \nProjection Negative PC 5\n-0.288130401840477\n \nNegative observation PC 6\n[-0.3090164  -0.73592703 -1.52187203 -1.01805206  0.84327404 -0.41847013\n -0.60832142 -0.68442413 -1.90610262]\n \nProjection Negative PC 6\n-0.33908424672744053\n \n\n\n\n# Projection and observation of present class\nall_positive_data = Y2[y==1,:]\n\nfor i in pcs:\n    print('Positive observation PC',i+1)\n    print(all_positive_data[i,:])\n    print(' ')\n    print('Projection positive PC', i+1)\n    print(all_positive_data[i,:]@V2[:,i])\n    print(' ')\n\nPositive observation PC 1\n[ 1.05856354  1.89631778  0.35683172 -0.2461592  -1.18585412 -0.41847013\n -0.1331828   3.29375784  0.62933573]\n \nProjection positive PC 1\n-1.7700787625087326\n \nPositive observation PC 2\n[ 0.27708929 -0.73592703 -0.11284422  0.39708486  0.84327404  0.19334379\n  0.57952514 -0.60239976  1.3831147 ]\n \nProjection positive PC 2\n-0.6584875465166402\n \nPositive observation PC 3\n[ 1.54698495  0.79954911  0.82650765  1.68357296 -1.18585412 -0.21453216\n  1.29223308  0.29986832  1.04048789]\n \nProjection positive PC 3\n0.021054017788524754\n \nPositive observation PC 4\n[-0.21133212  2.11567151 -0.58252015  0.26843605 -1.18585412  0.70318873\n -0.1331828   1.65327043  0.42375964]\n \nProjection positive PC 4\n0.4703292272987059\n \nPositive observation PC 5\n[-1.18817494  0.14148791 -0.11284422 -1.40399849 -1.18585412  0.9071267\n -0.60832142 -0.43835102  1.04048789]\n \nProjection positive PC 5\n-0.9983260807297992\n \nPositive observation PC 6\n[-0.3090164  -0.73592703  0.35683172  0.65438248 -1.18585412  1.62090962\n  1.05466377 -0.68442413  0.69786109]\n \nProjection positive PC 6\n-0.5230688965844813"
  },
  {
    "objectID": "projectML.html#visualization-of-dataset",
    "href": "projectML.html#visualization-of-dataset",
    "title": "Project 1: South Africa Heart Disease",
    "section": "Visualization of dataset",
    "text": "Visualization of dataset\n\nDistribution of variables\n\nimport pandas as pd\n\n\n\nNon-stardardized data\n\n\nfrom matplotlib.pyplot import figure, subplot, hist, xlabel, ylim, show\nimport numpy as np\n\n\nfigure(figsize=(8,7))\nu = int(np.floor(np.sqrt(M))); v = int(np.ceil(float(M)/u))\nfor i in range(M):\n    subplot(u,v,i+1)\n    hist(X[:,i], color=(0.2, 1-i*0.1, 0.4))\n    xlabel(attributeNames[i])\n    ylim(0,N/2)\n    \nshow()\n\n\n\n\n\n\nfrom matplotlib.pyplot import boxplot, xticks, ylabel, title, show\n\nboxplot(X)\nxticks(range(1,10),attributeNames)\nylabel('cm')\ntitle('SAHeart Disease Data - boxplot')\nshow()\n\n\n\n\n\n\n\nfrom matplotlib.pyplot import (figure, subplot, boxplot, title, xticks, ylim, \n                               show)\n\nclassNamesp= ['Negative','Positive']\n\nfigure(figsize=(14,7))\nfor c in range(C):\n    subplot(1,C,c+1)\n    class_mask = (y==c) \n    \n    boxplot(X[class_mask,:])\n    #title('Class: {0}'.format(classNames[c]))\n    title('Class: '+classNamesp[c])\n    xticks(range(1,len(attributeNames)+1), [a[:7] for a in attributeNames], rotation=45)\n    y_up = X.max()+(X.max()-X.min())*0.1; y_down = X.min()-(X.max()-X.min())*0.1\n    ylim(y_down, y_up)\n\nshow()\n\n\n\n\n\nfrom matplotlib.pyplot import (figure, subplot, plot, xlabel, ylabel, \n                               xticks, yticks,legend,show)\n\n\nfigure(figsize=(12,10))\nfor m1 in range(M):\n    for m2 in range(M):\n        subplot(M, M, m1*M + m2 + 1)\n        for c in range(C):\n            class_mask = (y==c)\n            plot(np.array(X[class_mask,m2]), np.array(X[class_mask,m1]), '.')\n            if m1==M-1:\n                xlabel(attributeNames[m2])\n            else:\n                xticks([])\n            if m2==0:\n                ylabel(attributeNames[m1])\n            else:\n                yticks([])\n            #ylim(0,X.max()*1.1)\n            #xlim(0,X.max()*1.1)\nlegend(classNames)\n\nshow()"
  },
  {
    "objectID": "SalamiProject.html",
    "href": "SalamiProject.html",
    "title": "WebTest",
    "section": "",
    "text": "import glob\nimport numpy as np\nimport os\nimport scipy.io as sio\nimport numpy as np\nfrom scipy import misc\n\nimport sys\nsys.path.insert(1, '/Users/Owner/Downloads/salami/')\n\n\npip install nbconvert\n\nCollecting nbconvert\n  Downloading nbconvert-7.3.1-py3-none-any.whl (284 kB)\n     -------------------------------------- 284.1/284.1 KB 4.4 MB/s eta 0:00:00\nCollecting defusedxml\n  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\nCollecting beautifulsoup4\n  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n     -------------------------------------- 143.0/143.0 KB 8.3 MB/s eta 0:00:00\nCollecting mistune&lt;3,&gt;=2.0.3\n  Downloading mistune-2.0.5-py2.py3-none-any.whl (24 kB)\nCollecting jinja2&gt;=3.0\n  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n     ---------------------------------------- 133.1/133.1 KB ? eta 0:00:00\nCollecting markupsafe&gt;=2.0\n  Downloading MarkupSafe-2.1.2-cp310-cp310-win_amd64.whl (16 kB)\nRequirement already satisfied: pygments&gt;=2.4.1 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nbconvert) (2.12.0)\nCollecting nbclient&gt;=0.5.0\n  Downloading nbclient-0.7.4-py3-none-any.whl (73 kB)\n     ---------------------------------------- 73.1/73.1 KB 3.9 MB/s eta 0:00:00\nCollecting pandocfilters&gt;=1.4.1\n  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\nCollecting nbformat&gt;=5.1\n  Downloading nbformat-5.8.0-py3-none-any.whl (77 kB)\n     ---------------------------------------- 77.4/77.4 KB ? eta 0:00:00\nCollecting jupyterlab-pygments\n  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\nRequirement already satisfied: jupyter-core&gt;=4.7 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nbconvert) (4.10.0)\nRequirement already satisfied: traitlets&gt;=5.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nbconvert) (5.1.1)\nCollecting tinycss2\n  Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\nCollecting bleach\n  Downloading bleach-6.0.0-py3-none-any.whl (162 kB)\n     -------------------------------------- 162.5/162.5 KB 9.5 MB/s eta 0:00:00\nRequirement already satisfied: packaging in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nbconvert) (21.3)\nRequirement already satisfied: pywin32&gt;=1.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-core&gt;=4.7-&gt;nbconvert) (304)\nCollecting traitlets&gt;=5.0\n  Downloading traitlets-5.9.0-py3-none-any.whl (117 kB)\n     ---------------------------------------- 117.4/117.4 KB ? eta 0:00:00\nCollecting jupyter-core&gt;=4.7\n  Downloading jupyter_core-5.3.0-py3-none-any.whl (93 kB)\n     ---------------------------------------- 93.2/93.2 KB ? eta 0:00:00\nRequirement already satisfied: jupyter-client&gt;=6.1.12 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nbclient&gt;=0.5.0-&gt;nbconvert) (7.3.0)\nCollecting platformdirs&gt;=2.5\n  Downloading platformdirs-3.5.0-py3-none-any.whl (15 kB)\nCollecting fastjsonschema\n  Downloading fastjsonschema-2.16.3-py3-none-any.whl (23 kB)\nCollecting jsonschema&gt;=2.6\n  Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n     ---------------------------------------- 90.4/90.4 KB 5.0 MB/s eta 0:00:00\nCollecting soupsieve&gt;1.2\n  Downloading soupsieve-2.4.1-py3-none-any.whl (36 kB)\nCollecting webencodings\n  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\nRequirement already satisfied: six&gt;=1.9.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bleach-&gt;nbconvert) (1.16.0)\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging-&gt;nbconvert) (3.0.8)\nCollecting attrs&gt;=17.4.0\n  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n     ---------------------------------------- 61.2/61.2 KB 3.2 MB/s eta 0:00:00\nCollecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,&gt;=0.14.0\n  Downloading pyrsistent-0.19.3-cp310-cp310-win_amd64.whl (62 kB)\n     ---------------------------------------- 62.7/62.7 KB 3.3 MB/s eta 0:00:00\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-client&gt;=6.1.12-&gt;nbclient&gt;=0.5.0-&gt;nbconvert) (2.8.2)\nRequirement already satisfied: entrypoints in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-client&gt;=6.1.12-&gt;nbclient&gt;=0.5.0-&gt;nbconvert) (0.4)\nRequirement already satisfied: nest-asyncio&gt;=1.5.4 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-client&gt;=6.1.12-&gt;nbclient&gt;=0.5.0-&gt;nbconvert) (1.5.5)\nRequirement already satisfied: tornado&gt;=6.0 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-client&gt;=6.1.12-&gt;nbclient&gt;=0.5.0-&gt;nbconvert) (6.1)\nRequirement already satisfied: pyzmq&gt;=22.3 in c:\\users\\owner\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-client&gt;=6.1.12-&gt;nbclient&gt;=0.5.0-&gt;nbconvert) (22.3.0)\nInstalling collected packages: webencodings, mistune, fastjsonschema, traitlets, tinycss2, soupsieve, pyrsistent, platformdirs, pandocfilters, markupsafe, jupyterlab-pygments, defusedxml, bleach, attrs, jupyter-core, jsonschema, jinja2, beautifulsoup4, nbformat, nbclient, nbconvert\n  Attempting uninstall: traitlets\n    Found existing installation: traitlets 5.1.1\n    Uninstalling traitlets-5.1.1:\n      Successfully uninstalled traitlets-5.1.1\n  Attempting uninstall: jupyter-core\n    Found existing installation: jupyter-core 4.10.0\n    Uninstalling jupyter-core-4.10.0:\n      Successfully uninstalled jupyter-core-4.10.0\nSuccessfully installed attrs-23.1.0 beautifulsoup4-4.12.2 bleach-6.0.0 defusedxml-0.7.1 fastjsonschema-2.16.3 jinja2-3.1.2 jsonschema-4.17.3 jupyter-core-5.3.0 jupyterlab-pygments-0.2.2 markupsafe-2.1.2 mistune-2.0.5 nbclient-0.7.4 nbconvert-7.3.1 nbformat-5.8.0 pandocfilters-1.5.0 platformdirs-3.5.0 pyrsistent-0.19.3 soupsieve-2.4.1 tinycss2-1.2.1 traitlets-5.9.0 webencodings-0.5.1\nNote: you may need to restart the kernel to use updated packages.\n\n\n  WARNING: The scripts jupyter-migrate.exe, jupyter-troubleshoot.exe and jupyter.exe are installed in 'c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n  WARNING: The script jsonschema.exe is installed in 'c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n  WARNING: The script jupyter-trust.exe is installed in 'c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n  WARNING: The script jupyter-execute.exe is installed in 'c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n  WARNING: The scripts jupyter-dejavu.exe and jupyter-nbconvert.exe are installed in 'c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the 'c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n\n\n\nimport helpFunctions as hf\n\ndirIn = \"C:\\\\Users\\\\Owner\\\\Downloads\\\\salami\\\\\"\n\n#Uploading the soreted images by name into an array \nmultiIm, annotationIm = hf.loadMulti('multispectral_day01.mat' , 'annotation_day01.png', dirIn)\n\n\nInvestigate the data. A manual segmentation has been made of fat and meat in a single image for each day – have a look at day 1. (On DTU Learn code is available for loading data, making histograms, etc.).\n\nOutput # multiIm - multispectral image # annotationIm - image with annotation mask of size r x c x 3. Layer 1 is # the salami annotation (both fat and meat, layer 2 is the fat, and # layer 3 is the meat. The pixel value is 1 in the annotation and 0 # elsewhere.\n\n# Extracting the fat and meat from the annotation numpy array \nsalami = annotationIm[:,:,0]\nfat = annotationIm[:,:,1]\nmeat= annotationIm[:,:,2]\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(salami)\nplt.show()\nprint(fat)\n\n\n\n\n[[False False False ... False False False]\n [False False False ... False False False]\n [False False False ... False False False]\n ...\n [False False False ... False False False]\n [False False False ... False False False]\n [False False False ... False False False]]\n\n\n\n# Extracting the fat and meat from the annotation numpy array \nsalami = annotationIm[:,:,0]\nfat = annotationIm[:,:,1]\nmeat= annotationIm[:,:,2]\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(annotationIm * 255)\nplt.show()\nprint(multiIm.shape)\n\nplt.imshow(multiIm[:,:,0],cmap=\"gray\")\nplt.show()\n\nplt.imshow(fat*255, cmap=\"gray\")\nplt.show()\n\nplt.imshow(fat * multiIm[:,:,0], cmap=\"gray\")\nplt.show()\n\nrr, cc = np.where(fat)\nfat_pix_spec0 = multiIm[rr, cc, :]\nrr.shape, cc.shape\nfat_pix_spec0.shape\n\n\n\n\n(514, 514, 19)\n\n\n\n\n\n\n\n\n\n\n\n(756, 19)\n\n\n\n\n\n\nimport helpFunctions as hf \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport imageio as imio\n    \n\n## Example of loading a multi spectral image\n\n\nmultiIm, annotationIm = hf.loadMulti('multispectral_day20.mat' , 'annotation_day20.png', dirIn)\n\n# multiIm is a multi spectral image - the dimensions can be seen by\nmultiIm.shape\n\n## Show image óf spectral band 7\n\nplt.imshow(multiIm[:,:,6])\nplt.show()\n\n## annotationIm is a binary image with 3 layers\n\nannotationIm.shape\n\n## In each layer we have a binary image:\n# 0 - background with salami\n# 1 fat annotation\n# 2 meat annotation\n\n# Here we show the meat annotation\n\nplt.imshow(annotationIm[:,:,2])\nplt.show()\n\n\n## The function getPix extracts the multi spectral pixels from the annotation\n\n# Here is an example with meat- and fat annotation\n[fatPix, fatR, fatC] = hf.getPix(multiIm, annotationIm[:,:,1]);\n[meatPix, meatR, meatC] = hf.getPix(multiIm, annotationIm[:,:,2]);\n\n# Here we plot the mean values for pixels with meat and fat respectively\nplt.plot(np.mean(meatPix,0),'b')\nplt.plot(np.mean(fatPix,0),'r')\nplt.show()\n\n\n\n\n## The function showHistogram makes a histogram that is returned as a vector\n\n# Here is an example - last argument tells the function to plot the histogram for meat and fat\nh = hf.showHistograms(multiIm, annotationIm[:,:,1:3], 2, 1)\n\n\n## The histogram is also in h\n# But not truncated like in the plot. If we wnat to avoid plotting all 256 dimensions, \n# we can do like below, and only plot the first 50 values\n\nplt.plot(h[0:50:,:])\nplt.show()\n\n## The function setImagePix produces a colour image\n# where the pixel coordinates are given as input\n\n# Load RGB image\nimRGB = imio.imread(dirIn + 'color_day20.png')\n\n# Pixel coordinates for the fat annotation\n[fatPix, fatR, fatC] = hf.getPix(multiIm, annotationIm[:,:,1])\n\n# Concatenate the pixel coordinates to a matrix\npixId = np.stack((fatR, fatC), axis=1)\n\n# Make the new images\nrgbOut = hf.setImagePix(imRGB, pixId)\nplt.imshow(rgbOut)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9644\\888994786.py:66: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  imRGB = imio.imread(dirIn + 'color_day20.png')\n\n\n\n\n\n\n\n# Here is an example with meat- and fat annotation\n[fatPix, fatR, fatC] = hf.getPix(multiIm, annotationIm[:,:,1]);\n[meatPix, meatR, meatC] = hf.getPix(multiIm, annotationIm[:,:,2]);\n\n# Here we plot the mean values for pixels with meat and fat respectively\nplt.plot(np.mean(meatPix,0),'b')\nplt.plot(np.mean(fatPix,0),'r')\nplt.show()\n\nspectrum = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19], dtype = np.int64)\nplt.errorbar(spectrum, np.mean(meatPix,0), np.std(meatPix,0), linestyle='None', marker='^')\n\nplt.show()\n\n\nplt.figure()\nplt.errorbar(spectrum, mean_fat, signific*sd_fat, capsize=capsize, ecolor=ecolor[0], label='fat', color=color[0])\nplt.errorbar(spectrum ,mean_meat, signific*sd_meat, capsize=capsize, ecolor=ecolor[1],  label='meat', color=color[1])\nplt.legend()\nplt.grid()\nplt.xlabel(\"Wave Band\")\nplt.ylabel(\"Mean Intensity\")\nplt.title(title)\nplt.xticks(spectrum)\n#if xtick_align: plt.xticks(spectrum)\nplt.show()\n\n\nshape = multiIm.shape\nshape[2]\n\n19\n\n\n\nwaves = []\nfor i in range(shape[2]):\n    \n\n\n# Here is an example with meat- and fat annotation\n[fatPix, fatR, fatC] = hf.getPix(multiIm, annotationIm[:,:,1]);\n[meatPix, meatR, meatC] = hf.getPix(multiIm, annotationIm[:,:,2]);\nwaves = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n\nerror1 = np.std(meatPix,0)\nerror2 = np.std(fatPix,0)\n# Here we plot the mean values for pixels with meat and fat respectively\nplt.plot(np.mean(meatPix,0),'b')\nplt.plot(np.mean(fatPix,0),'r')\n#plt.show()\nplt.errorbar(waves, np.mean(meatPix,0), 2*np.std(meatPix,0), fmt = \"o\", ecolor = 'black' )\nplt.errorbar(waves, np.mean(fatPix,0), 2*np.std(fatPix,0), fmt = \"o\", ecolor = 'black' )\nplt.xticks(waves)\nplt.show()\n\n\n\n\n\nplt.figure()\nplt.errorbar(x=waves, y=np.mean(meatPix,0), yerr=np.std(meatPix,0), ecolor = 'black', capsize = 5, label ='Meat', color = 'red')\nplt.errorbar(x=waves, y=np.mean(fatPix,0), yerr=np.std(fatPix,0), ecolor = 'black', capsize = 5, label = 'Fat', color = 'blue')\nplt.legend()\n\nplt.xlabel(\"Spectral Wave\")\nplt.ylabel(\"Mean and respective Standard Deviation\")\nplt.title(\"Spectral Distribution of Day 1\")\nplt.xticks(waves)\nplt.show()\n\n\n\n\n\nlol = np.mean(meatPix,0)\nlol.shape\n\n(19,)\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport skimage\n\n#spectre = np.array([410, 438, 450, 468, 502, 519, 572, 591, 625, 639, 653, 695, 835, 863, 880, 913, 929, 940, 955])\n\nspectre = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19], dtype = np.int64)\n\ndef compare_image(image, day = 1, title=\"Titel\"):\n\n    # if np.shape(image1) != np.shape(image2):\n    #     print(\"Shapes of images mismatch!\")\n    #     return None\n\n    try:\n        color_im = skimage.io.imread(\"color_day\" + \"0\"*(day&lt;10) + str(day) + \".png\")\n    except FileNotFoundError:\n        print(\"No image found for day \" + str(day))\n        color_im = np.ones(np.shape(image))\n\n    fig, [ax1,ax2] = plt.subplots(1,2, figsize=(8,4))\n    fig.suptitle(title)\n    ax1.imshow(color_im)\n    ax2.imshow(image)\n    plt.show()\n\n\n# Here is an example with meat- and fat annotation\n[fatPix, fatR, fatC] = hf.getPix(multiIm, annotationIm[:,:,1]);\n[meatPix, meatR, meatC] = hf.getPix(multiIm, annotationIm[:,:,2]);\n\n# Here we plot the mean values for pixels with meat and fat respectively\nplt.plot(np.mean(meatPix,0),'b')\nplt.plot(np.mean(fatPix,0),'r')\nplt.show()\n\n\n# Here is an example - last argument tells the function to plot the histogram for meat and fat\nh = hf.showHistograms(multiIm, annotationIm[:,:,1:3], 2, 1)\n\n\ncompare_spectrum(fat_vector_means, fat_vector_sd, meat_vector_means, meat_vector_sd, title = \"Mean Intensity of Sausage at Day 1\")\n\ndef compare_spectrum(mean_fat, sd_fat, mean_meat, sd_meat, signific = 2, ecolor = [\"black\", \"black\"], capsize = 5, color = [\"orange\", \"blue\"], spectrum = spectre, title = \"Titel\", xtick_align = False):\n    \n\n    plt.figure()\n    plt.errorbar(spectrum, mean_fat, signific*sd_fat, capsize=capsize, ecolor=ecolor[0], label='fat', color=color[0])\n    plt.errorbar(spectrum ,mean_meat, signific*sd_meat, capsize=capsize, ecolor=ecolor[1],  label='meat', color=color[1])\n    plt.legend()\n    plt.grid()\n    plt.xlabel(\"Wave Band\")\n    plt.ylabel(\"Mean Intensity\")\n    plt.title(title)\n    plt.xticks(spectrum)\n    #if xtick_align: plt.xticks(spectrum)\n    plt.show()\n\n\nimport helpFunctions as hf\n\ndirIn = \"C:\\\\Users\\\\Owner\\\\Downloads\\\\salami\\\\\"\ndays = [1,6,13,20,28]\nmultiImD = []\nannotationImD = []\ncolor_imD = []\nfat = []\nmeat = []\nnFat = []\nnMeat =[]\n\nindex_background = []\n\nfor d in days:\n    str_day = \"0\"*(d&lt;10)+str(d)\n    imName= (\"multispectral_day\" + str_day + \".mat\")\n    annotationName = (\"annotation_day\" + str_day + \".png\")\n    multiIm, annotationIm = hf.loadMulti(imName, annotationName)\n    multiImD.append(multiIm)\n    annotationImD.append(annotationIm)\n    \n    [fatPix, fatR, fatC] = hf.getPix(multiIm, annotationIm[:,:,1])\n    [meatPix, meatR, meatC] = hf.getPix(multiIm, annotationIm[:,:,2])\n    \n    fat.append(fatPix)\n    meat.append(meatPix)\n    index_background.append((annotationIm[:,:,0]+annotationIm[:,:,1]+annotationIm[:,:,2])==0)\n\nfatM = np.asarray(fat)\nmeatM = np.asarray(meat)\n\nfor i in range(5):\n    nFat.append(np.size(fatM[i][:,0]))\n    nMeat.append(np.size(meatM[i][:,0]))\n\nC:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9644\\2665790727.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  fatM = np.asarray(fat)\nC:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_9644\\2665790727.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  meatM = np.asarray(meat)\n\n\n\nmaskIm = annotationIm[:,:,1]\nnMask = maskIm.sum()\nclPix = np.zeros([nMask, multiIm.shape[2]])\nr, c = np.where(maskIm == 1)\nclPix = multiIm[r,c,:]\n\n\nhf.getPix(multiIm, annotationIm[:,:,1])\n\n[fatPix, fatR, fatC] = hf.getPix(multiIm, annotationIm[:,:,1])\n\n\nfat[0].shape\n\n(756, 19)\n\n\n\nclPix.shape\n\n(756, 19)\n\n\n\nfat_vector.shape\n\n(756, 19)\n\n\n\n# Here is an example with meat- and fat annotation\n[fatPix, fatR, fatC] = hf.getPix(multiIm, annotationIm[:,:,1]);\n[meatPix, meatR, meatC] = hf.getPix(multiIm, annotationIm[:,:,2]);\n\nmaskIm = annotationIm[:,:,1]\nnMask = maskIm.sum()\nclPix = np.zeros([nMask, multiIm.shape[2]])\nclPix = multiIm[r,c,:]\n\n\n# Here we plot the mean values for pixels with meat and fat respectively\nplt.plot(np.mean(meatPix,0),'b')\nplt.plot(np.mean(fatPix,0),'r')\nplt.show()\n\nplt.figure()\nplt.errorbar(spectrum, mean_fat, signific*sd_fat, capsize=capsize, ecolor=ecolor[0], label='fat', color=color[0])\nplt.errorbar(spectrum ,mean_meat, signific*sd_meat, capsize=capsize, ecolor=ecolor[1],  label='meat', color=color[1])\nplt.legend()\nplt.grid()\nplt.xlabel(\"Wave Band\")\nplt.ylabel(\"Mean Intensity\")\nplt.title(title)\nplt.xticks(spectrum)\n#if xtick_align: plt.xticks(spectrum)\nplt.show()\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import  LinearLocator\nimport matplotlib.animation as anim\n\n\n# Opgave 1.2)\nmu1 = 175.5; mu2 = 162.9\nsigma = 6.7\nvar = sigma**2\n\nx = np.arange(140,200,0.1)\n\nf1 = 1/(np.sqrt(2*np.pi)*sigma) *np.exp(-1/2 * 1/var * np.power(x-mu1,2))\nf2 = 1/(np.sqrt(2*np.pi)*sigma) *np.exp(-1/2 * 1/var * np.power(x-mu2,2))\n\n\nfig1 = plt.figure()\n\nplt.plot(x,f1, label = \"Male\")\nplt.plot(x,f2, label = \"Female\")\nplt.title(\"Gaussian Distributions\")\nplt.xlabel(\"Height [cm]\")\nplt.ylabel(\"Probability Density\")\nplt.legend()\nplt.show()\n\n# opgave 1.3)\nfig2 = plt.figure()\n\nmale = f1/f2\nfemale = f2/f1\nprobmale = male/(male+1)\nprobfemale = female/(female+1)\n\nplt.plot(x,probmale, label = \"Male\")\nplt.plot(x,probfemale, label = \"Female\")\nplt.title(\" Distribution Given Height\")\nplt.xlabel(\"Height [cm]\")\nplt.ylabel(\"Probability Distribution\")\nplt.ylim([0,1])\nplt.legend()\nplt.grid()\nplt.show()\n\n# Opgave 1.4)\n# svaret er (mu1 + mu2)/2\nprint(f\"Critical point: {(mu1+mu2)/2}\")\n\n\n\n\n# Opgave 1.5)\nax = plt.figure().add_subplot(projection='3d')\n\n# Make data.\nx = np.arange(-5,5,0.05)\nx1, x2 = np.meshgrid(x, x)\n\nmu1 = 0; mu2 = 1\nsigma1 = 2; sigma2 = 3\nvar1 = sigma1**2; var2 = sigma2**2\n\ng = 1/(2*np.pi) * 1/(sigma1*sigma2) * np.exp(-1/2 * ( 1/var1*np.power(x1 - mu1,2) + 1/var2*np.power(x2 - mu2,2)))\n\nax.plot_surface(x1,x2,g, cmap = \"gist_ncar\", linewidth=0)\nplt.show()\n\n\n# Opgave 1.6)\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\n#plt.rcParams['text.usetex'] = True\n\nx = np.arange(-6,6,0.05)\nx1, x2 = np.meshgrid(x, x)\n# Make data.\ncovar = sigma1*sigma2; rho = 2/3\n\n\ndef g_cor(rho):\n    return 1/(2*np.pi) * 1/(sigma1*sigma2) * 1/np.sqrt(1 - rho**2) * np.exp(-1/2 * 1/(1 - rho**2) * ( 1/var1*np.power(x1 - mu1,2) - 2*rho/covar*(x1 - mu1)*(x2 - mu2) + 1/var2*np.power(x2 - mu2,2)))\ng_cors = g_cor(0)\n\nsurf = ax.plot_surface(x1,x2,g_cors, cmap = \"gist_ncar\", linewidth=0)\nax.zaxis.set_major_locator(LinearLocator(6))\nax.set_xlabel(r'$x_1$')\nax.set_ylabel(r'$x_2$')\nax.set_zlabel(r'$P(x_1,x_2)$')\n\nax.set_zlim([0,0.03])\n\n# Make an animation\nname = \"Covariant Gaussian Distribution\"\nfps = 24\nmovie_writer = anim.writers['ffmpeg']\nmetadata = dict(title=name)\nmovie = movie_writer(fps=fps, metadata=metadata)\n\nwith movie.saving(fig, name + \".mp4\", 100):\n    for i in range(100):\n        fig.suptitle(r'$\\rho$ = ' + f'{0.01*i:0.2f}', fontsize=16)\n        new_data = g_cor(0.01*i)\n        surf.remove()\n        surf = ax.plot_surface(x1,x2,new_data, cmap = \"gist_ncar\", linewidth=0)\n        plt.draw()\n        movie.grab_frame()\n\nnew_data = g_cor(2/3)\nsurf.remove()\nsurf = ax.plot_surface(x1,x2,new_data, cmap = \"gist_ncar\", linewidth=0)\nplt.show()\n\n\n\n\n\n\n\nCritical point: 169.2\n\n\n\n\n\nRuntimeError: Requested MovieWriter (ffmpeg) not available\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os \nimport helpFunctions as hf\n\n\n\n\n\n\n\ndef load_day_data(day):\n    str_day = \"0\"*(day&lt;10)+str(day)\n    imName = \"multispectral_day\" + str_day + \".mat\"\n    annotationName = \"annotation_day\" + str_day + \".png\"\n    [multiIm, annotationIm] = hf.loadMulti(imName, annotationName)\n    \n    true_fat = annotationIm[:,:,1]\n    true_meat = annotationIm[:,:,2]\n    index_background = (annotationIm[:,:,0] + annotationIm[:,:,1] + annotationIm[:,:,2]) == 0\n\n    fat_vector = multiIm[true_fat, :]\n    meat_vector = multiIm[true_meat, :]\n\n    return multiIm, true_fat, true_meat, index_background, fat_vector, meat_vector\n\n# ---------------------------------------------------------------------\n\ndef compute_errorrate(true_fat, true_meat, index_fat, index_meat, method_name = None):\n\n    errorrate = (np.sum(index_fat[true_fat] == 0) + np.sum(index_meat[true_meat] == 0)) / (np.sum(true_fat) + np.sum(true_meat))\n\n    if method_name != None: \n        error_fat = np.sum(index_fat[true_fat] == 0)/np.sum(true_fat)\n        error_meat = np.sum(index_meat[true_meat] == 0)/np.sum(true_meat)\n        \n        print(f\"{method_name} has an error rate of {error_fat*100:.2f}% for fat and {error_meat*100:.2f}% for meat for a total of {errorrate*100:0.2f}%\")\n    \n    return errorrate\n\n# ---------------------------------------------------------------------\n\ndef construct_entire_im(index_fat, index_meat, index_background):\n    entire_im = np.zeros(np.shape(index_fat))\n    entire_im[index_fat] = 1\n    entire_im[index_meat] = 2\n    entire_im[index_background] = 0\n    return entire_im\n# ---------------------------------------------------------------------\n\ndef train_multivariate_linear_discriminant(multiIm, fat_vector, meat_vector):\n\n    # Preprocessing\n    m_fat = np.size(fat_vector,0) - 1\n    m_meat = np.size(meat_vector,0) - 1\n\n    mean_fat = np.mean(fat_vector,axis=0)\n    mean_meat = np.mean(meat_vector,axis=0)\n\n    k = [np.size(multiIm,0), np.size(multiIm,1)]\n\n    # Processing\n    Sigma_fat = np.cov(fat_vector, rowvar = False)\n    Sigma_meat = np.cov(meat_vector, rowvar = False)\n\n    Sigma = 1/(m_fat + m_meat) * (m_fat*Sigma_fat + m_meat * Sigma_meat)\n\n    Sigma_inv = np.linalg.inv(Sigma)\n\n    mu_fat = np.copy(mean_fat)\n    mu_meat = mean_meat\n\n    return Sigma_inv, mu_fat, mu_meat, k\n\n# ---------------------------------------------------------------------\n\ndef compute_multivariate_linear_discriminant(multiIm, Sigma_inv, mu_fat, mu_meat, k, pi = 1):\n\n    XT = XT = np.reshape(multiIm,(k[0]*k[1],np.size(mu_fat)))\n\n    S_fat = (np.linalg.multi_dot([XT, Sigma_inv, mu_fat]) - 1/2 * np.linalg.multi_dot([mu_fat.T, Sigma_inv, mu_fat]) + np.log(pi)).reshape(k)\n    S_meat = (np.linalg.multi_dot([XT, Sigma_inv, mu_meat]) - 1/2 * np.linalg.multi_dot([mu_meat.T, Sigma_inv, mu_meat]) + np.log(pi)).reshape(k)\n    \n    index_fat = (S_fat &gt; S_meat)\n    index_meat = (S_fat &lt;= S_meat)\n\n    return index_fat, index_meat\n\n# ---------------------------------------------------------------------\n\ndef train_threshold_value(fat_vector, meat_vector):\n\n    # Preprocessing\n    m_fat = np.size(fat_vector,0)\n    m_meat = np.size(meat_vector,0)\n    mean_fat = np.mean(fat_vector,axis=0)\n    mean_meat = np.mean(meat_vector,axis=0)\n\n    t = (mean_fat + mean_meat)/2\n\n    error_rate = ( np.sum(fat_vector &lt; t, axis=0) + np.sum(meat_vector &gt; t, axis=0)  ) / (m_fat + m_meat)\n    best_band = np.argmin(error_rate)\n\n    return t, best_band\n\n# ---------------------------------------------------------------------\n\ndef compute_threshold_value(multiIm, t, best_band):\n    best_band_im = multiIm[:,:,best_band]\n    index_fat = (best_band_im &gt; t[best_band])\n    index_meat = (best_band_im &lt;= t[best_band])\n\n    return index_fat, index_meat\n\n\nif __name__ == \"__main__\":\n    multiIm, true_fat, true_meat, index_background, fat_vector, meat_vector = load_day_data(1)\n\n    # Threshold Value\n    t, best_band = train_threshold_value(fat_vector, meat_vector)\n    index_fat, index_meat = compute_threshold_value(multiIm,t, best_band)\n    errorrate_1 = compute_errorrate(true_fat, true_meat, index_fat, index_meat, method_name = \"TV\")\n\n    # Multivariate Linear Discriminant\n    Sigma_inv, mu_fat, mu_meat, k = train_multivariate_linear_discriminant(multiIm, fat_vector, meat_vector)\n    index_fat, index_meat = compute_multivariate_linear_discriminant(multiIm, Sigma_inv, mu_fat, mu_meat, k)\n    errorrate_2 = compute_errorrate(true_fat, true_meat, index_fat, index_meat, method_name = \"MLD\")\n\n    #Evaluation\n    print(\"The best method is \" + \"TV\"*int(errorrate_1 &lt; errorrate_2) + \"MLD\"*int(errorrate_1 &gt; errorrate_2))\n\nTV has an error rate of 0.93% for fat and 0.10% for meat for a total of 0.46%\nMLD has an error rate of 1.32% for fat and 0.00% for meat for a total of 0.58%\nThe best method is TV\n\n\n\nSigma_inv.shape\n\n(19, 19)\n\n\n\n\n\nnp.reshape(multiIm,(k[0]*k[1],np.size(mu_fat))).shape\n\n(264196, 19)\n\n\n\nXT.shape\n\n(264196, 19)\n\n\n\n# CAMBIE FATVECTOR A fat Y MEAT VECTOR A meat !!!!!\n\n# M FAT A NF Y M MEAT A NM !!!!\n\n\nWhat is the spectral distribution of meat and fat respectively?\n\n\nfat_vector.shape\n\n(756, 19)\n\n\n\nfat_vector06.shape\n\n(670, 19)\n\n\n\nm_fat\n\n755\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os \nimport helpFunctions as hf\n\n\n# for i in range(5):\nimName = \"multispectral_day01.mat\"\nannotationName = \"annotation_day01.png\"\n\n\n\n\n[multiIm, annotationIm] = hf.loadMulti(imName, annotationName)\n\nfat_vector = multiIm[annotationIm[:,:,1], :]\nmeat_vector = multiIm[annotationIm[:,:,2], :]\n\n\nprint(np.shape(annotationIm[:,:, 0]))\nprint(np.shape(multiIm[:,:]))\nprint(np.shape(fat_vector))\n\n# amount of points in each annoted category for day 1\nm_fat = np.size(fat_vector[:,0])\nm_meat = np.size(meat_vector[:,0])\n\n# Means and standard devations for fat and meat categories\nfat_vector_means = np.mean(fat_vector, axis=0)\nmeat_vector_means = np.mean(meat_vector, axis=0)\nfat_vector_sd = np.std(fat_vector, axis=0)\nmeat_vector_sd = np.std(meat_vector, axis=0)\n\n# Finding the combined standard devation for fat and meat as\ncombined_sd = 1/(m_fat + m_meat) * (m_fat*fat_vector_sd + m_meat*meat_vector_sd) # (strictly speaking useless)\n\n# finding the simple threshold under the assumption that the standard devs are the same corresponds\n# to finding the mid point between the 2 means\nt = (fat_vector_means + meat_vector_means)/2\n\n# Finding the error classification rates\nerror_rate = ( np.sum(fat_vector &lt; t, axis=0) + np.sum(meat_vector &gt; t, axis=0)  ) / (m_fat + m_meat)\nprint(np.round(error_rate,7))\n\n\n# The best spectral band for this simple analysis is\nbest_band = np.argmin(error_rate) # This is 0-indexed\nprint(f\"the best band is { best_band+1}\")\n\n# the background index is the not index of all the annotations added together\nindex_background = (annotationIm[:,:,0]+annotationIm[:,:,1]+annotationIm[:,:,2])==0\n\n# Classifying the entire image with band 14 gives us\nentire_im = np.copy(multiIm[:,:,best_band])\nindex_fat = (entire_im &gt; t[best_band])\nindex_meat = (entire_im &lt; t[best_band])\nentire_im[index_fat] = 1 #set all fat values to class 1\nentire_im[index_meat] = 2 # set all meat values to class 2\nentire_im[index_background] = 0 # set background to class 0\n\ncompare_image(image = entire_im, day = 1 , title = \"Day 1 - Classification by Threshold Value\")\n\n#plt.imshow(entire_im)\n\n\n\"\"\"\nFigures \n\"\"\"\n\ncompare_spectrum(fat_vector_means, fat_vector_sd, meat_vector_means, meat_vector_sd, title = \"Mean Intensity of Sausage at Day 1\")\n\n\n##  Discriminant method\nSigma_fat = np.cov(fat_vector, rowvar = False) \nSigma_meat = np.cov(meat_vector, rowvar = False)\n\nm_fat = np.size(fat_vector,0) - 1\nm_meat = np.size(meat_vector,0) - 1\n\nSigma = 1/(m_fat + m_meat) * (m_fat*Sigma_fat + m_meat * Sigma_meat)\n\nSigma_inv = np.linalg.inv(Sigma)\n\nmu_fat = fat_vector_means #delete\nmu_meat = meat_vector_means \n\nXT = np.reshape(multiIm,(514*514,19))\n\n\n\nS_fat = (np.linalg.multi_dot([XT, Sigma_inv, mu_fat]) - 1/2 * np.linalg.multi_dot([mu_fat.T, Sigma_inv, mu_fat])).reshape((514,514))\nS_meat = (np.linalg.multi_dot([XT, Sigma_inv, mu_meat]) - 1/2 * np.linalg.multi_dot([mu_meat.T, Sigma_inv, mu_meat])).reshape((514,514))\n\nindex_fat[true_fat]\n\n# Post processing\n\ntrue_fat_index = annotationIm[:,:,1]\ntrue_meat_index = annotationIm[:,:,2]\n\nindex_fat2 = (S_fat &gt; S_meat)\nindex_meat2 = (S_meat &gt;= S_fat)\nentire_im2 = construct_entire_im(index_fat2,index_meat2,index_background)\n\n#Chekc the result\ncompute_errorrate(true_fat_index, true_meat_index, index_fat2, index_meat2)\n\n(514, 514)\n(514, 514, 19)\n(756, 19)\n[0.0104348 0.0162319 0.0121739 0.0086957 0.0081159 0.0086957 0.0075362\n 0.0104348 0.0081159 0.0104348 0.0057971 0.0052174 0.0110145 0.0046377\n 0.0046377 0.0092754 0.0150725 0.0156522 0.4614493]\nthe best band is 14\n\n\n\n\n\n\n\n\n0.005797101449275362\n\n\n\nsum(meat_vector_means )\n\narray([23.79845037, 17.06151408, 25.00177322, 27.41107262, 29.71776459,\n       30.97639933, 29.99817763, 32.00265984, 49.85284289, 55.16145961,\n       58.90757941, 64.41891698, 69.17798786, 68.96551428, 66.78297869,\n       61.4899176 , 60.16728914, 51.58782782,  1.2867258 ])\n\n\n\nnp.unique(sum(index_meat2))\n\narray([433, 441, 443, 444, 447, 448, 449, 450, 451, 452, 454, 455, 456,\n       457, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470,\n       471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483,\n       484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496,\n       497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 509, 510,\n       511, 514])\n\n\n\ncompare_image(image = entire_im2, day = 1 , title = \"Day 1 - Classification by Multivariate Linear Discriminant\")\n\n\n\n\n\n# CLASS OF DAY SIMPLE METHOD TRAINED WITH DAY 1\n\n\n# for i in range(5):\nimName06 = \"multispectral_day13.mat\"\nannotationName06 = \"annotation_day13.png\"\n\n\n\n\n[multiIm06, annotationIm06] = hf.loadMulti(imName06, annotationName06)\n\n\ntrue_fat_index = annotationIm06[:,:,1]\ntrue_meat_index = annotationIm06[:,:,2]\n\nfat_vector06 = multiIm06[annotationIm06[:,:,1], :]\nmeat_vector06 = multiIm06[annotationIm06[:,:,2], :]\n# amount of points in each annoted category for day 1\nm_fat06 = np.size(fat_vector06[:,0])\nm_meat06 = np.size(meat_vector06[:,0])\n\n\n# Finding the error classification rates\nerror_rate06 = ( np.sum(fat_vector06 &lt; t, axis=0) + np.sum(meat_vector06 &gt; t, axis=0)  ) / (m_fat06 + m_meat06)\nprint(np.round(error_rate06,7))\n\n\n\n# The best spectral band for this simple analysis is\nbest_band06 = np.argmin(error_rate06) # This is 0-indexed\nprint(f\"the best band is { best_band06+1}\")\n\n[0.0564565 0.0582583 0.0678679 0.0696697 0.0702703 0.0732733 0.0798799\n 0.0846847 0.1051051 0.1069069 0.1045045 0.1135135 0.1321321 0.1237237\n 0.1189189 0.1309309 0.1435435 0.0762763 0.6018018]\nthe best band is 1\n\n\n\nindex_background06 = (annotationIm06[:,:,0]+annotationIm06[:,:,1]+annotationIm06[:,:,2])==0\n\n# Classifying the entire image with band 14 gives us\nentire_im06 = np.copy(multiIm06[:,:,best_band])\nindex_fat06 = (entire_im06 &gt; t[best_band])\nindex_meat06 = (entire_im06 &lt; t[best_band])\nentire_im06[index_fat06] = 1 #set all fat values to class 1\nentire_im06[index_meat06] = 2 # set all meat values to class 2\nentire_im06[index_background06] = 0 # set background to class 0\n\ncompare_image(image = entire_im06 , day = 6 , title = \"Day 1 - Classification by Multivariate Linear Discriminant\")\n\n\n\ntrue_fat_index = annotationIm06[:,:,1]\ntrue_meat_index = annotationIm06[:,:,2]\n\n\ncompute_errorrate(true_fat_index, true_meat_index, index_fat2, index_meat2)\n\n\n\n\n0.3933933933933934\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport skimage\n\n#spectre = np.array([410, 438, 450, 468, 502, 519, 572, 591, 625, 639, 653, 695, 835, 863, 880, 913, 929, 940, 955])\n\nspectre = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19], dtype = np.int64)\n\ndef compare_image(image, day, title=\"Titel\"):\n\n    try:\n        color_im = skimage.io.imread(\"color_day\" + \"0\"*(day&lt;10) + str(day) + \".png\")\n    except FileNotFoundError:\n        print(\"No image found for day \" + str(day))\n        color_im = np.ones(np.shape(image))\n\n    fig, [ax1,ax2] = plt.subplots(1,2, figsize=(8,4))\n    fig.suptitle(title)\n    ax1.imshow(color_im)\n    ax2.imshow(image)\n    plt.show()\n\n\n#spectre = np.array([410, 438, 450, 468, 502, 519, 572, 591, 625, 639, 653, 695, 835, 863, 880, 913, 929, 940, 955])\n\nspectre = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19], dtype = np.int64)\n\n\ndef compare_spectrum(mean_fat, sd_fat, mean_meat, sd_meat, signific = 2, ecolor = [\"black\", \"black\"], capsize = 5, color = [\"orange\", \"blue\"], spectrum = spectre, title = \"Titel\", xtick_align = False):\n    \n\n    plt.figure()\n    plt.errorbar(spectrum, mean_fat, signific*sd_fat, capsize=capsize, ecolor=ecolor[0], label='fat', color=color[0])\n    plt.errorbar(spectrum ,mean_meat, signific*sd_meat, capsize=capsize, ecolor=ecolor[1],  label='meat', color=color[1])\n    plt.legend()\n    plt.grid()\n    plt.xlabel(\"Wave Band\")\n    plt.ylabel(\"Mean Intensity\")\n    plt.title(title)\n    plt.xticks(spectrum)\n    #if xtick_align: plt.xticks(spectrum)\n    plt.show()\n\n\nerror, salami_t,wave = simple(multiImD[0], annotationImD[0], fat[0], meat[0], nFat[0], nMeat[0],t,  index_background[0])\n\nprint(np.round(t,7))\n\nprint(np.round(error,7))\nprint(f\"the best band is { wave+1}\")\n\n#plt.imshow(multiIm[:,:,0],cmap=\"gray\")\n#plt.show()\n\nfig, [ax1,ax2] = plt.subplots(1,2, figsize=(8,4))\nfig.suptitle(\"Comparison Classification by Threshold Day 1\")\nax1.imshow(multiImD[0][:,:,0],cmap=\"gray\")\nax2.imshow(salami_t)\nplt.show()"
  }
]