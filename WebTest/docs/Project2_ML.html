<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.340">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>WebTest - Project 2: South African Heart Disease</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">WebTest</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label=""><i class="bi bi-github"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://github.com/carolinahdzm/MLandDataM-Project2/blob/main/Project2_ML.ipynb">
            Source Code
            </a>
          </li>
      </ul>
    </div>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Project2_ML.html">Machine Learning Project Part 2</a></li><li class="breadcrumb-item"><a href="./Project2_ML.html">Project 2: South African Heart Disease</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Machine Learning Project Part 1</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./projectML.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Project 1: South Africa Heart Disease</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Machine Learning Project Part 2</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Project2_ML.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Project 2: South African Heart Disease</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#data-visualization" id="toc-data-visualization" class="nav-link active" data-scroll-target="#data-visualization">Data visualization</a></li>
  <li><a href="#dependent-variable" id="toc-dependent-variable" class="nav-link" data-scroll-target="#dependent-variable">Dependent variable</a>
  <ul class="collapse">
  <li><a href="#feature-selection" id="toc-feature-selection" class="nav-link" data-scroll-target="#feature-selection">Feature selection</a></li>
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization">Regularization</a></li>
  </ul></li>
  <li><a href="#comparison-of-the-3-models" id="toc-comparison-of-the-3-models" class="nav-link" data-scroll-target="#comparison-of-the-3-models">Comparison of the 3 models</a></li>
  <li><a href="#classification-models" id="toc-classification-models" class="nav-link" data-scroll-target="#classification-models">Classification Models</a></li>
  <li><a href="#classification-models-1" id="toc-classification-models-1" class="nav-link" data-scroll-target="#classification-models-1">Classification Models</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Project 2: South African Heart Disease</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Group 22</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.linalg <span class="im">import</span> svd</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> figure, plot, xlabel, ylabel, legend, show, clim, semilogx, loglog, title, subplot, grid</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.io <span class="im">import</span> loadmat</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.linear_model <span class="im">as</span> lm</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge, LinearRegression, LogisticRegression</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> model_selection, tree</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> toolbox_02450 <span class="im">import</span> feature_selector_lr, bmplot, rlr_validate, train_neural_net</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data from csv and store it in dataframe'</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'SAheart.data.txt'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="data-visualization" class="level2">
<h2 class="anchored" data-anchor-id="data-visualization">Data visualization</h2>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Change variables to integers: Replace absent and present by 0 and 1</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.replace(<span class="st">'Absent'</span>,<span class="dv">0</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.replace(<span class="st">'Present'</span>,<span class="dv">1</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<div id="tbl-df" class="anchored">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<caption>Table&nbsp;1: The South Africa Heart Disease Dataset</caption>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">row.names</th>
<th data-quarto-table-cell-role="th">sbp</th>
<th data-quarto-table-cell-role="th">tobacco</th>
<th data-quarto-table-cell-role="th">ldl</th>
<th data-quarto-table-cell-role="th">adiposity</th>
<th data-quarto-table-cell-role="th">famhist</th>
<th data-quarto-table-cell-role="th">typea</th>
<th data-quarto-table-cell-role="th">obesity</th>
<th data-quarto-table-cell-role="th">alcohol</th>
<th data-quarto-table-cell-role="th">age</th>
<th data-quarto-table-cell-role="th">chd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1</td>
<td>160</td>
<td>12.00</td>
<td>5.73</td>
<td>23.11</td>
<td>1</td>
<td>49</td>
<td>25.30</td>
<td>97.20</td>
<td>52</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>2</td>
<td>144</td>
<td>0.01</td>
<td>4.41</td>
<td>28.61</td>
<td>0</td>
<td>55</td>
<td>28.87</td>
<td>2.06</td>
<td>63</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>3</td>
<td>118</td>
<td>0.08</td>
<td>3.48</td>
<td>32.28</td>
<td>1</td>
<td>52</td>
<td>29.14</td>
<td>3.81</td>
<td>46</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4</td>
<td>170</td>
<td>7.50</td>
<td>6.41</td>
<td>38.03</td>
<td>1</td>
<td>51</td>
<td>31.99</td>
<td>24.26</td>
<td>58</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5</td>
<td>134</td>
<td>13.60</td>
<td>3.50</td>
<td>27.78</td>
<td>1</td>
<td>60</td>
<td>25.99</td>
<td>57.34</td>
<td>49</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">457</td>
<td>459</td>
<td>214</td>
<td>0.40</td>
<td>5.98</td>
<td>31.72</td>
<td>0</td>
<td>64</td>
<td>28.45</td>
<td>0.00</td>
<td>58</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">458</td>
<td>460</td>
<td>182</td>
<td>4.20</td>
<td>4.41</td>
<td>32.10</td>
<td>0</td>
<td>52</td>
<td>28.61</td>
<td>18.72</td>
<td>52</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">459</td>
<td>461</td>
<td>108</td>
<td>3.00</td>
<td>1.59</td>
<td>15.23</td>
<td>0</td>
<td>40</td>
<td>20.09</td>
<td>26.64</td>
<td>55</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">460</td>
<td>462</td>
<td>118</td>
<td>5.40</td>
<td>11.61</td>
<td>30.79</td>
<td>0</td>
<td>64</td>
<td>27.35</td>
<td>23.97</td>
<td>40</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">461</td>
<td>463</td>
<td>132</td>
<td>0.00</td>
<td>4.82</td>
<td>33.41</td>
<td>1</td>
<td>62</td>
<td>14.70</td>
<td>0.00</td>
<td>46</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>

<p>462 rows × 11 columns</p>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop row column</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>df.drop([<span class="st">'row.names'</span>], axis <span class="op">=</span> <span class="st">'columns'</span>, inplace<span class="op">=</span><span class="va">True</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create matrix with values</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>raw_data <span class="op">=</span> df.values  </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">9</span>) </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> raw_data[:, cols]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.asarray(X, dtype <span class="op">=</span> np.intc)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>attributeNames <span class="op">=</span> np.asarray(df.columns[cols])</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>classLabels <span class="op">=</span> raw_data[:,<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>classNames <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">set</span>(classLabels))</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>classDict <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(classNames,<span class="bu">range</span>(<span class="bu">len</span>(classNames))))</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([classDict[cl] <span class="cf">for</span> cl <span class="kw">in</span> classLabels])</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>N, M <span class="op">=</span> X.shape</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> <span class="bu">len</span>(classNames)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="dependent-variable" class="level2">
<h2 class="anchored" data-anchor-id="dependent-variable">Dependent variable</h2>
<p>Which attribute is the best suitable for regression analysis?</p>
<p>We use OLS linear regression model. We select one variable to be predicted according to all our other variables in the dataset, and measure the values of the MSE residuals when plotting the estimated data points against the true line of fitting.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>criterion_variables <span class="op">=</span> [<span class="st">'sbp'</span>, <span class="st">'tobacco'</span>, <span class="st">'ldl'</span>, <span class="st">'adiposity'</span>, <span class="st">'typea'</span>, <span class="st">'obesity'</span>, <span class="st">'alcohol'</span>, <span class="st">'age'</span>,<span class="st">'chd'</span>,<span class="st">'famhist'</span>]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Display plot</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">25</span>,<span class="dv">15</span>))</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>plt.subplots_adjust(wspace <span class="op">=</span> <span class="fl">0.5</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    attr_col <span class="op">=</span> <span class="bu">list</span>(df.columns).index(criterion_variables[i])</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    cols <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">0</span>, attr_col)) <span class="op">+</span> <span class="bu">list</span>(<span class="bu">range</span>(attr_col <span class="op">+</span> <span class="dv">1</span>, <span class="bu">len</span>(df.columns)))</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> raw_data[:, cols]</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> raw_data[:, attr_col] <span class="co"># the 'oldpeak' column</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    attributeNames <span class="op">=</span> <span class="bu">list</span>(df.columns[cols])</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    N, M <span class="op">=</span> X.shape</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> stats.zscore(X)<span class="op">;</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fit ordinary least squares regression model    </span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> lm.LinearRegression()</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    model.fit(X,y)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predict thalach</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    y_est <span class="op">=</span> model.predict(X)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    residual <span class="op">=</span> y_est<span class="op">-</span>y</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">5</span>, i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    plt.plot(y, y, <span class="st">'--r'</span>)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    plt.plot(y, y_est, <span class="st">'.g'</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Value of </span><span class="sc">{0}</span><span class="st"> (true)'</span>.<span class="bu">format</span>(criterion_variables[i]))<span class="op">;</span> ylabel(<span class="st">'Value of </span><span class="sc">{0}</span><span class="st"> variable (estimated)'</span>.<span class="bu">format</span>(criterion_variables[i]))</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    plt.legend([<span class="st">'True values'</span>, <span class="st">'Estimated values'</span>], loc <span class="op">=</span> <span class="dv">2</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Project2_ML_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Estimation values of ‘adiposity’, ‘obesity’, and ‘age’ seem to align well with the line of true fit. However ‘obesity’ has lower average MSE residuals overall, so we will choose it as our dependent variable to be estimated using all other dataset attributes.</p>
<section id="feature-selection" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection">Feature selection</h3>
<p>Can we improve the model by reaching the same prediction power using fewer attributes?</p>
<p>We use feature forward selection with cross-validation across 10 outer folds and 10 inner folds, where we train a model starting with no features and gradually select and add, one-by-one, the features which minimize the squared error in the inner cross-validation loop, until the error cannot be minimized by adding any further predictors. Finally, we compute the R2 value of both the full model without feature selection and the newly-fitted feature-selecting model upon a test set, in order to see what percentage of the variance within the dataset is explained by each model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>raw_data <span class="op">=</span> df.to_numpy()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>attr_col <span class="op">=</span> <span class="bu">list</span>(df.columns).index(<span class="st">'obesity'</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">0</span>, attr_col)) <span class="op">+</span> <span class="bu">list</span>(<span class="bu">range</span>(attr_col <span class="op">+</span> <span class="dv">1</span>, <span class="bu">len</span>(df.columns)))</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> raw_data[:, cols]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> raw_data[:, attr_col] <span class="co"># the 'adiposity' column</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>attributeNames <span class="op">=</span> <span class="bu">list</span>(df.columns[cols])</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>N, M <span class="op">=</span> X.shape</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> stats.zscore(X)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co">## Crossvalidation</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create crossvalidation partition for evaluation</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>CV <span class="op">=</span> model_selection.KFold(n_splits<span class="op">=</span>K,shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize variables</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>Features <span class="op">=</span> np.zeros((M,K))</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>Error_train <span class="op">=</span> np.empty((K,<span class="dv">1</span>))</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>Error_test <span class="op">=</span> np.empty((K,<span class="dv">1</span>))</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>Error_train_fs <span class="op">=</span> np.empty((K,<span class="dv">1</span>))</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>Error_test_fs <span class="op">=</span> np.empty((K,<span class="dv">1</span>))</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>Error_train_nofeatures <span class="op">=</span> np.empty((K,<span class="dv">1</span>))</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>Error_test_nofeatures <span class="op">=</span> np.empty((K,<span class="dv">1</span>))</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>k<span class="op">=</span><span class="dv">0</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> CV.split(X):</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># extract training and test set for current CV fold</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    X_train <span class="op">=</span> X[train_index,:]</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    y_train <span class="op">=</span> y[train_index]</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    X_test <span class="op">=</span> X[test_index,:]</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>    y_test <span class="op">=</span> y[test_index]</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>    internal_cross_validation <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute squared error without using the input data at all</span></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    Error_train_nofeatures[k] <span class="op">=</span> np.square(y_train<span class="op">-</span>y_train.mean()).<span class="bu">sum</span>()<span class="op">/</span>y_train.shape[<span class="dv">0</span>]</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>    Error_test_nofeatures[k] <span class="op">=</span> np.square(y_test<span class="op">-</span>y_test.mean()).<span class="bu">sum</span>()<span class="op">/</span>y_test.shape[<span class="dv">0</span>]</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute squared error with all features selected (no feature selection)</span></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> lm.LinearRegression(fit_intercept<span class="op">=</span><span class="va">True</span>).fit(X_train, y_train)</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>    Error_train[k] <span class="op">=</span> np.square(y_train<span class="op">-</span>m.predict(X_train)).<span class="bu">sum</span>()<span class="op">/</span>y_train.shape[<span class="dv">0</span>]</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>    Error_test[k] <span class="op">=</span> np.square(y_test<span class="op">-</span>m.predict(X_test)).<span class="bu">sum</span>()<span class="op">/</span>y_test.shape[<span class="dv">0</span>]</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute squared error with feature subset selection</span></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>    textout <span class="op">=</span> <span class="st">''</span></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>    selected_features, features_record, loss_record <span class="op">=</span> feature_selector_lr(X_train, y_train, internal_cross_validation,display<span class="op">=</span>textout)</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>    Features[selected_features,k] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># .. alternatively you could use module sklearn.feature_selection</span></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(selected_features) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'No features were selected, i.e. the data (X) in the fold cannot describe the outcomes (y).'</span> )</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> lm.LinearRegression(fit_intercept<span class="op">=</span><span class="va">True</span>).fit(X_train[:,selected_features], y_train)</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>        Error_train_fs[k] <span class="op">=</span> np.square(y_train<span class="op">-</span>m.predict(X_train[:,selected_features])).<span class="bu">sum</span>()<span class="op">/</span>y_train.shape[<span class="dv">0</span>]</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>        Error_test_fs[k] <span class="op">=</span> np.square(y_test<span class="op">-</span>m.predict(X_test[:,selected_features])).<span class="bu">sum</span>()<span class="op">/</span>y_test.shape[<span class="dv">0</span>]</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>        figure(k)</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>        subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>        plot(<span class="bu">range</span>(<span class="dv">1</span>,<span class="bu">len</span>(loss_record)), loss_record[<span class="dv">1</span>:])</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>        xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>        ylabel(<span class="st">'Squared error (crossvalidation)'</span>)    </span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>        subplot(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">3</span>)</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>        bmplot(attributeNames, <span class="bu">range</span>(<span class="dv">1</span>,features_record.shape[<span class="dv">1</span>]), <span class="op">-</span>features_record[:,<span class="dv">1</span>:])</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>        clim(<span class="op">-</span><span class="fl">1.5</span>,<span class="dv">0</span>)</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>        xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print('Cross validation fold {0}/{1}'.format(k+1,K))</span></span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print('Train indices: {0}'.format(train_index))</span></span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print('Test indices: {0}'.format(test_index))</span></span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print('Features no: {0}\n'.format(selected_features.size))</span></span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>    k<span class="op">+=</span><span class="dv">1</span></span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a><span class="co"># Display results</span></span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Linear regression without feature selection:'</span>)</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'- Training error: </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(Error_train.mean()))</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'- Test error:     </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(Error_test.mean()))</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'- R^2 train:     </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>((Error_train_nofeatures.<span class="bu">sum</span>()<span class="op">-</span>Error_train.<span class="bu">sum</span>())<span class="op">/</span>Error_train_nofeatures.<span class="bu">sum</span>()))</span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'- R^2 test:     </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>((Error_test_nofeatures.<span class="bu">sum</span>()<span class="op">-</span>Error_test.<span class="bu">sum</span>())<span class="op">/</span>Error_test_nofeatures.<span class="bu">sum</span>()))</span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Linear regression with feature selection:'</span>)</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'- Training error: </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(Error_train_fs.mean()))</span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'- Test error:     </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(Error_test_fs.mean()))</span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'- R^2 train:     </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>((Error_train_nofeatures.<span class="bu">sum</span>()<span class="op">-</span>Error_train_fs.<span class="bu">sum</span>())<span class="op">/</span>Error_train_nofeatures.<span class="bu">sum</span>()))</span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'- R^2 test:     </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>((Error_test_nofeatures.<span class="bu">sum</span>()<span class="op">-</span>Error_test_fs.<span class="bu">sum</span>())<span class="op">/</span>Error_test_nofeatures.<span class="bu">sum</span>()))</span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a>figure(k)</span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>subplot(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">2</span>)</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>bmplot(attributeNames, <span class="bu">range</span>(<span class="dv">1</span>,Features.shape[<span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>), <span class="op">-</span>Features)</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>clim(<span class="op">-</span><span class="fl">1.5</span>,<span class="dv">0</span>)</span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a>xlabel(<span class="st">'Crossvalidation fold'</span>)</span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>ylabel(<span class="st">'Attribute'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Linear regression without feature selection:
- Training error: 7.676131475685457
- Test error:     8.104654216746123
- R^2 train:     0.5666276636565277
- R^2 test:     0.5337053644554508


Linear regression with feature selection:
- Training error: 7.7469134976404685
- Test error:     8.109273625721451
- R^2 train:     0.5626315139914363
- R^2 test:     0.5334395905474004</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>Text(0, 0.5, 'Attribute')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="Project2_ML_files/figure-html/cell-8-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Project2_ML_files/figure-html/cell-8-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Project2_ML_files/figure-html/cell-8-output-5.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Project2_ML_files/figure-html/cell-8-output-6.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Project2_ML_files/figure-html/cell-8-output-7.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Project2_ML_files/figure-html/cell-8-output-8.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Project2_ML_files/figure-html/cell-8-output-9.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Project2_ML_files/figure-html/cell-8-output-10.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Project2_ML_files/figure-html/cell-8-output-11.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Project2_ML_files/figure-html/cell-8-output-12.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="Project2_ML_files/figure-html/cell-8-output-13.png" class="img-fluid"></p>
</div>
</div>
<p>From the forward selection algorithm, we can see that our dependent variable ‘obesity’ is best described using the features: ‘adiposity’, ‘typea’ and ‘age’, while many other variables have been left out completely from our model, due to the low assistance in further reducing the MSE of the estimates.</p>
<p>However, as can be seen in the table, the R2 value in the test set of both models is similar, representing a variance of ~54%, suggesting that feature selection does not have an optimising impact on our regression model, and may even slightly increase the MSE. This means that, although the criterion is largely predicted by only a few attributes, the full model definitely does not over-fit the data and even terms that have a very low correlation will help to reduce the MSE.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect selected feature coefficients effect on the entire dataset </span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># and plot the fitted model residual error as function of each attribute</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># to inspect for systematic structure in the residual</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>f<span class="op">=</span><span class="dv">2</span> <span class="co"># cross-validation fold to inspect</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>ff<span class="op">=</span>Features[:,f<span class="op">-</span><span class="dv">1</span>].nonzero()[<span class="dv">0</span>]</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ff)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">len</span>(ff) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">No features were selected, i.e. the data (X) in the fold cannot describe the outcomes (y).'</span> )</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> lm.LinearRegression(fit_intercept<span class="op">=</span><span class="va">True</span>).fit(X[:,ff], y)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    y_est<span class="op">=</span> m.predict(X[:,ff])</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    residual<span class="op">=</span>y<span class="op">-</span>y_est</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    figure(k<span class="op">+</span><span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">6</span>))</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    title(<span class="st">'Residual error vs. Attributes for features selected in cross-validation fold </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(f))</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">len</span>(ff)):</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>       subplot(<span class="dv">4</span>, <span class="bu">int</span>( np.ceil(<span class="bu">len</span>(ff) <span class="op">//</span> <span class="dv">2</span>)),i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>       plot(X[:,ff[i]],residual,<span class="st">'.'</span>)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>       xlabel(attributeNames[ff[i]])</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>       ylabel(<span class="st">'residual error'</span>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[3 4 5 7]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="Project2_ML_files/figure-html/cell-9-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="regularization" class="level3">
<h3 class="anchored" data-anchor-id="regularization">Regularization</h3>
<p>Here we introduce a regularization parameter λ into the linear regression, which will take values between <span class="math inline">\(10^-3\)</span> and <span class="math inline">\(10^7\)</span> because we want to obtain the lowest possible generalization error when using our linear regression model.</p>
<p>In order to reliably estimate the generalization error for different values of λ, we have performed two-level cross-validation testing, with the outer layer having K1 = 10 folds and the inner fold being selected to K2 = 10 folds.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Add offset attribute</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.concatenate((np.ones((X.shape[<span class="dv">0</span>],<span class="dv">1</span>)),X),<span class="dv">1</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>attributeNames<span class="op">=</span> [<span class="st">u'Offset'</span>]<span class="op">+</span>attributeNames</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> M<span class="op">+</span><span class="dv">1</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>attributeNames</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>['Offset',
 'sbp',
 'tobacco',
 'ldl',
 'adiposity',
 'famhist',
 'typea',
 'alcohol',
 'age',
 'chd']</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Crossvalidation</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create crossvalidation partition for evaluation</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>CV <span class="op">=</span> model_selection.KFold(K, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">#CV = model_selection.KFold(K, shuffle=False)</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Values of lambda</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>lambdas <span class="op">=</span> np.power(<span class="fl">10.</span>,<span class="bu">range</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">9</span>))</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize variables</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">#T = len(lambdas)</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>Error_train <span class="op">=</span> np.empty((K,<span class="dv">1</span>))</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>Error_test <span class="op">=</span> np.empty((K,<span class="dv">1</span>))</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>Error_train_rlr <span class="op">=</span> np.empty((K,<span class="dv">1</span>))</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>Error_test_rlr <span class="op">=</span> np.empty((K,<span class="dv">1</span>))</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>Error_train_nofeatures <span class="op">=</span> np.empty((K,<span class="dv">1</span>))</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>Error_test_nofeatures <span class="op">=</span> np.empty((K,<span class="dv">1</span>))</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>val_error_lambdas <span class="op">=</span> np.empty((K,<span class="bu">len</span>(lambdas)))</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>w_rlr <span class="op">=</span> np.empty((M,K))</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.empty((K, M<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> np.empty((K, M<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>w_noreg <span class="op">=</span> np.empty((M,K))</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>k<span class="op">=</span><span class="dv">0</span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> CV.split(X,y):</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># extract training and test set for current CV fold</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>    X_train <span class="op">=</span> X[train_index]</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    y_train <span class="op">=</span> y[train_index]</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    X_test <span class="op">=</span> X[test_index]</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>    y_test <span class="op">=</span> y[test_index]</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>    internal_cross_validation <span class="op">=</span> <span class="dv">10</span>    </span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>    opt_val_err, opt_lambda, mean_w_vs_lambda, train_err_vs_lambda, test_err_vs_lambda <span class="op">=</span> rlr_validate(X_train, y_train, lambdas, internal_cross_validation)</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>    val_error_lambdas[k] <span class="op">=</span> test_err_vs_lambda</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Standardize outer fold based on training set, and save the mean and standard</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># deviations since they're part of the model (they would be needed for</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># making new predictions) - for brevity we won't always store these in the scripts</span></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>    mu[k, :] <span class="op">=</span> np.mean(X_train[:, <span class="dv">1</span>:], <span class="dv">0</span>)</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>    sigma[k, :] <span class="op">=</span> np.std(X_train[:, <span class="dv">1</span>:], <span class="dv">0</span>)</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>    X_train[:, <span class="dv">1</span>:] <span class="op">=</span> (X_train[:, <span class="dv">1</span>:] <span class="op">-</span> mu[k, :] ) <span class="op">/</span> sigma[k, :] </span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>    X_test[:, <span class="dv">1</span>:] <span class="op">=</span> (X_test[:, <span class="dv">1</span>:] <span class="op">-</span> mu[k, :] ) <span class="op">/</span> sigma[k, :] </span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>    Xty <span class="op">=</span> X_train.T <span class="op">@</span> y_train</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>    XtX <span class="op">=</span> X_train.T <span class="op">@</span> X_train</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute mean squared error without using the input data at all</span></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>    Error_train_nofeatures[k] <span class="op">=</span> np.square(y_train<span class="op">-</span>y_train.mean()).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>y_train.shape[<span class="dv">0</span>]</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>    Error_test_nofeatures[k] <span class="op">=</span> np.square(y_test<span class="op">-</span>y_test.mean()).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>y_test.shape[<span class="dv">0</span>]</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Estimate weights for the optimal value of lambda, on entire training set</span></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>    lambdaI <span class="op">=</span> opt_lambda <span class="op">*</span> np.eye(M)</span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>    lambdaI[<span class="dv">0</span>,<span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span> <span class="co"># Do no regularize the bias term</span></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>    w_rlr[:,k] <span class="op">=</span> np.linalg.solve(XtX<span class="op">+</span>lambdaI,Xty).squeeze()</span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute mean squared error with regularization with optimal lambda</span></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>    Error_train_rlr[k] <span class="op">=</span> np.square(y_train<span class="op">-</span>X_train <span class="op">@</span> w_rlr[:,k]).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>y_train.shape[<span class="dv">0</span>]</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a>    Error_test_rlr[k] <span class="op">=</span> np.square(y_test<span class="op">-</span>X_test <span class="op">@</span> w_rlr[:,k]).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>y_test.shape[<span class="dv">0</span>]</span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Estimate weights for unregularized linear regression, on entire training set</span></span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a>    w_noreg[:,k] <span class="op">=</span> np.linalg.solve(XtX,Xty).squeeze()</span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute mean squared error without regularization</span></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a>    Error_train[k] <span class="op">=</span> np.square(y_train<span class="op">-</span>X_train <span class="op">@</span> w_noreg[:,k]).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>y_train.shape[<span class="dv">0</span>]</span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a>    Error_test[k] <span class="op">=</span> np.square(y_test<span class="op">-</span>X_test <span class="op">@</span> w_noreg[:,k]).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span>y_test.shape[<span class="dv">0</span>]</span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a>    <span class="co"># OR ALTERNATIVELY: you can use sklearn.linear_model module for linear regression:</span></span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a>    <span class="co">#m = lm.LinearRegression().fit(X_train, y_train)</span></span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Error_train[k] = np.square(y_train-m.predict(X_train)).sum()/y_train.shape[0]</span></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Error_test[k] = np.square(y_test-m.predict(X_test)).sum()/y_test.shape[0]</span></span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display the results for the last cross-validation fold</span></span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> k <span class="op">==</span> K<span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a>        figure(k, figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">8</span>))</span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a>        subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a>        semilogx(lambdas,mean_w_vs_lambda.T[:,<span class="dv">1</span>:],<span class="st">'.-'</span>) <span class="co"># Don't plot the bias term</span></span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a>        xlabel(<span class="st">'Regularization factor'</span>)</span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a>        ylabel(<span class="st">'Mean Coefficient Values'</span>)</span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a>        grid()</span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a>        <span class="co"># You can choose to display the legend, but it's omitted for a cleaner </span></span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a>        <span class="co"># plot, since there are many attributes</span></span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a>        legend(attributeNames[<span class="dv">1</span>:], loc<span class="op">=</span><span class="st">'best'</span>)</span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a>        subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>)</span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a>        title(<span class="st">'Optimal lambda: 1e</span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(np.log10(opt_lambda)))</span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a>        loglog(lambdas,train_err_vs_lambda.T,<span class="st">'b.-'</span>,lambdas,test_err_vs_lambda.T,<span class="st">'r.-'</span>)</span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a>        xlabel(<span class="st">'Regularization factor'</span>)</span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a>        ylabel(<span class="st">'Squared error (crossvalidation)'</span>)</span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a>        legend([<span class="st">'Train error'</span>,<span class="st">'Validation error'</span>])</span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a>        grid()</span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a>    <span class="co"># To inspect the used indices, use these print statements</span></span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print('Cross validation fold {0}/{1}:'.format(k+1,K))</span></span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print('Train indices: {0}'.format(train_index))</span></span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print('Test indices: {0}\n'.format(test_index))</span></span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a>    k<span class="op">+=</span><span class="dv">1</span></span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a>show()</span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a><span class="co"># Display results</span></span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Linear regression without feature selection:'</span>)</span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'- Training error: </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(Error_train.mean()))</span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'- Test error:     </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(Error_test.mean()))</span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'- R^2 train:     </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>((Error_train_nofeatures.<span class="bu">sum</span>()<span class="op">-</span>Error_train.<span class="bu">sum</span>())<span class="op">/</span>Error_train_nofeatures.<span class="bu">sum</span>()))</span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'- R^2 test:     </span><span class="sc">{0}</span><span class="ch">\n</span><span class="st">'</span>.<span class="bu">format</span>((Error_test_nofeatures.<span class="bu">sum</span>()<span class="op">-</span>Error_test.<span class="bu">sum</span>())<span class="op">/</span>Error_test_nofeatures.<span class="bu">sum</span>()))</span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Regularized linear regression:'</span>)</span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'- Training error: </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(Error_train_rlr.mean()))</span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'- Test error:     </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(Error_test_rlr.mean()))</span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'- R^2 train:     </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>((Error_train_nofeatures.<span class="bu">sum</span>()<span class="op">-</span>Error_train_rlr.<span class="bu">sum</span>())<span class="op">/</span>Error_train_nofeatures.<span class="bu">sum</span>()))</span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'- R^2 test:     </span><span class="sc">{0}</span><span class="ch">\n</span><span class="st">'</span>.<span class="bu">format</span>((Error_test_nofeatures.<span class="bu">sum</span>()<span class="op">-</span>Error_test_rlr.<span class="bu">sum</span>())<span class="op">/</span>Error_test_nofeatures.<span class="bu">sum</span>()))</span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Weights in last fold:'</span>)</span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> m <span class="kw">in</span> <span class="bu">range</span>(M):</span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="sc">{:&gt;15}</span><span class="st"> </span><span class="sc">{:&gt;15}</span><span class="st">'</span>.<span class="bu">format</span>(attributeNames[m], np.<span class="bu">round</span>(w_rlr[m,<span class="op">-</span><span class="dv">1</span>],<span class="dv">2</span>)))</span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Generalization error for different values of lambda:'</span>)</span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(lambdas)):</span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="sc">{:&gt;20}</span><span class="st"> </span><span class="sc">{:&gt;20}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="bu">float</span>(lambdas[i]), <span class="bu">str</span>(np.<span class="bu">round</span>(val_error_lambdas.mean(axis <span class="op">=</span> <span class="dv">0</span>)[i],<span class="dv">2</span>))))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>&lt;Figure size 640x480 with 0 Axes&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="Project2_ML_files/figure-html/cell-11-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Linear regression without feature selection:
- Training error: 7.680674622271397
- Test error:     8.020988394051368
- R^2 train:     0.5664245683319278
- R^2 test:     0.5433320689535166

Regularized linear regression:
- Training error: 7.680865193377744
- Test error:     8.019454576648586
- R^2 train:     0.5664138105594974
- R^2 test:     0.5434193955005107

Weights in last fold:
         Offset           26.05
            sbp             0.2
        tobacco            0.02
            ldl            0.15
      adiposity            3.47
        famhist             0.2
          typea            0.42
        alcohol           -0.12
            age           -0.97
            chd           -0.34
Generalization error for different values of lambda:
               1e-05                 8.05
              0.0001                 8.05
               0.001                 8.05
                0.01                 8.05
                 0.1                 8.05
                 1.0                 8.05
                10.0                 8.06
               100.0                 8.82
              1000.0                13.04
             10000.0                16.89
            100000.0                17.72
           1000000.0                17.82
          10000000.0                17.83
         100000000.0                17.83</code></pre>
</div>
</div>
<p>The model shows the typical trend of the generalisation error falling and then growing as λ increases. The lowest error is obtained when λ=1, hence that is the optimal value for our regularization parameter.</p>
<p>When selecting this parameter we reach the best trade-off between bias and variance. If we had chosen a smaller λ value, the variance would be higher and the bias smaller, thus producing underfitting; whereas if we had set a higher value, the opposite would have happened, leading to overfitting.</p>
<p>Now we want to find the equation of the regularized linear regresion in order to know how each atribute contributes to the prediction of the obesity</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>raw_data <span class="op">=</span> df.to_numpy()</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>attr_col <span class="op">=</span> <span class="bu">list</span>(df.columns).index(<span class="st">'obesity'</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">0</span>, attr_col)) <span class="op">+</span> <span class="bu">list</span>(<span class="bu">range</span>(attr_col <span class="op">+</span> <span class="dv">1</span>, <span class="bu">len</span>(df.columns)))</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> raw_data[:, cols]</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> raw_data[:, attr_col]</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>attributeNames <span class="op">=</span> <span class="bu">list</span>(df.columns[cols])</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>N, M <span class="op">=</span> X.shape</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> stats.zscore(X)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>ridgereg_model <span class="op">=</span> Ridge(alpha <span class="op">=</span> <span class="dv">10</span>, fit_intercept <span class="op">=</span> <span class="va">True</span>).fit(X, y)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>linreg_model <span class="op">=</span> LinearRegression(fit_intercept <span class="op">=</span> <span class="va">True</span>).fit(X, y)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Weights for LinReg model with regularization:'</span>)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="sc">{:&gt;20}</span><span class="st"> </span><span class="sc">{:&gt;20}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="st">'Intercept'</span>, <span class="bu">str</span>(np.<span class="bu">round</span>(ridgereg_model.intercept_,<span class="dv">2</span>))))</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> m <span class="kw">in</span> <span class="bu">range</span>(M):</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="sc">{:&gt;20}</span><span class="st"> </span><span class="sc">{:&gt;20}</span><span class="st">'</span>.<span class="bu">format</span>(attributeNames[m], <span class="bu">str</span>(np.<span class="bu">round</span>(ridgereg_model.coef_[m],<span class="dv">2</span>))))</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Weights for LinReg model without regularization:'</span>)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="sc">{:&gt;20}</span><span class="st"> </span><span class="sc">{:&gt;20}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="st">'Intercept'</span>, <span class="bu">str</span>(np.<span class="bu">round</span>(linreg_model.intercept_,<span class="dv">2</span>))))</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> m <span class="kw">in</span> <span class="bu">range</span>(M):</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="sc">{:&gt;20}</span><span class="st"> </span><span class="sc">{:&gt;20}</span><span class="st">'</span>.<span class="bu">format</span>(attributeNames[m], <span class="bu">str</span>(np.<span class="bu">round</span>(linreg_model.coef_[m],<span class="dv">2</span>))))</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Weights for LinReg model with regularization:
           Intercept                26.04
                 sbp                 0.17
             tobacco                -0.04
                 ldl                 0.16
           adiposity                 3.45
             famhist                 0.09
               typea                 0.39
             alcohol                -0.05
                 age                 -0.9
                 chd                -0.24


Weights for LinReg model without regularization:
           Intercept                26.04
                 sbp                 0.16
             tobacco                -0.03
                 ldl                 0.12
           adiposity                 3.61
             famhist                 0.09
               typea                 0.39
             alcohol                -0.06
                 age                -1.01
                 chd                -0.24</code></pre>
</div>
</div>
<p>From this values we can say that a person with high values for sbp, ldl, adiposity, famhist, typea and age; and low values for tobacco, alcohol and chd will be more obese. Conversely, if the values of the variables sbp, ldl, adiposity, famhist, typea and ageare low; and the values of tobacco, alcohol and chd are high, the person will be thinner.</p>
</section>
</section>
<section id="comparison-of-the-3-models" class="level2">
<h2 class="anchored" data-anchor-id="comparison-of-the-3-models">Comparison of the 3 models</h2>
<p>Two-level cross-validation with K1 = K2 = 10 outer and inner folds. The inner folds are used to calculate, for each model, a complexity control parameter (λ and number of hidden layers) that minimises the generalisation error for that model, and the outer folds to test model performance.</p>
<p>We use the Setup I (training set is fixed): paired -test. Then, we calculate the 1- α confidence interval and the p-value.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>raw_data <span class="op">=</span> df.to_numpy()</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>attr_col <span class="op">=</span> <span class="bu">list</span>(df.columns).index(<span class="st">'obesity'</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">0</span>, attr_col)) <span class="op">+</span> <span class="bu">list</span>(<span class="bu">range</span>(attr_col <span class="op">+</span> <span class="dv">1</span>, <span class="bu">len</span>(df.columns)))</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> raw_data[:, cols]</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> raw_data[:, attr_col]</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>attributeNames <span class="op">=</span> <span class="bu">list</span>(df.columns[cols])</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>N, M <span class="op">=</span> X.shape</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>K1 <span class="op">=</span> <span class="dv">10</span> <span class="co"># for model selection</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>K2 <span class="op">=</span> <span class="dv">10</span> <span class="co"># for optimal parameter selection</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co"># K-fold crossvalidation</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>CV1 <span class="op">=</span> model_selection.KFold(n_splits<span class="op">=</span>K1, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> stats.zscore(X)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize variable</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>linreg_test_error_k1 <span class="op">=</span> np.zeros(K1)</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>ann_test_error_k1 <span class="op">=</span> np.zeros(K1)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>baseline_test_error_k1 <span class="op">=</span> np.zeros(K1)</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>k1<span class="op">=</span><span class="dv">0</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> par_index, test_index <span class="kw">in</span> CV1.split(X):</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Computing CV1 fold: </span><span class="sc">{0}</span><span class="st">/</span><span class="sc">{1}</span><span class="st">..'</span>.<span class="bu">format</span>(k1<span class="op">+</span><span class="dv">1</span>,K1))</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># extract training and test set for current CV fold</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>    X_par, y_par <span class="op">=</span> X[par_index,:], y[par_index]</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>    X_test, y_test <span class="op">=</span> X[test_index,:], y[test_index]</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>    CV2 <span class="op">=</span> model_selection.KFold(n_splits<span class="op">=</span>K2, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Regularized Linear Regression---------------------------------------------------------------------------------</span></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>    lambda_interval <span class="op">=</span> np.power(<span class="fl">10.</span>,<span class="bu">range</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">9</span>))</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>    linreg_gen_error_rate_s <span class="op">=</span> np.zeros(<span class="bu">len</span>(lambda_interval))</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(lambda_interval)):</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>        k2 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>        linreg_val_error_rate <span class="op">=</span> np.zeros(K2)</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> train_index, val_index <span class="kw">in</span> CV2.split(X_par):</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>            <span class="co"># extract training and test set for current CV fold</span></span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>            X_train, y_train <span class="op">=</span> X_par[train_index,:], y_par[train_index]</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>            X_val, y_val <span class="op">=</span> X_par[val_index,:], y_par[val_index]</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>            linreg_model <span class="op">=</span> Ridge(alpha <span class="op">=</span> lambda_interval[s], fit_intercept <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>            linreg_model <span class="op">=</span> linreg_model.fit(X_train, y_train)</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a>            linreg_y_val_estimated <span class="op">=</span> linreg_model.predict(X_val).T</span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>            linreg_val_error_rate[k2] <span class="op">=</span> np.square(y_val <span class="op">-</span> linreg_y_val_estimated).<span class="bu">sum</span>() <span class="op">/</span> <span class="bu">len</span>(y_val)</span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>            k2 <span class="op">=</span> k2 <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>        linreg_gen_error_rate_s[s] <span class="op">=</span> np.<span class="bu">sum</span>(linreg_val_error_rate) <span class="op">/</span> <span class="bu">len</span>(linreg_val_error_rate)</span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>    linreg_min_error <span class="op">=</span> np.<span class="bu">min</span>(linreg_gen_error_rate_s)</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a>    opt_lambda_index <span class="op">=</span> np.argmin(linreg_gen_error_rate_s)</span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>    opt_lambda <span class="op">=</span> lambda_interval[opt_lambda_index]</span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a>    linreg_model <span class="op">=</span> Ridge(alpha <span class="op">=</span> lambda_interval[opt_lambda_index], fit_intercept <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a>    linreg_model <span class="op">=</span> linreg_model.fit(X_par, y_par)</span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a>    linreg_y_test_estimated <span class="op">=</span> linreg_model.predict(X_test).T</span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a>    linreg_test_error_k1[k1] <span class="op">=</span> np.square(y_test <span class="op">-</span> linreg_y_test_estimated).<span class="bu">sum</span>() <span class="op">/</span> <span class="bu">len</span>(y_test)</span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Error rate - regularized lin-reg - CV1 fold </span><span class="sc">{0}</span><span class="st">/</span><span class="sc">{1}</span><span class="st">: </span><span class="sc">{2}</span><span class="st">'</span>.<span class="bu">format</span>(k1<span class="op">+</span><span class="dv">1</span>, K1, np.<span class="bu">round</span>(linreg_test_error_k1[k1], decimals <span class="op">=</span> <span class="dv">2</span>)))</span>
<span id="cb19-68"><a href="#cb19-68" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Optimal lambda: </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(opt_lambda))</span>
<span id="cb19-69"><a href="#cb19-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-70"><a href="#cb19-70" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-71"><a href="#cb19-71" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ANN Regression -----------------------------------------------------------------------------------------------</span></span>
<span id="cb19-72"><a href="#cb19-72" aria-hidden="true" tabindex="-1"></a>    h_unit_interval <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>) <span class="co"># number of hidden units in the single hidden layer</span></span>
<span id="cb19-73"><a href="#cb19-73" aria-hidden="true" tabindex="-1"></a>    nr_of_nn_replicates <span class="op">=</span> <span class="dv">3</span> <span class="co"># when finding loss, take the best neural network from n replicates (to deal with local minima issues)</span></span>
<span id="cb19-74"><a href="#cb19-74" aria-hidden="true" tabindex="-1"></a>    max_iter <span class="op">=</span> <span class="dv">10000</span> <span class="co"># max nr. of epochs (if convergence is not yet reached)</span></span>
<span id="cb19-75"><a href="#cb19-75" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-76"><a href="#cb19-76" aria-hidden="true" tabindex="-1"></a>    ann_gen_error_rate_s <span class="op">=</span> np.zeros(<span class="bu">len</span>(h_unit_interval))</span>
<span id="cb19-77"><a href="#cb19-77" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-78"><a href="#cb19-78" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(h_unit_interval)):</span>
<span id="cb19-79"><a href="#cb19-79" aria-hidden="true" tabindex="-1"></a>        k2 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb19-80"><a href="#cb19-80" aria-hidden="true" tabindex="-1"></a>        ann_val_error_rate <span class="op">=</span> np.zeros(K2)</span>
<span id="cb19-81"><a href="#cb19-81" aria-hidden="true" tabindex="-1"></a>             </span>
<span id="cb19-82"><a href="#cb19-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> train_index, val_index <span class="kw">in</span> CV2.split(X_par):</span>
<span id="cb19-83"><a href="#cb19-83" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print('hello')</span></span>
<span id="cb19-84"><a href="#cb19-84" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb19-85"><a href="#cb19-85" aria-hidden="true" tabindex="-1"></a>            ann_model <span class="op">=</span> <span class="kw">lambda</span>: torch.nn.Sequential(</span>
<span id="cb19-86"><a href="#cb19-86" aria-hidden="true" tabindex="-1"></a>                                     torch.nn.Linear(M, h_unit_interval[s]),</span>
<span id="cb19-87"><a href="#cb19-87" aria-hidden="true" tabindex="-1"></a>                                     torch.nn.Tanh(),   </span>
<span id="cb19-88"><a href="#cb19-88" aria-hidden="true" tabindex="-1"></a>                                     torch.nn.Linear(h_unit_interval[s], <span class="dv">1</span>), <span class="co"># H hidden units to 1 output neuron</span></span>
<span id="cb19-89"><a href="#cb19-89" aria-hidden="true" tabindex="-1"></a>                                     <span class="co"># no final tranfer function, since we are interested in the "linear output"</span></span>
<span id="cb19-90"><a href="#cb19-90" aria-hidden="true" tabindex="-1"></a>                                 )</span>
<span id="cb19-91"><a href="#cb19-91" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb19-92"><a href="#cb19-92" aria-hidden="true" tabindex="-1"></a>            <span class="co">#extract training and test set for current CV fold</span></span>
<span id="cb19-93"><a href="#cb19-93" aria-hidden="true" tabindex="-1"></a>            X_train <span class="op">=</span> torch.tensor(X_par[train_index,:], dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb19-94"><a href="#cb19-94" aria-hidden="true" tabindex="-1"></a>            y_train <span class="op">=</span> torch.tensor(y_par[train_index], dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb19-95"><a href="#cb19-95" aria-hidden="true" tabindex="-1"></a>            X_val <span class="op">=</span> torch.tensor(X_par[val_index,:], dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb19-96"><a href="#cb19-96" aria-hidden="true" tabindex="-1"></a>            y_val <span class="op">=</span> torch.tensor(y_par[val_index], dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb19-97"><a href="#cb19-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-98"><a href="#cb19-98" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb19-99"><a href="#cb19-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-100"><a href="#cb19-100" aria-hidden="true" tabindex="-1"></a>            loss_fn <span class="op">=</span> torch.nn.MSELoss() <span class="co"># Mean squared error loss function</span></span>
<span id="cb19-101"><a href="#cb19-101" aria-hidden="true" tabindex="-1"></a>            best_trained_neural_net, final_loss, learning_curve <span class="op">=</span> train_neural_net(ann_model, loss_fn, X <span class="op">=</span> X_train, y <span class="op">=</span> y_train, n_replicates <span class="op">=</span> nr_of_nn_replicates, max_iter <span class="op">=</span> max_iter)</span>
<span id="cb19-102"><a href="#cb19-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-103"><a href="#cb19-103" aria-hidden="true" tabindex="-1"></a>            ann_y_val_estimated <span class="op">=</span> best_trained_neural_net(X_val)</span>
<span id="cb19-104"><a href="#cb19-104" aria-hidden="true" tabindex="-1"></a>             <span class="co"># Convert tensors to numpy arrays, to work smoothly with class comparisons</span></span>
<span id="cb19-105"><a href="#cb19-105" aria-hidden="true" tabindex="-1"></a>            ann_y_val_estimated <span class="op">=</span> ann_y_val_estimated.detach().numpy().reshape(<span class="bu">len</span>(ann_y_val_estimated))</span>
<span id="cb19-106"><a href="#cb19-106" aria-hidden="true" tabindex="-1"></a>            y_val <span class="op">=</span> y_val.numpy().reshape(<span class="bu">len</span>(y_val))</span>
<span id="cb19-107"><a href="#cb19-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-108"><a href="#cb19-108" aria-hidden="true" tabindex="-1"></a>            ann_val_error_rate[k2] <span class="op">=</span> np.square(y_val <span class="op">-</span> ann_y_val_estimated).<span class="bu">sum</span>() <span class="op">/</span> <span class="bu">len</span>(y_val)</span>
<span id="cb19-109"><a href="#cb19-109" aria-hidden="true" tabindex="-1"></a>            k2 <span class="op">=</span> k2 <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb19-110"><a href="#cb19-110" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb19-111"><a href="#cb19-111" aria-hidden="true" tabindex="-1"></a>        ann_gen_error_rate_s[s] <span class="op">=</span> np.<span class="bu">sum</span>(ann_val_error_rate) <span class="op">/</span> <span class="bu">len</span>(ann_val_error_rate)</span>
<span id="cb19-112"><a href="#cb19-112" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb19-113"><a href="#cb19-113" aria-hidden="true" tabindex="-1"></a>    ann_min_error <span class="op">=</span> np.<span class="bu">min</span>(ann_gen_error_rate_s)</span>
<span id="cb19-114"><a href="#cb19-114" aria-hidden="true" tabindex="-1"></a>    opt_nr_h_units_index <span class="op">=</span> np.argmin(ann_gen_error_rate_s)</span>
<span id="cb19-115"><a href="#cb19-115" aria-hidden="true" tabindex="-1"></a>    opt_nr_h_units <span class="op">=</span> h_unit_interval[opt_nr_h_units_index]</span>
<span id="cb19-116"><a href="#cb19-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-117"><a href="#cb19-117" aria-hidden="true" tabindex="-1"></a>    tensor_X_par <span class="op">=</span> torch.tensor(X_par, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb19-118"><a href="#cb19-118" aria-hidden="true" tabindex="-1"></a>    tensor_y_par <span class="op">=</span> torch.tensor(y_par, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb19-119"><a href="#cb19-119" aria-hidden="true" tabindex="-1"></a>    tensor_X_test <span class="op">=</span> torch.tensor(X_test, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb19-120"><a href="#cb19-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-121"><a href="#cb19-121" aria-hidden="true" tabindex="-1"></a>    ann_model <span class="op">=</span> <span class="kw">lambda</span>: torch.nn.Sequential(</span>
<span id="cb19-122"><a href="#cb19-122" aria-hidden="true" tabindex="-1"></a>                                torch.nn.Linear(M, h_unit_interval[opt_nr_h_units_index]),</span>
<span id="cb19-123"><a href="#cb19-123" aria-hidden="true" tabindex="-1"></a>                                torch.nn.Tanh(),   </span>
<span id="cb19-124"><a href="#cb19-124" aria-hidden="true" tabindex="-1"></a>                                torch.nn.Linear(h_unit_interval[opt_nr_h_units_index], <span class="dv">1</span>), <span class="co"># H hidden units to 1 output neuron</span></span>
<span id="cb19-125"><a href="#cb19-125" aria-hidden="true" tabindex="-1"></a>                                <span class="co"># no final tranfer function, since we are interested in the "linear output"</span></span>
<span id="cb19-126"><a href="#cb19-126" aria-hidden="true" tabindex="-1"></a>                            )</span>
<span id="cb19-127"><a href="#cb19-127" aria-hidden="true" tabindex="-1"></a>    loss_fn <span class="op">=</span> torch.nn.MSELoss() <span class="co"># Binary classification loss</span></span>
<span id="cb19-128"><a href="#cb19-128" aria-hidden="true" tabindex="-1"></a>    best_trained_neural_net, final_loss, learning_curve <span class="op">=</span> train_neural_net(ann_model, loss_fn, X <span class="op">=</span> tensor_X_par, y <span class="op">=</span> tensor_y_par, n_replicates <span class="op">=</span> nr_of_nn_replicates, max_iter <span class="op">=</span> max_iter)</span>
<span id="cb19-129"><a href="#cb19-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-130"><a href="#cb19-130" aria-hidden="true" tabindex="-1"></a>    ann_y_test_estimated <span class="op">=</span> best_trained_neural_net(tensor_X_test)</span>
<span id="cb19-131"><a href="#cb19-131" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert tensors to numpy arrays, to work smoothly with class comparisons</span></span>
<span id="cb19-132"><a href="#cb19-132" aria-hidden="true" tabindex="-1"></a>    ann_y_test_estimated <span class="op">=</span> ann_y_test_estimated.detach().numpy().reshape(<span class="bu">len</span>(ann_y_test_estimated))</span>
<span id="cb19-133"><a href="#cb19-133" aria-hidden="true" tabindex="-1"></a>    ann_test_error_k1[k1] <span class="op">=</span> np.square(y_test <span class="op">-</span> ann_y_test_estimated).<span class="bu">sum</span>() <span class="op">/</span> <span class="bu">len</span>(y_test)</span>
<span id="cb19-134"><a href="#cb19-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-135"><a href="#cb19-135" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Error rate - Regression ANN - CV1 fold </span><span class="sc">{0}</span><span class="st">/</span><span class="sc">{1}</span><span class="st">: </span><span class="sc">{2}</span><span class="st">'</span>.<span class="bu">format</span>(k1<span class="op">+</span><span class="dv">1</span>, K1, np.<span class="bu">round</span>(ann_test_error_k1[k1], decimals <span class="op">=</span> <span class="dv">2</span>)))</span>
<span id="cb19-136"><a href="#cb19-136" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Optimal number of hidden units: </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(opt_nr_h_units))</span>
<span id="cb19-137"><a href="#cb19-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-138"><a href="#cb19-138" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-139"><a href="#cb19-139" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Baseline - LinearRegression ---------------------------------------------------------------------------------</span></span>
<span id="cb19-140"><a href="#cb19-140" aria-hidden="true" tabindex="-1"></a>    baseline_test_error_k1[k1] <span class="op">=</span> np.square(y_test <span class="op">-</span> y_par.mean()).<span class="bu">sum</span>() <span class="op">/</span> <span class="bu">len</span>(y_test)</span>
<span id="cb19-141"><a href="#cb19-141" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-142"><a href="#cb19-142" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Error rate - baseline lin-reg - CV1 fold </span><span class="sc">{0}</span><span class="st">/</span><span class="sc">{1}</span><span class="st">: </span><span class="sc">{2}</span><span class="st">'</span>.<span class="bu">format</span>(k1<span class="op">+</span><span class="dv">1</span>, K1, np.<span class="bu">round</span>(baseline_test_error_k1[k1], decimals <span class="op">=</span> <span class="dv">2</span>)))</span>
<span id="cb19-143"><a href="#cb19-143" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-144"><a href="#cb19-144" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-145"><a href="#cb19-145" aria-hidden="true" tabindex="-1"></a>    k1 <span class="op">=</span> k1 <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb19-146"><a href="#cb19-146" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb19-147"><a href="#cb19-147" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb19-148"><a href="#cb19-148" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-149"><a href="#cb19-149" aria-hidden="true" tabindex="-1"></a><span class="co"># Statistical evaluation of models</span></span>
<span id="cb19-150"><a href="#cb19-150" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize <span class="op">=</span> [<span class="dv">18</span>, <span class="dv">5</span>])</span>
<span id="cb19-151"><a href="#cb19-151" aria-hidden="true" tabindex="-1"></a>plt.subplots_adjust(wspace <span class="op">=</span> <span class="fl">0.4</span>)</span>
<span id="cb19-152"><a href="#cb19-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-153"><a href="#cb19-153" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> (ann_test_error_k1 <span class="op">-</span> linreg_test_error_k1)</span>
<span id="cb19-154"><a href="#cb19-154" aria-hidden="true" tabindex="-1"></a>z_mean <span class="op">=</span> z.mean()</span>
<span id="cb19-155"><a href="#cb19-155" aria-hidden="true" tabindex="-1"></a>deg_f <span class="op">=</span> K1<span class="op">-</span><span class="dv">1</span></span>
<span id="cb19-156"><a href="#cb19-156" aria-hidden="true" tabindex="-1"></a>sig <span class="op">=</span>  (z <span class="op">-</span> z_mean).std() <span class="op">/</span> np.sqrt(deg_f)</span>
<span id="cb19-157"><a href="#cb19-157" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb19-158"><a href="#cb19-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-159"><a href="#cb19-159" aria-hidden="true" tabindex="-1"></a>zL <span class="op">=</span> z_mean <span class="op">+</span> sig <span class="op">*</span> stats.t.ppf(alpha<span class="op">/</span><span class="dv">2</span>, deg_f)<span class="op">;</span></span>
<span id="cb19-160"><a href="#cb19-160" aria-hidden="true" tabindex="-1"></a>zH <span class="op">=</span> z_mean <span class="op">+</span> sig <span class="op">*</span> stats.t.ppf(<span class="dv">1</span><span class="op">-</span>alpha<span class="op">/</span><span class="dv">2</span>, deg_f)<span class="op">;</span></span>
<span id="cb19-161"><a href="#cb19-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-162"><a href="#cb19-162" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb19-163"><a href="#cb19-163" aria-hidden="true" tabindex="-1"></a>plt.boxplot(np.concatenate((ann_test_error_k1.reshape((<span class="bu">len</span>(ann_test_error_k1), <span class="dv">1</span>)), linreg_test_error_k1.reshape((<span class="bu">len</span>(linreg_test_error_k1), <span class="dv">1</span>))), axis <span class="op">=</span> <span class="dv">1</span>))</span>
<span id="cb19-164"><a href="#cb19-164" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Regression ANN vs. Regularized Linear Regression'</span>)</span>
<span id="cb19-165"><a href="#cb19-165" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cross-validation error'</span>)</span>
<span id="cb19-166"><a href="#cb19-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-167"><a href="#cb19-167" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Credibility interval has z-scores: (</span><span class="sc">{0}</span><span class="st">, </span><span class="sc">{1}</span><span class="st">)'</span>.<span class="bu">format</span>(np.<span class="bu">round</span>(zL, decimals <span class="op">=</span> <span class="dv">2</span>), np.<span class="bu">round</span>(zH, decimals <span class="op">=</span> <span class="dv">2</span>)))</span>
<span id="cb19-168"><a href="#cb19-168" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> zL <span class="op">&lt;=</span> <span class="dv">0</span> <span class="kw">and</span> zH <span class="op">&gt;=</span> <span class="dv">0</span> :</span>
<span id="cb19-169"><a href="#cb19-169" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Regression ANN and regularized lin-reg are NOT significantly different'</span>)        </span>
<span id="cb19-170"><a href="#cb19-170" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb19-171"><a href="#cb19-171" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Regression ANN and regularized lin-reg are significantly different.'</span>)</span>
<span id="cb19-172"><a href="#cb19-172" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb19-173"><a href="#cb19-173" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb19-174"><a href="#cb19-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-175"><a href="#cb19-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-176"><a href="#cb19-176" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> (ann_test_error_k1 <span class="op">-</span> baseline_test_error_k1)</span>
<span id="cb19-177"><a href="#cb19-177" aria-hidden="true" tabindex="-1"></a>z_mean <span class="op">=</span> z.mean()</span>
<span id="cb19-178"><a href="#cb19-178" aria-hidden="true" tabindex="-1"></a>deg_f <span class="op">=</span> K1<span class="op">-</span><span class="dv">1</span></span>
<span id="cb19-179"><a href="#cb19-179" aria-hidden="true" tabindex="-1"></a>sig <span class="op">=</span>  (z <span class="op">-</span> z_mean).std() <span class="op">/</span> np.sqrt(deg_f)</span>
<span id="cb19-180"><a href="#cb19-180" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb19-181"><a href="#cb19-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-182"><a href="#cb19-182" aria-hidden="true" tabindex="-1"></a>zL <span class="op">=</span> z_mean <span class="op">+</span> sig <span class="op">*</span> stats.t.ppf(alpha<span class="op">/</span><span class="dv">2</span>, deg_f)<span class="op">;</span></span>
<span id="cb19-183"><a href="#cb19-183" aria-hidden="true" tabindex="-1"></a>zH <span class="op">=</span> z_mean <span class="op">+</span> sig <span class="op">*</span> stats.t.ppf(<span class="dv">1</span><span class="op">-</span>alpha<span class="op">/</span><span class="dv">2</span>, deg_f)<span class="op">;</span></span>
<span id="cb19-184"><a href="#cb19-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-185"><a href="#cb19-185" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb19-186"><a href="#cb19-186" aria-hidden="true" tabindex="-1"></a>plt.boxplot(np.concatenate((ann_test_error_k1.reshape((<span class="bu">len</span>(ann_test_error_k1), <span class="dv">1</span>)), baseline_test_error_k1.reshape((<span class="bu">len</span>(baseline_test_error_k1), <span class="dv">1</span>))), axis <span class="op">=</span> <span class="dv">1</span>))</span>
<span id="cb19-187"><a href="#cb19-187" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Regression ANN vs. Baseline-form Linear Regression'</span>)</span>
<span id="cb19-188"><a href="#cb19-188" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cross-validation error'</span>)</span>
<span id="cb19-189"><a href="#cb19-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-190"><a href="#cb19-190" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Credibility interval has z-scores: (</span><span class="sc">{0}</span><span class="st">, </span><span class="sc">{1}</span><span class="st">)'</span>.<span class="bu">format</span>(np.<span class="bu">round</span>(zL, decimals <span class="op">=</span> <span class="dv">2</span>), np.<span class="bu">round</span>(zH, decimals <span class="op">=</span> <span class="dv">2</span>)))</span>
<span id="cb19-191"><a href="#cb19-191" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> zL <span class="op">&lt;=</span> <span class="dv">0</span> <span class="kw">and</span> zH <span class="op">&gt;=</span> <span class="dv">0</span> :</span>
<span id="cb19-192"><a href="#cb19-192" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Regression ANN and baseline lin-reg classifiers are   NOT significantly different'</span>)        </span>
<span id="cb19-193"><a href="#cb19-193" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb19-194"><a href="#cb19-194" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Regression ANN and baseline lin-reg classifiers are significantly different.'</span>)</span>
<span id="cb19-195"><a href="#cb19-195" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb19-196"><a href="#cb19-196" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb19-197"><a href="#cb19-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-198"><a href="#cb19-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-199"><a href="#cb19-199" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> (linreg_test_error_k1 <span class="op">-</span> baseline_test_error_k1)</span>
<span id="cb19-200"><a href="#cb19-200" aria-hidden="true" tabindex="-1"></a>z_mean <span class="op">=</span> z.mean()</span>
<span id="cb19-201"><a href="#cb19-201" aria-hidden="true" tabindex="-1"></a>deg_f <span class="op">=</span> K1<span class="op">-</span><span class="dv">1</span></span>
<span id="cb19-202"><a href="#cb19-202" aria-hidden="true" tabindex="-1"></a>sig <span class="op">=</span>  (z <span class="op">-</span> z_mean).std() <span class="op">/</span> np.sqrt(deg_f)</span>
<span id="cb19-203"><a href="#cb19-203" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb19-204"><a href="#cb19-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-205"><a href="#cb19-205" aria-hidden="true" tabindex="-1"></a>zL <span class="op">=</span> z_mean <span class="op">+</span> sig <span class="op">*</span> stats.t.ppf(alpha<span class="op">/</span><span class="dv">2</span>, deg_f)<span class="op">;</span></span>
<span id="cb19-206"><a href="#cb19-206" aria-hidden="true" tabindex="-1"></a>zH <span class="op">=</span> z_mean <span class="op">+</span> sig <span class="op">*</span> stats.t.ppf(<span class="dv">1</span><span class="op">-</span>alpha<span class="op">/</span><span class="dv">2</span>, deg_f)<span class="op">;</span></span>
<span id="cb19-207"><a href="#cb19-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-208"><a href="#cb19-208" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb19-209"><a href="#cb19-209" aria-hidden="true" tabindex="-1"></a>plt.boxplot(np.concatenate((linreg_test_error_k1.reshape((<span class="bu">len</span>(linreg_test_error_k1), <span class="dv">1</span>)), baseline_test_error_k1.reshape((<span class="bu">len</span>(baseline_test_error_k1), <span class="dv">1</span>))), axis <span class="op">=</span> <span class="dv">1</span>))</span>
<span id="cb19-210"><a href="#cb19-210" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Regularized Linear Regression vs. Baseline-form Linear Regression'</span>)</span>
<span id="cb19-211"><a href="#cb19-211" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cross-validation error'</span>)</span>
<span id="cb19-212"><a href="#cb19-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-213"><a href="#cb19-213" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Credibility interval has z-scores: (</span><span class="sc">{0}</span><span class="st">, </span><span class="sc">{1}</span><span class="st">)'</span>.<span class="bu">format</span>(np.<span class="bu">round</span>(zL, decimals <span class="op">=</span> <span class="dv">2</span>), np.<span class="bu">round</span>(zH, decimals <span class="op">=</span> <span class="dv">2</span>)))</span>
<span id="cb19-214"><a href="#cb19-214" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> zL <span class="op">&lt;=</span> <span class="dv">0</span> <span class="kw">and</span> zH <span class="op">&gt;=</span> <span class="dv">0</span> :</span>
<span id="cb19-215"><a href="#cb19-215" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Regularized lin-reg and baseline lin-reg cla  ssifiers are NOT significantly different'</span>)        </span>
<span id="cb19-216"><a href="#cb19-216" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb19-217"><a href="#cb19-217" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Regularized lin-reg and baseline lin-reg classifiers are significantly different.'</span>)</span>
<span id="cb19-218"><a href="#cb19-218" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb19-219"><a href="#cb19-219" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb19-220"><a href="#cb19-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-221"><a href="#cb19-221" aria-hidden="true" tabindex="-1"></a><span class="co"># Generalized error rates </span></span>
<span id="cb19-222"><a href="#cb19-222" aria-hidden="true" tabindex="-1"></a>gen_error_regularized_linreg <span class="op">=</span> np.<span class="bu">sum</span>(linreg_test_error_k1) <span class="op">/</span> <span class="bu">len</span>(linreg_test_error_k1)</span>
<span id="cb19-223"><a href="#cb19-223" aria-hidden="true" tabindex="-1"></a>gen_error_ann <span class="op">=</span> np.<span class="bu">sum</span>(ann_test_error_k1) <span class="op">/</span> <span class="bu">len</span>(ann_test_error_k1)</span>
<span id="cb19-224"><a href="#cb19-224" aria-hidden="true" tabindex="-1"></a>gen_error_baseline_linreg <span class="op">=</span> np.<span class="bu">sum</span>(baseline_test_error_k1) <span class="op">/</span> <span class="bu">len</span>(baseline_test_error_k1)</span>
<span id="cb19-225"><a href="#cb19-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-226"><a href="#cb19-226" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Generalized error rate - regularized lin-reg: </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(np.<span class="bu">round</span>(gen_error_regularized_linreg, decimals <span class="op">=</span> <span class="dv">2</span>)))</span>
<span id="cb19-227"><a href="#cb19-227" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Generalized error rate - regression ANN: </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(np.<span class="bu">round</span>(gen_error_ann, decimals <span class="op">=</span> <span class="dv">2</span>)))</span>
<span id="cb19-228"><a href="#cb19-228" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Generalized error rate - baseline lin-reg: </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(np.<span class="bu">round</span>(gen_error_baseline_linreg, decimals <span class="op">=</span> <span class="dv">2</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Computing CV1 fold: 1/10..

Error rate - regularized lin-reg - CV1 fold 1/10: 9.93
Optimal lambda: 1.0
Error rate - Regression ANN - CV1 fold 1/10: 19.7
Optimal number of hidden units: 3
Error rate - baseline lin-reg - CV1 fold 1/10: 19.63


Computing CV1 fold: 2/10..

Error rate - regularized lin-reg - CV1 fold 2/10: 6.74
Optimal lambda: 1.0
Error rate - Regression ANN - CV1 fold 2/10: 11.9
Optimal number of hidden units: 3
Error rate - baseline lin-reg - CV1 fold 2/10: 11.79


Computing CV1 fold: 3/10..

Error rate - regularized lin-reg - CV1 fold 3/10: 5.38
Optimal lambda: 1.0
Error rate - Regression ANN - CV1 fold 3/10: 11.17
Optimal number of hidden units: 3
Error rate - baseline lin-reg - CV1 fold 3/10: 11.22


Computing CV1 fold: 4/10..

Error rate - regularized lin-reg - CV1 fold 4/10: 5.37
Optimal lambda: 1.0
Error rate - Regression ANN - CV1 fold 4/10: 10.81
Optimal number of hidden units: 3
Error rate - baseline lin-reg - CV1 fold 4/10: 10.9


Computing CV1 fold: 5/10..

Error rate - regularized lin-reg - CV1 fold 5/10: 8.78
Optimal lambda: 1.0
Error rate - Regression ANN - CV1 fold 5/10: 26.12
Optimal number of hidden units: 3
Error rate - baseline lin-reg - CV1 fold 5/10: 25.99


Computing CV1 fold: 6/10..

Error rate - regularized lin-reg - CV1 fold 6/10: 5.42
Optimal lambda: 1.0
Error rate - Regression ANN - CV1 fold 6/10: 14.16
Optimal number of hidden units: 3
Error rate - baseline lin-reg - CV1 fold 6/10: 14.18


Computing CV1 fold: 7/10..

Error rate - regularized lin-reg - CV1 fold 7/10: 4.8
Optimal lambda: 1.0
Error rate - Regression ANN - CV1 fold 7/10: 11.87
Optimal number of hidden units: 3
Error rate - baseline lin-reg - CV1 fold 7/10: 11.88


Computing CV1 fold: 8/10..

Error rate - regularized lin-reg - CV1 fold 8/10: 9.71
Optimal lambda: 1.0
Error rate - Regression ANN - CV1 fold 8/10: 29.91
Optimal number of hidden units: 3
Error rate - baseline lin-reg - CV1 fold 8/10: 29.87


Computing CV1 fold: 9/10..

Error rate - regularized lin-reg - CV1 fold 9/10: 18.64
Optimal lambda: 1.0
Error rate - Regression ANN - CV1 fold 9/10: 25.1
Optimal number of hidden units: 3
Error rate - baseline lin-reg - CV1 fold 9/10: 25.22


Computing CV1 fold: 10/10..

Error rate - regularized lin-reg - CV1 fold 10/10: 5.25
Optimal lambda: 1.0
Error rate - Regression ANN - CV1 fold 10/10: 17.3
Optimal number of hidden units: 3
Error rate - baseline lin-reg - CV1 fold 10/10: 17.31


Credibility interval has z-scores: (6.06, 13.55)
Regression ANN and regularized lin-reg are significantly different.


Credibility interval has z-scores: (-0.05, 0.06)
Regression ANN and baseline lin-reg classifiers are   NOT significantly different


Credibility interval has z-scores: (-13.51, -6.08)
Regularized lin-reg and baseline lin-reg classifiers are significantly different.


Generalized error rate - regularized lin-reg: 8.0
Generalized error rate - regression ANN: 17.8
Generalized error rate - baseline lin-reg: 17.8</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="Project2_ML_files/figure-html/cell-13-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>The best model for our dataset is the regularized linear regression as it achieves the lowest error.</p>
<p>The optimal λ is 1 as obtained in the previous exercise and the best number of hidden layers is 3.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Statistical comparison of models</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute confidence interval of z = zA-zB and p-value of Null hypothesis</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"># ANN vs. regularized linear regression </span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>z1 <span class="op">=</span> (ann_test_error_k1 <span class="op">-</span> linreg_test_error_k1)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>CI1 <span class="op">=</span> stats.t.interval(<span class="dv">1</span><span class="op">-</span>alpha, <span class="bu">len</span>(z1)<span class="op">-</span><span class="dv">1</span>, loc<span class="op">=</span>np.mean(z1), scale<span class="op">=</span>stats.sem(z1))  <span class="co"># Confidence interval</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>p1 <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>stats.t.cdf(<span class="op">-</span>np.<span class="bu">abs</span>( np.mean(z1) )<span class="op">/</span>stats.sem(z1), df<span class="op">=</span><span class="bu">len</span>(z1)<span class="op">-</span><span class="dv">1</span>)  <span class="co"># p-value</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'ANN vs. regularized linear regression'</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confidence interval:'</span>, CI1)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'p-value:'</span>, p1)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="co"># ANN vs. baseline</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>z2 <span class="op">=</span> (ann_test_error_k1 <span class="op">-</span> baseline_test_error_k1)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>CI2 <span class="op">=</span> stats.t.interval(<span class="dv">1</span><span class="op">-</span>alpha, <span class="bu">len</span>(z2)<span class="op">-</span><span class="dv">1</span>, loc<span class="op">=</span>np.mean(z2), scale<span class="op">=</span>stats.sem(z2))  <span class="co"># Confidence interval</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>p2 <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>stats.t.cdf(<span class="op">-</span>np.<span class="bu">abs</span>( np.mean(z2) )<span class="op">/</span>stats.sem(z2), df<span class="op">=</span><span class="bu">len</span>(z2)<span class="op">-</span><span class="dv">1</span>)  <span class="co"># p-value</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'ANN vs. baseline'</span>)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confidence interval:'</span>, CI2)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'p-value:'</span>, p2)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a><span class="co"># regularized linear regression vs. baseline</span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>z3 <span class="op">=</span> (linreg_test_error_k1 <span class="op">-</span> baseline_test_error_k1)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>CI3 <span class="op">=</span> stats.t.interval(<span class="dv">1</span><span class="op">-</span>alpha, <span class="bu">len</span>(z3)<span class="op">-</span><span class="dv">1</span>, loc<span class="op">=</span>np.mean(z3), scale<span class="op">=</span>stats.sem(z3))  <span class="co"># Confidence interval</span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>p3 <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>stats.t.cdf( <span class="op">-</span>np.<span class="bu">abs</span>( np.mean(z3) )<span class="op">/</span>stats.sem(z3), df<span class="op">=</span><span class="bu">len</span>(z3)<span class="op">-</span><span class="dv">1</span>)  <span class="co"># p-value</span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Regularized linear regression vs. baseline'</span>)</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confidence interval:'</span>, CI3)</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'p-value:'</span>, p3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ANN vs. regularized linear regression
Confidence interval: (6.057463143680125, 13.545235443619362)
p-value: 0.00022284981919282824

ANN vs. baseline
Confidence interval: (-0.05424606240787217, 0.06419466703832133)
p-value: 0.8535153425885907

Regularized linear regression vs. baseline
Confidence interval: (-13.51169331642087, -6.0810566662481715)
p-value: 0.00021146298100565937</code></pre>
</div>
</div>
<p>The confidence intervals of the ANN and baseline models coincide, which means that there is no difference between the metrics and, in fact, the generalised error rate is the same. However, if we compare the intervals of these two models with that of the regularised linear regression, we can see that there is no overlap and therefore the difference between the metrics is statistically significant.</p>
</section>
<section id="classification-models" class="level1">
<h1>Classification Models</h1>
</section>
<section id="classification-models-1" class="level1">
<h1>Classification Models</h1>
<p>For classification, we have chosen to solve the models predicting whether or not the patient suffers of coronary hear disease (i.e.&nbsp;if the variable ‘chd’ is either 0 or 1), by using the rest of our attributes, and again standardizing the values for better results.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>raw_data <span class="op">=</span> df.to_numpy()</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>attr_col <span class="op">=</span> <span class="bu">list</span>(df.columns).index(<span class="st">'chd'</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">0</span>, attr_col)) <span class="op">+</span> <span class="bu">list</span>(<span class="bu">range</span>(attr_col <span class="op">+</span> <span class="dv">1</span>, <span class="bu">len</span>(df.columns)))</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> raw_data[:, cols]</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> raw_data[:, attr_col] <span class="co"># the 'target' column</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>attributeNames <span class="op">=</span> <span class="bu">list</span>(df.columns[cols])</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>classLabels <span class="op">=</span> raw_data[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>classNames <span class="op">=</span> np.unique(classLabels)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>N, M <span class="op">=</span> X.shape</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> <span class="bu">len</span>(classNames)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>K1 <span class="op">=</span> <span class="dv">10</span> <span class="co"># for model selection</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>K2 <span class="op">=</span> <span class="dv">10</span> <span class="co"># for optimal parameter selection</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="co"># K-fold crossvalidation</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>CV1 <span class="op">=</span> model_selection.KFold(n_splits<span class="op">=</span>K1, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> stats.zscore(X)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize variable</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>logreg_test_error_k1 <span class="op">=</span> np.zeros(K1)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a><span class="co"># tree_test_error_k1 = np.zeros(K1)</span></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>ann_test_error_k1 <span class="op">=</span> np.zeros(K1)</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>baseline_test_error_k1 <span class="op">=</span> np.zeros(K1)</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>k1<span class="op">=</span><span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> par_index, test_index <span class="kw">in</span> CV1.split(X):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Computing CV1 fold: </span><span class="sc">{0}</span><span class="st">/</span><span class="sc">{1}</span><span class="st">..'</span>.<span class="bu">format</span>(k1<span class="op">+</span><span class="dv">1</span>,K1))</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># extract training and test set for current CV fold</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    X_par, y_par <span class="op">=</span> X[par_index,:], y[par_index]</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    X_test, y_test <span class="op">=</span> X[test_index,:], y[test_index]</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    CV2 <span class="op">=</span> model_selection.KFold(n_splits<span class="op">=</span>K2, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">#--------------------------------Regularized - LogRegression ----------------------------------------#</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    lambda_interval <span class="op">=</span> np.power(<span class="fl">10.</span>,<span class="bu">range</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">9</span>))</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    logreg_gen_error_rate_s <span class="op">=</span> np.zeros(<span class="bu">len</span>(lambda_interval))</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(lambda_interval)):</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>        k2 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>        logreg_val_error_rate <span class="op">=</span> np.zeros(K2)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> train_index, val_index <span class="kw">in</span> CV2.split(X_par):</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># extract training and test set for current CV fold</span></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>            X_train, y_train <span class="op">=</span> X_par[train_index,:], y_par[train_index]</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>            X_val, y_val <span class="op">=</span> X_par[val_index,:], y_par[val_index]</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>            logreg_model <span class="op">=</span> LogisticRegression(penalty<span class="op">=</span><span class="st">'l2'</span>, C<span class="op">=</span><span class="dv">1</span><span class="op">/</span>lambda_interval[s], solver <span class="op">=</span> <span class="st">'lbfgs'</span>)</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>            logreg_model <span class="op">=</span> logreg_model.fit(X_train, y_train)</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>            logreg_y_val_estimated <span class="op">=</span> logreg_model.predict(X_val).T</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>            logreg_val_error_rate[k2] <span class="op">=</span> np.<span class="bu">sum</span>(logreg_y_val_estimated <span class="op">!=</span> y_val) <span class="op">/</span> <span class="bu">len</span>(y_val)</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>            k2 <span class="op">=</span> k2 <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>        logreg_gen_error_rate_s[s] <span class="op">=</span> np.<span class="bu">sum</span>(logreg_val_error_rate) <span class="op">/</span> <span class="bu">len</span>(logreg_val_error_rate)</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>    logreg_min_error <span class="op">=</span> np.<span class="bu">min</span>(logreg_gen_error_rate_s)</span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>    opt_lambda_index <span class="op">=</span> np.argmin(logreg_gen_error_rate_s)</span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>    opt_lambda <span class="op">=</span> lambda_interval[opt_lambda_index]</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>    logreg_model <span class="op">=</span> LogisticRegression(penalty<span class="op">=</span><span class="st">'l2'</span>, C<span class="op">=</span><span class="dv">1</span><span class="op">/</span>lambda_interval[opt_lambda_index], solver <span class="op">=</span> <span class="st">'lbfgs'</span>)</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a>    logreg_model <span class="op">=</span> logreg_model.fit(X_par, y_par)</span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>    logreg_y_test_estimated <span class="op">=</span> logreg_model.predict(X_test).T</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>    logreg_test_error_k1[k1] <span class="op">=</span> np.<span class="bu">sum</span>(logreg_y_test_estimated <span class="op">!=</span> y_test) <span class="op">/</span> <span class="bu">len</span>(y_test)</span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Error rate - regularized log-reg - CV1 fold </span><span class="sc">{0}</span><span class="st">/</span><span class="sc">{1}</span><span class="st">: </span><span class="sc">{2}</span><span class="st">%'</span>.<span class="bu">format</span>(k1<span class="op">+</span><span class="dv">1</span>, K1, np.<span class="bu">round</span>(<span class="dv">100</span> <span class="op">*</span> logreg_test_error_k1[k1], decimals <span class="op">=</span> <span class="dv">2</span>)))</span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Optimal lambda: </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(opt_lambda))</span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>    h_unit_interval <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>) <span class="co"># number of hidden units in the single hidden layer</span></span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>    nr_of_nn_replicates <span class="op">=</span> <span class="dv">3</span> <span class="co"># when finding loss, take the best neural network from n replicates (to deal with local minima issues)</span></span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>    max_iter <span class="op">=</span> <span class="dv">10000</span> <span class="co"># max nr. of epochs (if convergence is not yet reached)</span></span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>    ann_gen_error_rate_s <span class="op">=</span> np.zeros(<span class="bu">len</span>(h_unit_interval))</span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(h_unit_interval)):</span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a>        k2 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a>        ann_val_error_rate <span class="op">=</span> np.zeros(K2)</span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> train_index, val_index <span class="kw">in</span> CV2.split(X_par):</span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a>            <span class="co"># extract training and test set for current CV fold</span></span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a>            <span class="co">#X_par = X_par.unsqueeze(-1)</span></span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a>            X_train <span class="op">=</span> torch.tensor(X_par[train_index,:], dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a>            <span class="co">#X_train = torch.tensor(X_par[train_index,:])</span></span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>            <span class="co">#X_train = X_train.unsqueeze(1)</span></span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a>            <span class="co">#X_train = X_train.float()</span></span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>            y_train <span class="op">=</span> torch.tensor(y_par[train_index], dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a>            y_train <span class="op">=</span> y_train.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb24-68"><a href="#cb24-68" aria-hidden="true" tabindex="-1"></a>            X_val <span class="op">=</span> torch.tensor(X_par[val_index,:], dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb24-69"><a href="#cb24-69" aria-hidden="true" tabindex="-1"></a>            y_val <span class="op">=</span> torch.tensor(y_par[val_index], dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb24-70"><a href="#cb24-70" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb24-71"><a href="#cb24-71" aria-hidden="true" tabindex="-1"></a>            ann_model <span class="op">=</span> <span class="kw">lambda</span>: torch.nn.Sequential(</span>
<span id="cb24-72"><a href="#cb24-72" aria-hidden="true" tabindex="-1"></a>                                    torch.nn.Linear(M, h_unit_interval[s]),</span>
<span id="cb24-73"><a href="#cb24-73" aria-hidden="true" tabindex="-1"></a>                                    torch.nn.Tanh(),   </span>
<span id="cb24-74"><a href="#cb24-74" aria-hidden="true" tabindex="-1"></a>                                    torch.nn.Linear(h_unit_interval[s], <span class="dv">1</span>), <span class="co"># H hidden units to 1 output neuron</span></span>
<span id="cb24-75"><a href="#cb24-75" aria-hidden="true" tabindex="-1"></a>                                    torch.nn.Sigmoid() <span class="co"># final tranfer function</span></span>
<span id="cb24-76"><a href="#cb24-76" aria-hidden="true" tabindex="-1"></a>                                )</span>
<span id="cb24-77"><a href="#cb24-77" aria-hidden="true" tabindex="-1"></a>            loss_fn <span class="op">=</span> torch.nn.BCELoss() <span class="co"># Binary classification loss</span></span>
<span id="cb24-78"><a href="#cb24-78" aria-hidden="true" tabindex="-1"></a>            best_trained_neural_net, final_loss, learning_curve <span class="op">=</span> train_neural_net(ann_model, loss_fn, X <span class="op">=</span> X_train, y <span class="op">=</span> y_train, n_replicates <span class="op">=</span> nr_of_nn_replicates, max_iter <span class="op">=</span> max_iter)</span>
<span id="cb24-79"><a href="#cb24-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-80"><a href="#cb24-80" aria-hidden="true" tabindex="-1"></a>            ann_y_val_estimated <span class="op">=</span> (best_trained_neural_net(X_val) <span class="op">&gt;</span> <span class="fl">0.5</span>)</span>
<span id="cb24-81"><a href="#cb24-81" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convert tensors to numpy arrays, to work smoothly with class comparisons</span></span>
<span id="cb24-82"><a href="#cb24-82" aria-hidden="true" tabindex="-1"></a>            ann_y_val_estimated <span class="op">=</span> ann_y_val_estimated.numpy().reshape(<span class="bu">len</span>(ann_y_val_estimated))</span>
<span id="cb24-83"><a href="#cb24-83" aria-hidden="true" tabindex="-1"></a>            y_val <span class="op">=</span> y_val.numpy().reshape(<span class="bu">len</span>(y_val))</span>
<span id="cb24-84"><a href="#cb24-84" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb24-85"><a href="#cb24-85" aria-hidden="true" tabindex="-1"></a>            ann_val_error_rate[k2] <span class="op">=</span> np.<span class="bu">sum</span>(ann_y_val_estimated <span class="op">!=</span> y_val) <span class="op">/</span> <span class="bu">len</span>(y_val)</span>
<span id="cb24-86"><a href="#cb24-86" aria-hidden="true" tabindex="-1"></a>            k2 <span class="op">=</span> k2 <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb24-87"><a href="#cb24-87" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb24-88"><a href="#cb24-88" aria-hidden="true" tabindex="-1"></a>        ann_gen_error_rate_s[s] <span class="op">=</span> np.<span class="bu">sum</span>(ann_val_error_rate) <span class="op">/</span> <span class="bu">len</span>(ann_val_error_rate)</span>
<span id="cb24-89"><a href="#cb24-89" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb24-90"><a href="#cb24-90" aria-hidden="true" tabindex="-1"></a>    ann_min_error <span class="op">=</span> np.<span class="bu">min</span>(ann_gen_error_rate_s)</span>
<span id="cb24-91"><a href="#cb24-91" aria-hidden="true" tabindex="-1"></a>    opt_nr_h_units_index <span class="op">=</span> np.argmin(ann_gen_error_rate_s)</span>
<span id="cb24-92"><a href="#cb24-92" aria-hidden="true" tabindex="-1"></a>    opt_nr_h_units <span class="op">=</span> h_unit_interval[opt_nr_h_units_index]</span>
<span id="cb24-93"><a href="#cb24-93" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-94"><a href="#cb24-94" aria-hidden="true" tabindex="-1"></a>    tensor_X_par <span class="op">=</span> torch.tensor(X_par, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb24-95"><a href="#cb24-95" aria-hidden="true" tabindex="-1"></a>    <span class="co">#tensor_X_par = tensor_X_par.unsqueeze(1)</span></span>
<span id="cb24-96"><a href="#cb24-96" aria-hidden="true" tabindex="-1"></a>    tensor_y_par <span class="op">=</span> torch.tensor(y_par, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb24-97"><a href="#cb24-97" aria-hidden="true" tabindex="-1"></a>    tensor_y_par <span class="op">=</span> tensor_y_par.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb24-98"><a href="#cb24-98" aria-hidden="true" tabindex="-1"></a>    tensor_X_test <span class="op">=</span> torch.tensor(X_test, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb24-99"><a href="#cb24-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-100"><a href="#cb24-100" aria-hidden="true" tabindex="-1"></a>    ann_model <span class="op">=</span> <span class="kw">lambda</span>: torch.nn.Sequential(</span>
<span id="cb24-101"><a href="#cb24-101" aria-hidden="true" tabindex="-1"></a>                                torch.nn.Linear(M, h_unit_interval[opt_nr_h_units_index]),</span>
<span id="cb24-102"><a href="#cb24-102" aria-hidden="true" tabindex="-1"></a>                                torch.nn.Tanh(),   </span>
<span id="cb24-103"><a href="#cb24-103" aria-hidden="true" tabindex="-1"></a>                                torch.nn.Linear(h_unit_interval[opt_nr_h_units_index], <span class="dv">1</span>), <span class="co"># H hidden units to 1 output neuron</span></span>
<span id="cb24-104"><a href="#cb24-104" aria-hidden="true" tabindex="-1"></a>                                torch.nn.Sigmoid() <span class="co"># final tranfer function</span></span>
<span id="cb24-105"><a href="#cb24-105" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-106"><a href="#cb24-106" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-107"><a href="#cb24-107" aria-hidden="true" tabindex="-1"></a>    loss_fn <span class="op">=</span> torch.nn.BCELoss() <span class="co"># Binary classification loss</span></span>
<span id="cb24-108"><a href="#cb24-108" aria-hidden="true" tabindex="-1"></a>    best_trained_neural_net, final_loss, learning_curve <span class="op">=</span> train_neural_net(ann_model, loss_fn, X <span class="op">=</span> tensor_X_par, y <span class="op">=</span> tensor_y_par, n_replicates <span class="op">=</span> nr_of_nn_replicates, max_iter <span class="op">=</span> max_iter)</span>
<span id="cb24-109"><a href="#cb24-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-110"><a href="#cb24-110" aria-hidden="true" tabindex="-1"></a>    ann_y_test_estimated <span class="op">=</span> (best_trained_neural_net(tensor_X_test) <span class="op">&gt;</span> <span class="fl">0.5</span>)</span>
<span id="cb24-111"><a href="#cb24-111" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert tensors to numpy arrays, to work smoothly with class comparisons</span></span>
<span id="cb24-112"><a href="#cb24-112" aria-hidden="true" tabindex="-1"></a>    ann_y_test_estimated <span class="op">=</span> ann_y_test_estimated.numpy().reshape(<span class="bu">len</span>(ann_y_test_estimated))</span>
<span id="cb24-113"><a href="#cb24-113" aria-hidden="true" tabindex="-1"></a>    ann_test_error_k1[k1] <span class="op">=</span> np.<span class="bu">sum</span>(ann_y_test_estimated <span class="op">!=</span> y_test) <span class="op">/</span> <span class="bu">len</span>(y_test)</span>
<span id="cb24-114"><a href="#cb24-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-115"><a href="#cb24-115" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Error rate - Classification ANN - CV1 fold </span><span class="sc">{0}</span><span class="st">/</span><span class="sc">{1}</span><span class="st">: </span><span class="sc">{2}</span><span class="st">%'</span>.<span class="bu">format</span>(k1<span class="op">+</span><span class="dv">1</span>, K1, np.<span class="bu">round</span>(<span class="dv">100</span> <span class="op">*</span> ann_test_error_k1[k1], decimals <span class="op">=</span> <span class="dv">2</span>)))</span>
<span id="cb24-116"><a href="#cb24-116" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Optimal number of hidden units: </span><span class="sc">{0}</span><span class="st">'</span>.<span class="bu">format</span>(opt_nr_h_units))</span>
<span id="cb24-117"><a href="#cb24-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-118"><a href="#cb24-118" aria-hidden="true" tabindex="-1"></a>    <span class="co">#----------------------------------------------------------------------------------------------------#</span></span>
<span id="cb24-119"><a href="#cb24-119" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-120"><a href="#cb24-120" aria-hidden="true" tabindex="-1"></a>    <span class="co">#--------------------------------Baseline - LogRegression -------------------------------------------#</span></span>
<span id="cb24-121"><a href="#cb24-121" aria-hidden="true" tabindex="-1"></a>    class_1_count <span class="op">=</span> y_par.<span class="bu">sum</span>() <span class="co"># class 1</span></span>
<span id="cb24-122"><a href="#cb24-122" aria-hidden="true" tabindex="-1"></a>    class_0_count <span class="op">=</span> <span class="bu">len</span>(y_par) <span class="op">-</span> y_par.<span class="bu">sum</span>() <span class="co"># class 0</span></span>
<span id="cb24-123"><a href="#cb24-123" aria-hidden="true" tabindex="-1"></a>    baseline_class <span class="op">=</span> <span class="bu">float</span>(np.argmax([class_0_count, class_1_count]))</span>
<span id="cb24-124"><a href="#cb24-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-125"><a href="#cb24-125" aria-hidden="true" tabindex="-1"></a>    baseline_test_error_k1[k1] <span class="op">=</span> np.<span class="bu">sum</span>(y_test <span class="op">!=</span> baseline_class) <span class="op">/</span> <span class="bu">len</span>(y_test)</span>
<span id="cb24-126"><a href="#cb24-126" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-127"><a href="#cb24-127" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Error rate - baseline log-reg - CV1 fold </span><span class="sc">{0}</span><span class="st">/</span><span class="sc">{1}</span><span class="st">: </span><span class="sc">{2}</span><span class="st">%'</span>.<span class="bu">format</span>(k1<span class="op">+</span><span class="dv">1</span>, K1, np.<span class="bu">round</span>(<span class="dv">100</span> <span class="op">*</span> baseline_test_error_k1[k1], decimals <span class="op">=</span> <span class="dv">2</span>)))</span>
<span id="cb24-128"><a href="#cb24-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-129"><a href="#cb24-129" aria-hidden="true" tabindex="-1"></a>    k1 <span class="op">=</span> k1 <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb24-130"><a href="#cb24-130" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb24-131"><a href="#cb24-131" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb24-132"><a href="#cb24-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-133"><a href="#cb24-133" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-134"><a href="#cb24-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-135"><a href="#cb24-135" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-136"><a href="#cb24-136" aria-hidden="true" tabindex="-1"></a>        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Computing CV1 fold: 1/10..

Error rate - regularized log-reg - CV1 fold 1/10: 36.17%
Optimal lambda: 10.0
Error rate - Classification ANN - CV1 fold 1/10: 34.04%
Optimal number of hidden units: 1
Error rate - baseline log-reg - CV1 fold 1/10: 40.43%


Computing CV1 fold: 2/10..

Error rate - regularized log-reg - CV1 fold 2/10: 31.91%
Optimal lambda: 10.0
Error rate - Classification ANN - CV1 fold 2/10: 34.04%
Optimal number of hidden units: 1
Error rate - baseline log-reg - CV1 fold 2/10: 38.3%


Computing CV1 fold: 3/10..

Error rate - regularized log-reg - CV1 fold 3/10: 28.26%
Optimal lambda: 10.0
Error rate - Classification ANN - CV1 fold 3/10: 30.43%
Optimal number of hidden units: 2
Error rate - baseline log-reg - CV1 fold 3/10: 32.61%


Computing CV1 fold: 4/10..

Error rate - regularized log-reg - CV1 fold 4/10: 23.91%
Optimal lambda: 10.0
Error rate - Classification ANN - CV1 fold 4/10: 23.91%
Optimal number of hidden units: 1
Error rate - baseline log-reg - CV1 fold 4/10: 34.78%


Computing CV1 fold: 5/10..

Error rate - regularized log-reg - CV1 fold 5/10: 19.57%
Optimal lambda: 10.0
Error rate - Classification ANN - CV1 fold 5/10: 23.91%
Optimal number of hidden units: 1
Error rate - baseline log-reg - CV1 fold 5/10: 26.09%


Computing CV1 fold: 6/10..

Error rate - regularized log-reg - CV1 fold 6/10: 26.09%
Optimal lambda: 1.0
Error rate - Classification ANN - CV1 fold 6/10: 26.09%
Optimal number of hidden units: 1
Error rate - baseline log-reg - CV1 fold 6/10: 34.78%


Computing CV1 fold: 7/10..

Error rate - regularized log-reg - CV1 fold 7/10: 32.61%
Optimal lambda: 10.0
Error rate - Classification ANN - CV1 fold 7/10: 32.61%
Optimal number of hidden units: 1
Error rate - baseline log-reg - CV1 fold 7/10: 34.78%


Computing CV1 fold: 8/10..

Error rate - regularized log-reg - CV1 fold 8/10: 32.61%
Optimal lambda: 10.0
Error rate - Classification ANN - CV1 fold 8/10: 32.61%
Optimal number of hidden units: 1
Error rate - baseline log-reg - CV1 fold 8/10: 39.13%


Computing CV1 fold: 9/10..

Error rate - regularized log-reg - CV1 fold 9/10: 21.74%
Optimal lambda: 10.0
Error rate - Classification ANN - CV1 fold 9/10: 19.57%
Optimal number of hidden units: 1
Error rate - baseline log-reg - CV1 fold 9/10: 36.96%


Computing CV1 fold: 10/10..

Error rate - regularized log-reg - CV1 fold 10/10: 19.57%
Optimal lambda: 10.0
Error rate - Classification ANN - CV1 fold 10/10: 19.57%
Optimal number of hidden units: 1
Error rate - baseline log-reg - CV1 fold 10/10: 28.26%

</code></pre>
</div>
</div>
<p>Now we compute the statistical evaluation of the methods using confidence intervals, z-scores, and error rates.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.figure(figsize = [18, 10])</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize <span class="op">=</span> [<span class="dv">18</span>, <span class="dv">5</span>])</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>plt.subplots_adjust(wspace <span class="op">=</span> <span class="fl">0.6</span>)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> (logreg_test_error_k1 <span class="op">-</span> baseline_test_error_k1)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>z_mean <span class="op">=</span> z.mean()</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>deg_f <span class="op">=</span> K1<span class="op">-</span><span class="dv">1</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>sig <span class="op">=</span>  (z <span class="op">-</span> z_mean).std() <span class="op">/</span> np.sqrt(deg_f)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>zL <span class="op">=</span> z_mean <span class="op">+</span> sig <span class="op">*</span> stats.t.ppf(alpha<span class="op">/</span><span class="dv">2</span>, deg_f)<span class="op">;</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>zH <span class="op">=</span> z_mean <span class="op">+</span> sig <span class="op">*</span> stats.t.ppf(<span class="dv">1</span><span class="op">-</span>alpha<span class="op">/</span><span class="dv">2</span>, deg_f)<span class="op">;</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>plt.boxplot(np.concatenate((logreg_test_error_k1.reshape((<span class="bu">len</span>(logreg_test_error_k1), <span class="dv">1</span>))<span class="op">*</span><span class="dv">100</span>, baseline_test_error_k1.reshape((<span class="bu">len</span>(baseline_test_error_k1), <span class="dv">1</span>))<span class="op">*</span><span class="dv">100</span>), axis <span class="op">=</span> <span class="dv">1</span>))</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Regularized Logistic Regression vs. Baseline-form Logistic Regression'</span>)</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cross-validation error [%]'</span>)</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Credibility interval has z-scores: (</span><span class="sc">{0}</span><span class="st">, </span><span class="sc">{1}</span><span class="st">)'</span>.<span class="bu">format</span>(np.<span class="bu">round</span>(zL, decimals <span class="op">=</span> <span class="dv">6</span>), np.<span class="bu">round</span>(zH, decimals <span class="op">=</span> <span class="dv">6</span>)))</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> zL <span class="op">&lt;=</span> <span class="dv">0</span> <span class="kw">and</span> zH <span class="op">&gt;=</span> <span class="dv">0</span> :</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Regularized log-reg and baseline log-reg classifiers are NOT significantly different'</span>)        </span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Regularized log-reg and baseline log-reg classifiers are significantly different.'</span>)</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> (ann_test_error_k1 <span class="op">-</span> logreg_test_error_k1)</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>z_mean <span class="op">=</span> z.mean()</span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a>deg_f <span class="op">=</span> K1<span class="op">-</span><span class="dv">1</span></span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a>sig <span class="op">=</span>  (z <span class="op">-</span> z_mean).std() <span class="op">/</span> np.sqrt(deg_f)</span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>zL <span class="op">=</span> z_mean <span class="op">+</span> sig <span class="op">*</span> stats.t.ppf(alpha<span class="op">/</span><span class="dv">2</span>, deg_f)<span class="op">;</span></span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a>zH <span class="op">=</span> z_mean <span class="op">+</span> sig <span class="op">*</span> stats.t.ppf(<span class="dv">1</span><span class="op">-</span>alpha<span class="op">/</span><span class="dv">2</span>, deg_f)<span class="op">;</span></span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a>plt.boxplot(np.concatenate((ann_test_error_k1.reshape((<span class="bu">len</span>(ann_test_error_k1), <span class="dv">1</span>))<span class="op">*</span><span class="dv">100</span>, logreg_test_error_k1.reshape((<span class="bu">len</span>(logreg_test_error_k1), <span class="dv">1</span>))<span class="op">*</span><span class="dv">100</span>), axis <span class="op">=</span> <span class="dv">1</span>))</span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Classification ANN vs. Regularized Logistic Regression'</span>)</span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cross-validation error [%]'</span>)</span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Credibility interval has z-scores: (</span><span class="sc">{0}</span><span class="st">, </span><span class="sc">{1}</span><span class="st">)'</span>.<span class="bu">format</span>(np.<span class="bu">round</span>(zL, decimals <span class="op">=</span> <span class="dv">6</span>), np.<span class="bu">round</span>(zH, decimals <span class="op">=</span> <span class="dv">6</span>)))</span>
<span id="cb26-46"><a href="#cb26-46" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> zL <span class="op">&lt;=</span> <span class="dv">0</span> <span class="kw">and</span> zH <span class="op">&gt;=</span> <span class="dv">0</span> :</span>
<span id="cb26-47"><a href="#cb26-47" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Classification ANN and regularized log-reg are NOT significantly different'</span>)        </span>
<span id="cb26-48"><a href="#cb26-48" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb26-49"><a href="#cb26-49" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Classification ANN and regularized log-reg are significantly different.'</span>)</span>
<span id="cb26-50"><a href="#cb26-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb26-51"><a href="#cb26-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb26-52"><a href="#cb26-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-53"><a href="#cb26-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-54"><a href="#cb26-54" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> (ann_test_error_k1 <span class="op">-</span> baseline_test_error_k1)</span>
<span id="cb26-55"><a href="#cb26-55" aria-hidden="true" tabindex="-1"></a>z_mean <span class="op">=</span> z.mean()</span>
<span id="cb26-56"><a href="#cb26-56" aria-hidden="true" tabindex="-1"></a>deg_f <span class="op">=</span> K1<span class="op">-</span><span class="dv">1</span></span>
<span id="cb26-57"><a href="#cb26-57" aria-hidden="true" tabindex="-1"></a>sig <span class="op">=</span>  (z <span class="op">-</span> z_mean).std() <span class="op">/</span> np.sqrt(deg_f)</span>
<span id="cb26-58"><a href="#cb26-58" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb26-59"><a href="#cb26-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-60"><a href="#cb26-60" aria-hidden="true" tabindex="-1"></a>zL <span class="op">=</span> z_mean <span class="op">+</span> sig <span class="op">*</span> stats.t.ppf(alpha<span class="op">/</span><span class="dv">2</span>, deg_f)<span class="op">;</span></span>
<span id="cb26-61"><a href="#cb26-61" aria-hidden="true" tabindex="-1"></a>zH <span class="op">=</span> z_mean <span class="op">+</span> sig <span class="op">*</span> stats.t.ppf(<span class="dv">1</span><span class="op">-</span>alpha<span class="op">/</span><span class="dv">2</span>, deg_f)<span class="op">;</span></span>
<span id="cb26-62"><a href="#cb26-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-63"><a href="#cb26-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-64"><a href="#cb26-64" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.subplot(2, 3, 6)</span></span>
<span id="cb26-65"><a href="#cb26-65" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>,<span class="dv">3</span>)</span>
<span id="cb26-66"><a href="#cb26-66" aria-hidden="true" tabindex="-1"></a>plt.boxplot(np.concatenate((ann_test_error_k1.reshape((<span class="bu">len</span>(ann_test_error_k1), <span class="dv">1</span>))<span class="op">*</span><span class="dv">100</span>, baseline_test_error_k1.reshape((<span class="bu">len</span>(baseline_test_error_k1), <span class="dv">1</span>))<span class="op">*</span><span class="dv">100</span>), axis <span class="op">=</span> <span class="dv">1</span>))</span>
<span id="cb26-67"><a href="#cb26-67" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Classification ANN vs. Baseline-form Logistic Regression'</span>)</span>
<span id="cb26-68"><a href="#cb26-68" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cross-validation error [%]'</span>)</span>
<span id="cb26-69"><a href="#cb26-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-70"><a href="#cb26-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Credibility interval has z-scores: (</span><span class="sc">{0}</span><span class="st">, </span><span class="sc">{1}</span><span class="st">)'</span>.<span class="bu">format</span>(np.<span class="bu">round</span>(zL, decimals <span class="op">=</span> <span class="dv">6</span>), np.<span class="bu">round</span>(zH, decimals <span class="op">=</span> <span class="dv">6</span>)))</span>
<span id="cb26-71"><a href="#cb26-71" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> zL <span class="op">&lt;=</span> <span class="dv">0</span> <span class="kw">and</span> zH <span class="op">&gt;=</span> <span class="dv">0</span> :</span>
<span id="cb26-72"><a href="#cb26-72" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Classification ANN and baseline log-reg classifiers are NOT significantly different'</span>)        </span>
<span id="cb26-73"><a href="#cb26-73" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb26-74"><a href="#cb26-74" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Classification ANN and baseline log-reg classifiers are significantly different.'</span>)</span>
<span id="cb26-75"><a href="#cb26-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb26-76"><a href="#cb26-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Credibility interval has z-scores: (-0.100404, -0.046959)
Regularized log-reg and baseline log-reg classifiers are significantly different.


Credibility interval has z-scores: (-0.009863, 0.018559)
Classification ANN and regularized log-reg are NOT significantly different


Credibility interval has z-scores: (-0.103614, -0.035054)
Classification ANN and baseline log-reg classifiers are significantly different.

</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="Project2_ML_files/figure-html/cell-17-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>gen_error_regularized_logreg <span class="op">=</span> np.<span class="bu">sum</span>(logreg_test_error_k1) <span class="op">/</span> <span class="bu">len</span>(logreg_test_error_k1)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co"># gen_error_tree = np.sum(tree_test_error_k1) / len(tree_test_error_k1)</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>gen_error_ann <span class="op">=</span> np.<span class="bu">sum</span>(ann_test_error_k1) <span class="op">/</span> <span class="bu">len</span>(ann_test_error_k1)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>gen_error_baseline_logreg <span class="op">=</span> np.<span class="bu">sum</span>(baseline_test_error_k1) <span class="op">/</span> <span class="bu">len</span>(baseline_test_error_k1)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Generalized error rate - regularized log-reg: </span><span class="sc">{0}</span><span class="st">%'</span>.<span class="bu">format</span>(np.<span class="bu">round</span>(<span class="dv">100</span> <span class="op">*</span> gen_error_regularized_logreg, decimals <span class="op">=</span> <span class="dv">2</span>)))</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co"># print('Generalized error rate - depth-controlled classification tree: {0}%'.format(np.round(100 * gen_error_tree, decimals = 2)))</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Generalized error rate - classification ANN: </span><span class="sc">{0}</span><span class="st">%'</span>.<span class="bu">format</span>(np.<span class="bu">round</span>(<span class="dv">100</span> <span class="op">*</span> gen_error_ann, decimals <span class="op">=</span> <span class="dv">2</span>)))</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Generalized error rate - baseline log-reg: </span><span class="sc">{0}</span><span class="st">%'</span>.<span class="bu">format</span>(np.<span class="bu">round</span>(<span class="dv">100</span> <span class="op">*</span> gen_error_baseline_logreg, decimals <span class="op">=</span> <span class="dv">2</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Generalized error rate - regularized log-reg: 27.24%
Generalized error rate - classification ANN: 27.68%
Generalized error rate - baseline log-reg: 34.61%</code></pre>
</div>
</div>
<p>ANN and logisctic regression give promising results with a great accuracy of predictions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>raw_data <span class="op">=</span> df.to_numpy()</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>attr_col <span class="op">=</span> <span class="bu">list</span>(df.columns).index(<span class="st">'chd'</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">0</span>, attr_col)) <span class="op">+</span> <span class="bu">list</span>(<span class="bu">range</span>(attr_col <span class="op">+</span> <span class="dv">1</span>, <span class="bu">len</span>(df.columns)))</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> raw_data[:, cols]</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> raw_data[:, attr_col] <span class="co"># the 'target' column</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>attributeNames <span class="op">=</span> <span class="bu">list</span>(df.columns[cols])</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>classLabels <span class="op">=</span> raw_data[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>classNames <span class="op">=</span> np.unique(classLabels)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>N, M <span class="op">=</span> X.shape</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> <span class="bu">len</span>(classNames)</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> stats.zscore(X)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>log_model_with_reg <span class="op">=</span> LogisticRegression(penalty<span class="op">=</span><span class="st">'l2'</span>, C <span class="op">=</span> <span class="dv">1</span><span class="op">/</span><span class="fl">10.0</span>, solver <span class="op">=</span> <span class="st">'lbfgs'</span>, fit_intercept <span class="op">=</span> <span class="va">True</span>) <span class="co"># remember to select the most-commonly found optimal lambda value</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>log_model_no_reg <span class="op">=</span> LogisticRegression(solver <span class="op">=</span> <span class="st">'lbfgs'</span>, fit_intercept <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>log_model_with_reg <span class="op">=</span> log_model_with_reg.fit(X, y)</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>log_model_no_reg <span class="op">=</span> log_model_no_reg.fit(X, y)</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Weights for LogReg model with regularization:'</span>)</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="sc">{:&gt;20}</span><span class="st"> </span><span class="sc">{:&gt;20}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="st">'Intercept'</span>, <span class="bu">str</span>(np.<span class="bu">round</span>(log_model_with_reg.intercept_[<span class="dv">0</span>],<span class="dv">3</span>))))</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> m <span class="kw">in</span> <span class="bu">range</span>(M):</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="sc">{:&gt;20}</span><span class="st"> </span><span class="sc">{:&gt;20}</span><span class="st">'</span>.<span class="bu">format</span>(attributeNames[m], <span class="bu">str</span>(np.<span class="bu">round</span>(log_model_with_reg.coef_[<span class="dv">0</span>][m],<span class="dv">3</span>))))</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Weights for LogReg model without regularization:'</span>)</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="sc">{:&gt;20}</span><span class="st"> </span><span class="sc">{:&gt;20}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="st">'Intercept'</span>, <span class="bu">str</span>(np.<span class="bu">round</span>(log_model_no_reg.intercept_[<span class="dv">0</span>],<span class="dv">3</span>))))</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> m <span class="kw">in</span> <span class="bu">range</span>(M):</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="sc">{:&gt;20}</span><span class="st"> </span><span class="sc">{:&gt;20}</span><span class="st">'</span>.<span class="bu">format</span>(attributeNames[m], <span class="bu">str</span>(np.<span class="bu">round</span>(log_model_no_reg.coef_[<span class="dv">0</span>][m],<span class="dv">3</span>))))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Weights for LogReg model with regularization:
           Intercept               -0.832
                 sbp                0.132
             tobacco                0.342
                 ldl                 0.32
           adiposity                0.134
             famhist                 0.41
               typea                0.318
             obesity               -0.197
             alcohol                0.006
                 age                0.556


Weights for LogReg model without regularization:
           Intercept               -0.873
                 sbp                0.133
             tobacco                0.362
                 ldl                0.355
           adiposity                0.143
             famhist                0.451
               typea                0.379
             obesity               -0.256
             alcohol                0.003
                 age                0.647</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Statistical comparison of models</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute confidence interval of z = zA-zB and p-value of Null hypothesis</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co"># ANN vs. regularized linear regression </span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>z1 <span class="op">=</span> (ann_test_error_k1 <span class="op">-</span> logreg_test_error_k1)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>CI1 <span class="op">=</span> stats.t.interval(<span class="dv">1</span><span class="op">-</span>alpha, <span class="bu">len</span>(z1)<span class="op">-</span><span class="dv">1</span>, loc<span class="op">=</span>np.mean(z1), scale<span class="op">=</span>stats.sem(z1))  <span class="co"># Confidence interval</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>p1 <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>stats.t.cdf(<span class="op">-</span>np.<span class="bu">abs</span>( np.mean(z1) )<span class="op">/</span>stats.sem(z1), df<span class="op">=</span><span class="bu">len</span>(z1)<span class="op">-</span><span class="dv">1</span>)  <span class="co"># p-value</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'ANN vs. regularized linear regression'</span>)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confidence interval:'</span>, CI1)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'p-value:'</span>, p1)</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="co"># ANN vs. baseline</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>z2 <span class="op">=</span> (ann_test_error_k1 <span class="op">-</span> baseline_test_error_k1)</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>CI2 <span class="op">=</span> stats.t.interval(<span class="dv">1</span><span class="op">-</span>alpha, <span class="bu">len</span>(z2)<span class="op">-</span><span class="dv">1</span>, loc<span class="op">=</span>np.mean(z2), scale<span class="op">=</span>stats.sem(z2))  <span class="co"># Confidence interval</span></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>p2 <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>stats.t.cdf(<span class="op">-</span>np.<span class="bu">abs</span>( np.mean(z2) )<span class="op">/</span>stats.sem(z2), df<span class="op">=</span><span class="bu">len</span>(z2)<span class="op">-</span><span class="dv">1</span>)  <span class="co"># p-value</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'ANN vs. baseline'</span>)</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confidence interval:'</span>, CI2)</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'p-value:'</span>, p2)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a><span class="co"># regularized linear regression vs. baseline</span></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>z3 <span class="op">=</span> (logreg_test_error_k1 <span class="op">-</span> baseline_test_error_k1)</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>CI3 <span class="op">=</span> stats.t.interval(<span class="dv">1</span><span class="op">-</span>alpha, <span class="bu">len</span>(z3)<span class="op">-</span><span class="dv">1</span>, loc<span class="op">=</span>np.mean(z3), scale<span class="op">=</span>stats.sem(z3))  <span class="co"># Confidence interval</span></span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>p3 <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>stats.t.cdf( <span class="op">-</span>np.<span class="bu">abs</span>( np.mean(z3) )<span class="op">/</span>stats.sem(z3), df<span class="op">=</span><span class="bu">len</span>(z3)<span class="op">-</span><span class="dv">1</span>)  <span class="co"># p-value</span></span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Regularized linear regression vs. baseline'</span>)</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confidence interval:'</span>, CI3)</span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'p-value:'</span>, p3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ANN vs. regularized linear regression
Confidence interval: (-0.009863390759518375, 0.018559042933431417)
p-value: 0.5063410363804055

ANN vs. baseline
Confidence interval: (-0.10361431267894713, -0.035053587413559774)
p-value: 0.001336672981066876

Regularized linear regression vs. baseline
Confidence interval: (-0.10040445833610023, -0.04695909393031971)
p-value: 0.00015190635545935752</code></pre>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>